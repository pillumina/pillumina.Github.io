<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>RL on CctoctoFX</title>
    <link>https://pillumina.github.io/tags/rl/</link>
    <description>Recent content in RL on CctoctoFX</description>
    <image>
      <title>CctoctoFX</title>
      <url>https://pillumina.github.io/imgs/icon_head.png</url>
      <link>https://pillumina.github.io/imgs/icon_head.png</link>
    </image>
    <generator>Hugo -- 0.148.2</generator>
    <language>en</language>
    <lastBuildDate>Thu, 14 Aug 2025 10:20:12 +0800</lastBuildDate>
    <atom:link href="https://pillumina.github.io/tags/rl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[VeRL] 参数速览</title>
      <link>https://pillumina.github.io/posts/aiinfra/05-verl-params/</link>
      <pubDate>Thu, 14 Aug 2025 10:20:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/05-verl-params/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;VeRL框架的参数众多，基于当前（2025.8.5）主线分支整理，附带了相关的理解，一些描述不一定完全正确，供学习参考。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;batch-size&#34;&gt;Batch Size&lt;/h2&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;参数名称&lt;/th&gt;
          &lt;th&gt;详细解释&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;data.train_batch_size&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;作用&lt;/strong&gt;：定义了单次训练发送给 Rollout Engine 的样本数量，也即这是在每个 PPO 迭代开始时，从训练数据集中采样的提示 （Prompt）数量。&lt;br&gt;&lt;br&gt;&lt;strong&gt;详细解释&lt;/strong&gt;：这个值是 RL 训练中的基本样本数量。例如，设置为 1024 意味着在一次迭代中会：&lt;br&gt;1. 从数据集中随机抽取 1024 个 prompt。&lt;br&gt; 2. 将这 1024 个 prompt 发送给当前的 Rollout Engine 中，从而得到 1024 组完整的 trajectories（prompt, response）。&lt;br&gt;3. 接下来，这 1024 个 trajectories 进行经验计算（make experience），后续用于 Actor 和 Critic 模型的更新。&lt;br&gt;&lt;br&gt;&lt;strong&gt;影响与权衡&lt;/strong&gt;：影响总共训练的样本量。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;data.val_batch_size&lt;/code&gt; （Deprecated)&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;作用&lt;/strong&gt;：在 Validation 阶段使用的批次大小。&lt;br&gt;&lt;br&gt;&lt;strong&gt;详细解释&lt;/strong&gt;：这与 &lt;code&gt;train_batch_size&lt;/code&gt; 类似，但仅用于评估模型性能，不参与训练。如果设置为 &lt;code&gt;null&lt;/code&gt;，会使用验证集的大小作为默认值。Note: 已经deprecated，推荐设置为 null。此时，整个 validation dataset 一次性发给 SGLang engines，自行进行内存管理。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;actor_rollout_ref.actor.ppo_mini_batch_size&lt;/code&gt; &lt;br&gt; &lt;code&gt;critic.ppo_mini_batch_size&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;作用&lt;/strong&gt;：定义了 PPO 训练更新中的 mini-batch 大小。&lt;br&gt;&lt;br&gt;&lt;strong&gt;详细解释&lt;/strong&gt;：&lt;code&gt;data.train_batch_size&lt;/code&gt; 收集到的全部经验数据将被分割成多个 mini-batch，每块的大小就是 &lt;code&gt;ppo_mini_batch_size&lt;/code&gt;。模型每处理完一个 mini-batch，才会进行一次参数更新。&lt;br&gt;例如，如果 &lt;code&gt;train_batch_size = 1024&lt;/code&gt;，&lt;code&gt;ppo_mini_batch_size = 256&lt;/code&gt;，那么在一个 PPO Epoch 中，模型会进行 &lt;code&gt;1024 / 256 = 4&lt;/code&gt; 次参数更新。&lt;br&gt;&lt;br&gt;&lt;strong&gt;影响与权衡&lt;/strong&gt;：增大 mini-batch，单次更新的梯度更稳定，但更新频率更低，更新次数减少。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu&lt;/code&gt; &lt;br&gt; &lt;code&gt;critic.ppo_micro_batch_size_per_gpu&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;作用&lt;/strong&gt;：定义了在单个 GPU 上进行一次 forward/backward 的数据大小。&lt;br&gt;&lt;br&gt;&lt;strong&gt;详细解释&lt;/strong&gt;：这是实现梯度累积的核心参数。mini-batch 会被再次切分为若干个 micro-batch。例如，在单卡上，&lt;code&gt;ppo_mini_batch_size = 256&lt;/code&gt;，&lt;code&gt;ppo_micro_batch_size_per_gpu = 32&lt;/code&gt;，那么梯度累积的步数就是 &lt;code&gt;256 / 32 = 8&lt;/code&gt;。这意味着模型会运行 8 次 forward 得到 loss，然后 backward 的到 gradient。每次处理 32 个样本，直到累积完整个 mini-batch 计算出的梯度。此时，使用累积的总梯度，对模型参数进行一次更新（&lt;code&gt;optimizer.step()&lt;/code&gt;）。这个值必须根据显存大小来严格调整，是防止 OOM 的关键。&lt;br&gt;&lt;br&gt;&lt;strong&gt;影响与权衡&lt;/strong&gt;：增大此值，减少了梯度累积的次数，可以提高训练的吞吐量，增大显存消耗。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;actor_rollout_ref.actor.ppo_micro_batch_size&lt;/code&gt; &lt;br&gt; &lt;code&gt;critic.ppo_micro_batch_size&lt;/code&gt;（Deprecated)&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;作用&lt;/strong&gt;：已弃用，被 &lt;code&gt;per_gpu&lt;/code&gt; 版本取代，因为它能更好地适应分布式训练环境。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;dynamic-batch-size&#34;&gt;Dynamic Batch Size&lt;/h2&gt;
&lt;p&gt;当样本长度差异很大时，按样本数量划分批次可能导致不同批次的计算量极不均衡，而基于 token 总数来控制 batch size 是一种平衡每个 batch 训练时间的方案。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[RL4LLM] 异步RL框架: Slime</title>
      <link>https://pillumina.github.io/posts/aiinfra/02-slime/</link>
      <pubDate>Thu, 07 Aug 2025 17:10:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/02-slime/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/THUDM/slime&#34;&gt;https://github.com/THUDM/slime&lt;/a&gt;&lt;br&gt;
一个异步实现但是非完全异步的RL框架&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;总体架构&#34;&gt;总体架构&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;从源码模块划分，有三大核心模块：
&lt;ul&gt;
&lt;li&gt;training（Megatron）：主训练流程，负责模型参数更新。&lt;/li&gt;
&lt;li&gt;rollout（SGLang + router）：负责采样、奖励/验证生成，产生训练数据。&lt;/li&gt;
&lt;li&gt;data buffer：桥接训练与采样，管理数据流、缓存与生成方式。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;分布式调度：关于资源分配、actor启动、任务调度都由于Ray管理，支持异步训练和采样&lt;/li&gt;
&lt;li&gt;插件机制：支持自定义buffer、模型、模型格式转换（mbridge）&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;mermaid&#34;&gt;
  flowchart LR
    subgraph Ray[Ray 分布式调度]
        A1[Actor Group&amp;lt;br&amp;gt;训练 Actor]
        A2[Rollout Group&amp;lt;br&amp;gt;采样/生成 Actor]
        A3[Placement Group&amp;lt;br&amp;gt;资源分配]
    end
    subgraph Training[Training &amp;lt;Megatron&amp;gt;]
        T1[模型训练]
        T2[权重同步]
        T3[评估/保存]
    end
    subgraph Rollout[Rollout &amp;lt;SGLang+Router&amp;gt;]
        R1[采样/生成]
        R2[奖励模型]
        R3[过滤器]
    end
    subgraph Buffer[Data Buffer]
        B1[数据缓存]
        B2[数据流转]
        B3[Offload/Onload]
    end
    subgraph Plugins[插件机制]
        P1[Buffer 插件]
        P2[Model 插件]
        P3[mbridge 格式转换]
    end

    A1--&amp;gt;|训练数据|B1
    A2--&amp;gt;|生成数据|B1
    B1--&amp;gt;|数据流|A1
    B1--&amp;gt;|数据流|A2
    A1--&amp;gt;|权重同步|A2
    A1--&amp;gt;|评估/保存|T3
    A2--&amp;gt;|采样/奖励/过滤|R1
    R1--&amp;gt;|奖励|R2
    R1--&amp;gt;|过滤|R3
    B1--&amp;gt;|插件扩展|P1
    A1--&amp;gt;|模型扩展|P2
    A1--&amp;gt;|格式转换|P3
    A3--&amp;gt;|资源分配|A1
    A3--&amp;gt;|资源分配|A2
&lt;/pre&gt;

&lt;h2 id=&#34;各模块视角的关系图&#34;&gt;各模块视角的关系图&lt;/h2&gt;
&lt;h3 id=&#34;slimerollout-组件图&#34;&gt;slime/rollout 组件图&lt;/h3&gt;
&lt;p&gt;rollout 负责采样、奖励、过滤，支持多种采样/奖励/过滤策略。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[RL4LLM] 异步RL框架: Areal</title>
      <link>https://pillumina.github.io/posts/aiinfra/03-areal/</link>
      <pubDate>Thu, 07 Aug 2025 14:40:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/03-areal/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/inclusionAI/AReaL&#34;&gt;https://github.com/inclusionAI/AReaL&lt;/a&gt;&lt;br&gt;
纯异步RL方案&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;异步ppo训练调用流程&#34;&gt;异步PPO训练调用流程&lt;/h2&gt;
&lt;pre class=&#34;mermaid&#34;&gt;
  graph TD
    A[用户执行: examples/run_async_ppo.sh] --&amp;gt; B[training/main_async_ppo.py]
    B --&amp;gt; C[AsyncPPOMATHConfig配置解析]
    C --&amp;gt; D[training/utils.py: run_experiment]
    
    D --&amp;gt; E[Ray初始化]
    E --&amp;gt; F[exp_cfg.initial_setup]
    F --&amp;gt; G[AsyncRLExperimentConfig.initial_setup]
    G --&amp;gt; H[创建ExperimentConfig]
    
    H --&amp;gt; I[启动Workers]
    I --&amp;gt; J[MasterWorker]
    I --&amp;gt; K[ModelWorker]
    I --&amp;gt; L[GenerationServer]
    I --&amp;gt; M[GserverManager]
    I --&amp;gt; N[RolloutWorker]
    
    %% MasterWorker训练流程
    J --&amp;gt; J1[MasterWorker._poll_async]
    J1 --&amp;gt; J2[FunctionExecutor.execute_step]
    J2 --&amp;gt; J3[执行数据流图遍历]
    J3 --&amp;gt; J4[发送训练请求到ModelWorker]
    
    %% ModelWorker处理流程
    K --&amp;gt; K1[ModelWorker._poll]
    K1 --&amp;gt; K2[接收MasterWorker请求]
    K2 --&amp;gt; K3[处理训练/推理请求]
    K3 --&amp;gt; K4[执行模型前向/反向传播]
    
    %% Rollout流程
    N --&amp;gt; N1[RolloutWorker._poll_async]
    N1 --&amp;gt; N2[load_next_data]
    N2 --&amp;gt; N3[allocate_new_rollout]
    N3 --&amp;gt; N4[agent.collect_trajectory]
    N4 --&amp;gt; N5[env.step计算奖励]
    N5 --&amp;gt; N6[推送数据到训练端]
    
    %% 生成服务器流程
    L --&amp;gt; L1[GenerationServer._poll]
    L1 --&amp;gt; L2[启动SGLang子进程]
    L2 --&amp;gt; L3[处理生成请求]
    
    %% 生成服务器管理器
    M --&amp;gt; M1[GserverManager._poll]
    M1 --&amp;gt; M2[HTTP服务线程]
    M2 --&amp;gt; M3[请求调度和权重更新]
    
    %% 数据流
    N6 --&amp;gt; O[stream_dataset.py]
    O --&amp;gt; J4
    
    %% 异步通信
    J4 -.-&amp;gt;|异步请求| K2
    N3 -.-&amp;gt;|HTTP请求| M2
    M2 -.-&amp;gt;|调度请求| L3
    
    %% 权重更新
    K4 --&amp;gt; P[参数更新]
    P --&amp;gt; Q[权重同步]
    Q --&amp;gt; M3
    M3 --&amp;gt; R[更新生成服务器权重]
    
    style A fill:#e1f5fe
    style J fill:#f3e5f5
    style K fill:#e8f5e8
    style L fill:#fff3e0
    style M fill:#fce4ec
    style N fill:#f1f8e9
&lt;/pre&gt;

&lt;h3 id=&#34;用户入口到配置解析&#34;&gt;用户入口到配置解析&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;examples/run_async_ppo.sh&lt;/code&gt; → &lt;code&gt;training/main_async_ppo.py&lt;/code&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
