<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>RL on CctoctoFX</title>
    <link>https://pillumina.github.io/tags/rl/</link>
    <description>Recent content in RL on CctoctoFX</description>
    <image>
      <title>CctoctoFX</title>
      <url>https://pillumina.github.io/imgs/icon_head.png</url>
      <link>https://pillumina.github.io/imgs/icon_head.png</link>
    </image>
    <generator>Hugo -- 0.148.2</generator>
    <language>en</language>
    <lastBuildDate>Thu, 20 Nov 2025 11:30:12 +0800</lastBuildDate>
    <atom:link href="https://pillumina.github.io/tags/rl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[Deterministic RL] deterministic问题的来源和相关工作总结</title>
      <link>https://pillumina.github.io/posts/aiinfra/14-deterministic-rl/</link>
      <pubDate>Thu, 20 Nov 2025 11:30:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/14-deterministic-rl/</guid>
      <description>&lt;h1 id=&#34;理解llm推理中deterministic问题来源&#34;&gt;理解LLM推理中deterministic问题来源&lt;/h1&gt;
&lt;p&gt;Wiki上对deterministic算法的定义是:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output.”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;而我们在文中要讨论的，即对于LLM这个context下的deterministic问题，我会先从inference角度（即重复给定一个确定的input，模型的推理为什么无法给定确定的输出）进行问题的理解，再进一步讨论RL工程中的training &amp;amp; inference之间差异，可能会导致RL训练的崩溃问题，并继续讨论业界现在已有的解决方案、与还在&lt;code&gt;working-in-progress&lt;/code&gt;的工作。&lt;/p&gt;
&lt;h2 id=&#34;浮点数的非结合性&#34;&gt;浮点数的非结合性&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/&#34;&gt;thinking machines lab针对batch invariant讨论的文章&lt;/a&gt;，详细地解释了在LLM推理中不确定性的来原，即因为精度有限，GPU浮点数运算中的结合性通常不成立：&lt;/p&gt;
$$(a+b)+c \neq a+(b+c) $$&lt;p&gt;&lt;br&gt;
&lt;a href=&#34;https://arxiv.org/abs/2506.09501&#34;&gt;这篇arxiv文章&lt;/a&gt;，则更深入得说明了这个问题：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Floating-point arithmetic in GPUs exhibits non-associativity, meaning (a+b)+c≠a+(b+c) due to finite precision and rounding errors. This property directly impacts the computation of attention scores and logits in the transformer architecture, where parallel operations across multiple threads can yield different results based on execution order.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[RL4LLM] 异步RL框架: Slime</title>
      <link>https://pillumina.github.io/posts/aiinfra/02-slime/</link>
      <pubDate>Thu, 07 Aug 2025 17:10:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/02-slime/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/THUDM/slime&#34;&gt;https://github.com/THUDM/slime&lt;/a&gt;&lt;br&gt;
一个异步实现但是非完全异步的RL框架&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;总体架构&#34;&gt;总体架构&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;从源码模块划分，有三大核心模块：
&lt;ul&gt;
&lt;li&gt;training（Megatron）：主训练流程，负责模型参数更新。&lt;/li&gt;
&lt;li&gt;rollout（SGLang + router）：负责采样、奖励/验证生成，产生训练数据。&lt;/li&gt;
&lt;li&gt;data buffer：桥接训练与采样，管理数据流、缓存与生成方式。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;分布式调度：关于资源分配、actor启动、任务调度都由于Ray管理，支持异步训练和采样&lt;/li&gt;
&lt;li&gt;插件机制：支持自定义buffer、模型、模型格式转换（mbridge）&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;mermaid&#34;&gt;
  flowchart LR
    subgraph Ray[Ray 分布式调度]
        A1[Actor Group&amp;lt;br&amp;gt;训练 Actor]
        A2[Rollout Group&amp;lt;br&amp;gt;采样/生成 Actor]
        A3[Placement Group&amp;lt;br&amp;gt;资源分配]
    end
    subgraph Training[Training &amp;lt;Megatron&amp;gt;]
        T1[模型训练]
        T2[权重同步]
        T3[评估/保存]
    end
    subgraph Rollout[Rollout &amp;lt;SGLang+Router&amp;gt;]
        R1[采样/生成]
        R2[奖励模型]
        R3[过滤器]
    end
    subgraph Buffer[Data Buffer]
        B1[数据缓存]
        B2[数据流转]
        B3[Offload/Onload]
    end
    subgraph Plugins[插件机制]
        P1[Buffer 插件]
        P2[Model 插件]
        P3[mbridge 格式转换]
    end

    A1--&amp;gt;|训练数据|B1
    A2--&amp;gt;|生成数据|B1
    B1--&amp;gt;|数据流|A1
    B1--&amp;gt;|数据流|A2
    A1--&amp;gt;|权重同步|A2
    A1--&amp;gt;|评估/保存|T3
    A2--&amp;gt;|采样/奖励/过滤|R1
    R1--&amp;gt;|奖励|R2
    R1--&amp;gt;|过滤|R3
    B1--&amp;gt;|插件扩展|P1
    A1--&amp;gt;|模型扩展|P2
    A1--&amp;gt;|格式转换|P3
    A3--&amp;gt;|资源分配|A1
    A3--&amp;gt;|资源分配|A2
&lt;/pre&gt;

&lt;h2 id=&#34;各模块视角的关系图&#34;&gt;各模块视角的关系图&lt;/h2&gt;
&lt;h3 id=&#34;slimerollout-组件图&#34;&gt;slime/rollout 组件图&lt;/h3&gt;
&lt;p&gt;rollout 负责采样、奖励、过滤，支持多种采样/奖励/过滤策略。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[RL4LLM] 异步RL框架: Areal</title>
      <link>https://pillumina.github.io/posts/aiinfra/03-areal/</link>
      <pubDate>Thu, 07 Aug 2025 14:40:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/03-areal/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/inclusionAI/AReaL&#34;&gt;https://github.com/inclusionAI/AReaL&lt;/a&gt;&lt;br&gt;
纯异步RL方案&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;异步ppo训练调用流程&#34;&gt;异步PPO训练调用流程&lt;/h2&gt;
&lt;pre class=&#34;mermaid&#34;&gt;
  graph TD
    A[用户执行: examples/run_async_ppo.sh] --&amp;gt; B[training/main_async_ppo.py]
    B --&amp;gt; C[AsyncPPOMATHConfig配置解析]
    C --&amp;gt; D[training/utils.py: run_experiment]
    
    D --&amp;gt; E[Ray初始化]
    E --&amp;gt; F[exp_cfg.initial_setup]
    F --&amp;gt; G[AsyncRLExperimentConfig.initial_setup]
    G --&amp;gt; H[创建ExperimentConfig]
    
    H --&amp;gt; I[启动Workers]
    I --&amp;gt; J[MasterWorker]
    I --&amp;gt; K[ModelWorker]
    I --&amp;gt; L[GenerationServer]
    I --&amp;gt; M[GserverManager]
    I --&amp;gt; N[RolloutWorker]
    
    %% MasterWorker训练流程
    J --&amp;gt; J1[MasterWorker._poll_async]
    J1 --&amp;gt; J2[FunctionExecutor.execute_step]
    J2 --&amp;gt; J3[执行数据流图遍历]
    J3 --&amp;gt; J4[发送训练请求到ModelWorker]
    
    %% ModelWorker处理流程
    K --&amp;gt; K1[ModelWorker._poll]
    K1 --&amp;gt; K2[接收MasterWorker请求]
    K2 --&amp;gt; K3[处理训练/推理请求]
    K3 --&amp;gt; K4[执行模型前向/反向传播]
    
    %% Rollout流程
    N --&amp;gt; N1[RolloutWorker._poll_async]
    N1 --&amp;gt; N2[load_next_data]
    N2 --&amp;gt; N3[allocate_new_rollout]
    N3 --&amp;gt; N4[agent.collect_trajectory]
    N4 --&amp;gt; N5[env.step计算奖励]
    N5 --&amp;gt; N6[推送数据到训练端]
    
    %% 生成服务器流程
    L --&amp;gt; L1[GenerationServer._poll]
    L1 --&amp;gt; L2[启动SGLang子进程]
    L2 --&amp;gt; L3[处理生成请求]
    
    %% 生成服务器管理器
    M --&amp;gt; M1[GserverManager._poll]
    M1 --&amp;gt; M2[HTTP服务线程]
    M2 --&amp;gt; M3[请求调度和权重更新]
    
    %% 数据流
    N6 --&amp;gt; O[stream_dataset.py]
    O --&amp;gt; J4
    
    %% 异步通信
    J4 -.-&amp;gt;|异步请求| K2
    N3 -.-&amp;gt;|HTTP请求| M2
    M2 -.-&amp;gt;|调度请求| L3
    
    %% 权重更新
    K4 --&amp;gt; P[参数更新]
    P --&amp;gt; Q[权重同步]
    Q --&amp;gt; M3
    M3 --&amp;gt; R[更新生成服务器权重]
    
    style A fill:#e1f5fe
    style J fill:#f3e5f5
    style K fill:#e8f5e8
    style L fill:#fff3e0
    style M fill:#fce4ec
    style N fill:#f1f8e9
&lt;/pre&gt;

&lt;h3 id=&#34;用户入口到配置解析&#34;&gt;用户入口到配置解析&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;examples/run_async_ppo.sh&lt;/code&gt; → &lt;code&gt;training/main_async_ppo.py&lt;/code&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
