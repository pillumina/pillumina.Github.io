<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Vllm-Ascend on CctoctoFX</title>
    <link>https://pillumina.github.io/series/vllm-ascend/</link>
    <description>Recent content in Vllm-Ascend on CctoctoFX</description>
    <image>
      <title>CctoctoFX</title>
      <url>https://pillumina.github.io/imgs/icon_head.png</url>
      <link>https://pillumina.github.io/imgs/icon_head.png</link>
    </image>
    <generator>Hugo -- 0.148.2</generator>
    <language>en</language>
    <lastBuildDate>Sat, 20 Sep 2025 11:30:12 +0800</lastBuildDate>
    <atom:link href="https://pillumina.github.io/series/vllm-ascend/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[vLLM-Ascend] MC2技术深度解析：从MoE架构到通信融合优化</title>
      <link>https://pillumina.github.io/posts/aiinfra/13-vllmascend-mc2/</link>
      <pubDate>Sat, 20 Sep 2025 11:30:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/13-vllmascend-mc2/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;源码分析依赖&lt;code&gt;vllm-ascend&lt;/code&gt;在&lt;code&gt;2025/9/20&lt;/code&gt;号的&lt;code&gt;main&lt;/code&gt;分支，阅读请注意时效性。&lt;br&gt;
阅读建议:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;了解MoE基本架构和关键推导&lt;/li&gt;
&lt;li&gt;初步了解集合通信各原语的含义&lt;/li&gt;
&lt;li&gt;对通算掩盖这类性能优化有基础的了解&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;
&lt;p&gt;MC2（Merged Compute and Communication）是vLLM Ascend项目中针对昇腾NPU优化的核心技术，专门解决MoE（Mixture of Experts）模型在专家并行推理中的通信瓶颈问题。本文档从MoE架构基础出发，深入分析MC2的设计原理、技术实现和性能优化。&lt;/p&gt;
&lt;h2 id=&#34;1-moe架构基础与挑战&#34;&gt;1. MoE架构基础与挑战&lt;/h2&gt;
&lt;h3 id=&#34;11-moe模型基本原理&#34;&gt;1.1 MoE模型基本原理&lt;/h3&gt;
&lt;h4 id=&#34;111-什么是moe&#34;&gt;1.1.1 什么是MoE？&lt;/h4&gt;
&lt;p&gt;**MoE(Mixture of Experts)**是一种神经网络架构，通过将模型参数分散到多个&amp;quot;专家&amp;quot;网络中，根据输入动态选择部分专家进行计算。这种架构在保持高模型容量的同时，降低了计算复杂度。&lt;/p&gt;
&lt;h4 id=&#34;112-moe的数学表达&#34;&gt;1.1.2 MoE的数学表达&lt;/h4&gt;
&lt;p&gt;给定输入 $\mathbf{x} \in \mathbb{R}^{d}$，MoE层的输出可以表示为：&lt;/p&gt;
$$
\mathbf{y} = \text{MoE}(\mathbf{x}) = \sum_{i=1}^{N} g_i(\mathbf{x}) \cdot E_i(\mathbf{x})
$$&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$N$ 是专家总数&lt;/li&gt;
&lt;li&gt;$E_i(\cdot)$ 是第 $i$ 个专家网络&lt;/li&gt;
&lt;li&gt;$g_i(\mathbf{x})$ 是门控网络对专家 $i$ 的权重&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;113-稀疏激活机制&#34;&gt;1.1.3 稀疏激活机制&lt;/h4&gt;
&lt;p&gt;为了提高效率，MoE通常采用稀疏激活机制，只选择 Top-K 个专家：&lt;/p&gt;
$$
\mathbf{y} = \sum_{i \in \text{Top-K}(\mathbf{x})} \frac{g_i(\mathbf{x})}{\sum_{j \in \text{Top-K}(\mathbf{x})} g_j(\mathbf{x})} \cdot E_i(\mathbf{x})
$$&lt;p&gt;&lt;a href=&#34;https://pillumina.github.io/posts/aiinfra/13-vllmascend-mc2/#a1-moe%e8%be%93%e5%87%ba%e5%85%ac%e5%bc%8f%e6%8e%a8%e5%af%bc&#34;&gt;详见附录A.1 MoE输出公式推导&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其中 $\text{Top-K}(\mathbf{x})$ 表示根据门控权重选择的 Top-K 个专家索引。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
