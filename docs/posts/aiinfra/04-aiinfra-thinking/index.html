<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AI Infra：颠覆性创新，还是经典工程范式的华丽转身？ | CctoctoFX</title><meta name=keywords content="AIInfra"><meta name=description content="近期看到一些关于传统基础设施（Traditional Infrastructure）与人工智能基础设施（AI Infrastructure，尤其大模型领域）差异的评论。其核心观点直指两者间的巨大鸿沟：许多精于网络、计算、存储等传统领域的工程师，在面对GPU集群、KV Cache管理、3D并行等全新概念时，常感过往经验难以直接套用，甚至产生踏入一个全然不同技术体系的“割裂感”。
这些看法颇具代表性，精准捕捉了工程师初探AI Infra时的普遍印象：陌生、高门槛、范式迥异。本文旨在分享我对此的一些初步思考：AI Infra究竟是颠覆传统的全新体系，抑或是既有Infra经验在智能时代的一次深度演化？

（免责声明：本文纯属个人观点，旨在抛砖引玉，欢迎指正谬误！）
我的核心论点：AI Infra并非平地起高楼，它实质上是传统Infra工程智慧在新场景下的重构与系统性延展。
表象差异：新术语与新挑战带来的“视觉冲击”
乍看之下，AI Infra与传统Infra确实分野明显：

核心任务不同： 传统Infra聚焦于处理海量Web请求（毫秒级、无状态）、保障数据持久化存储、实现分布式服务协调。而AI Infra（尤以大模型为甚）则围绕GPU驱动的模型训练/推理、KV Cache的高效管理、百亿/千亿级参数的分布式执行框架展开。
请求形态迥异： Web请求追求瞬时响应（毫秒级）、天然无状态。大模型（LLM）推理则常承载持续的会话交互（秒级乃至更长，随上下文窗口扩展而递增），需动态维护细粒度的Token级状态（KV Cache）。
技术栈迭代： 熟悉的Kubernetes + Docker堆栈旁，涌现出GPU硬件抽象、vLLM、DeepSpeed、FlashAttention、Triton、NCCL等专为AI设计、名号“高深”的组件。

由此观之，认为传统经验难以直接迁移，确有其表象依据。但这仅仅是“水面之上的冰山”，远非其底层基石。
本质共性：工程核心挑战的永恒回归
拨开“AI专属”的面纱，工程实践的核心命题依然如故：系统设计与资源调度的精妙艺术。 我们面临的，仍是那些传统Infra领域中反复锤炼的同类问题，只是约束条件和优化目标发生了变化：

资源调度： 核心资源从CPU/内存/磁盘IO，转向了更稀缺、更昂贵的GPU显存与算力。
负载处理： 承载对象从HTTP资源请求，变为密集的Prompt请求与大规模训练任务。
核心目标： 高效、稳定、低成本地协调跨节点资源的核心诉求丝毫未变。

概念的映射：经典范式的AI实践
这种延续性，清晰地体现在关键概念的对应关系上：

  
      
          传统 Infra 概念
          AI Infra 对应实践
          核心思想应用
      
  
  
      
          数据分片 (Data Sharding)
          数据并行 (Data Parallelism)
          数据集拆分，多副本并行处理
      
      
          负载均衡 (Load Balancer)
          MoE Router (Mixture of Experts)
          动态分配请求（Token）至专家网络，避免热点
      
      
          操作系统分页 (OS Paging)
          vLLM KV Cache Paging
          虚拟化显存空间，高效管理请求状态
      
  

以vLLM为例： 其核心创新在于将操作系统经典的内存管理机制（分页、交换），创造性地应用于管理LLM推理中关键的KV Cache状态。它如同为LLM定制了一个“显存操作系统”，管理“进程”（推理请求）和“内存页”（KV Cache Blocks），极致优化昂贵显存的利用率。这绝非凭空创造，而是经典系统原理在特定约束下的卓越应用。"><meta name=author content="Me"><link rel=canonical href=https://pillumina.github.io/posts/aiinfra/04-aiinfra-thinking/><link crossorigin=anonymous href=/assets/css/stylesheet.9d388901283682bb45dd422fcaa0d0a2054a3c8ff47c9cc6b2baab15508b1b90.css integrity="sha256-nTiJASg2grtF3UIvyqDQogVKPI/0fJzGsrqrFVCLG5A=" rel="preload stylesheet" as=style><link rel=icon href=https://pillumina.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://pillumina.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://pillumina.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://pillumina.github.io/apple-touch-icon.png><link rel=mask-icon href=https://pillumina.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://pillumina.github.io/posts/aiinfra/04-aiinfra-thinking/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>(function(){function t(){return document.querySelector(".post-content")||document.querySelector(".post-single")||document.body}function n(e){return/\$\$[\s\S]+?\$\$|\\\(|\\\)|\\\[|\\\]/.test(e)}function s(e){if(window.__mathjaxLoaded)return;window.__mathjaxLoaded=!0,window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code","tt"],ignoreHtmlClass:"no-math"}};var t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js",t.defer=!0,t.onload=function(){window.MathJax&&window.MathJax.typesetPromise&&window.MathJax.typesetPromise([e]).catch(function(e){console.warn("MathJax typeset error",e)})},document.head.appendChild(t)}function e(){try{if(typeof renderMathInElement=="function"){const e=t();renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,strict:!1,trust:!0,ignoredTags:["script","noscript","style","textarea","pre","code","tt"],ignoredClasses:["no-math"],macros:{"\\boldsymbol":"\\mathbf{#1}","\\bm":"\\mathbf{#1}"}}),setTimeout(function(){n(e.innerHTML)&&s(e)},200)}}catch(e){console.warn("KaTeX render error:",e)}}document.addEventListener("DOMContentLoaded",function(){e(),setTimeout(e,200)}),window.addEventListener("load",function(){setTimeout(e,0)})})()</script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>const config={startOnLoad:!0,theme:"neutral",themeVariables:{lineColor:"#0f0f0f"},flowchart:{useMaxWidth:!1,htmlLabels:!0}};mermaid.initialize(config),window.onload=()=>{window.mermaid.init(0[0],document.querySelectorAll(".language-mermaid"))}</script><link rel=stylesheet href=/css/custom.min.bda7229c4269a242639e058fb11a4782f02f8d77071ba16609befee67cc41c49.css integrity="sha256-vacinEJpokJjngWPsRpHgvAvjXcHG6FmCb7+5nzEHEk="><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]"),n=document.querySelectorAll(".toc a");if(t.length===0||n.length===0)return;const s={};t.forEach(e=>{s[e.id]=e.offsetTop});function i(){const t=window.scrollY+100;let e="";for(const[n,o]of Object.entries(s))if(t>=o)e=n;else break;return e}function o(){const e=i();if(n.forEach(e=>{e.classList.remove("active")}),e){const t=document.querySelector(`.toc a[href="#${e}"]`);t&&t.classList.add("active")}}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){o(),e=!1}),e=!0)}),o()})</script><meta property="og:url" content="https://pillumina.github.io/posts/aiinfra/04-aiinfra-thinking/"><meta property="og:site_name" content="CctoctoFX"><meta property="og:title" content="AI Infra：颠覆性创新，还是经典工程范式的华丽转身？"><meta property="og:description" content="近期看到一些关于传统基础设施（Traditional Infrastructure）与人工智能基础设施（AI Infrastructure，尤其大模型领域）差异的评论。其核心观点直指两者间的巨大鸿沟：许多精于网络、计算、存储等传统领域的工程师，在面对GPU集群、KV Cache管理、3D并行等全新概念时，常感过往经验难以直接套用，甚至产生踏入一个全然不同技术体系的“割裂感”。
这些看法颇具代表性，精准捕捉了工程师初探AI Infra时的普遍印象：陌生、高门槛、范式迥异。本文旨在分享我对此的一些初步思考：AI Infra究竟是颠覆传统的全新体系，抑或是既有Infra经验在智能时代的一次深度演化？
（免责声明：本文纯属个人观点，旨在抛砖引玉，欢迎指正谬误！）
我的核心论点：AI Infra并非平地起高楼，它实质上是传统Infra工程智慧在新场景下的重构与系统性延展。
表象差异：新术语与新挑战带来的“视觉冲击” 乍看之下，AI Infra与传统Infra确实分野明显：
核心任务不同： 传统Infra聚焦于处理海量Web请求（毫秒级、无状态）、保障数据持久化存储、实现分布式服务协调。而AI Infra（尤以大模型为甚）则围绕GPU驱动的模型训练/推理、KV Cache的高效管理、百亿/千亿级参数的分布式执行框架展开。 请求形态迥异： Web请求追求瞬时响应（毫秒级）、天然无状态。大模型（LLM）推理则常承载持续的会话交互（秒级乃至更长，随上下文窗口扩展而递增），需动态维护细粒度的Token级状态（KV Cache）。 技术栈迭代： 熟悉的Kubernetes + Docker堆栈旁，涌现出GPU硬件抽象、vLLM、DeepSpeed、FlashAttention、Triton、NCCL等专为AI设计、名号“高深”的组件。 由此观之，认为传统经验难以直接迁移，确有其表象依据。但这仅仅是“水面之上的冰山”，远非其底层基石。
本质共性：工程核心挑战的永恒回归 拨开“AI专属”的面纱，工程实践的核心命题依然如故：系统设计与资源调度的精妙艺术。 我们面临的，仍是那些传统Infra领域中反复锤炼的同类问题，只是约束条件和优化目标发生了变化：
资源调度： 核心资源从CPU/内存/磁盘IO，转向了更稀缺、更昂贵的GPU显存与算力。 负载处理： 承载对象从HTTP资源请求，变为密集的Prompt请求与大规模训练任务。 核心目标： 高效、稳定、低成本地协调跨节点资源的核心诉求丝毫未变。 概念的映射：经典范式的AI实践
这种延续性，清晰地体现在关键概念的对应关系上：
传统 Infra 概念 AI Infra 对应实践 核心思想应用 数据分片 (Data Sharding) 数据并行 (Data Parallelism) 数据集拆分，多副本并行处理 负载均衡 (Load Balancer) MoE Router (Mixture of Experts) 动态分配请求（Token）至专家网络，避免热点 操作系统分页 (OS Paging) vLLM KV Cache Paging 虚拟化显存空间，高效管理请求状态 以vLLM为例： 其核心创新在于将操作系统经典的内存管理机制（分页、交换），创造性地应用于管理LLM推理中关键的KV Cache状态。它如同为LLM定制了一个“显存操作系统”，管理“进程”（推理请求）和“内存页”（KV Cache Blocks），极致优化昂贵显存的利用率。这绝非凭空创造，而是经典系统原理在特定约束下的卓越应用。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-01T10:05:12+08:00"><meta property="article:modified_time" content="2025-08-01T10:05:12+08:00"><meta property="article:tag" content="AIInfra"><meta property="og:image" content="https://pillumina.github.io/imgs/icon_head.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://pillumina.github.io/imgs/icon_head.png"><meta name=twitter:title content="AI Infra：颠覆性创新，还是经典工程范式的华丽转身？"><meta name=twitter:description content="近期看到一些关于传统基础设施（Traditional Infrastructure）与人工智能基础设施（AI Infrastructure，尤其大模型领域）差异的评论。其核心观点直指两者间的巨大鸿沟：许多精于网络、计算、存储等传统领域的工程师，在面对GPU集群、KV Cache管理、3D并行等全新概念时，常感过往经验难以直接套用，甚至产生踏入一个全然不同技术体系的“割裂感”。
这些看法颇具代表性，精准捕捉了工程师初探AI Infra时的普遍印象：陌生、高门槛、范式迥异。本文旨在分享我对此的一些初步思考：AI Infra究竟是颠覆传统的全新体系，抑或是既有Infra经验在智能时代的一次深度演化？

（免责声明：本文纯属个人观点，旨在抛砖引玉，欢迎指正谬误！）
我的核心论点：AI Infra并非平地起高楼，它实质上是传统Infra工程智慧在新场景下的重构与系统性延展。
表象差异：新术语与新挑战带来的“视觉冲击”
乍看之下，AI Infra与传统Infra确实分野明显：

核心任务不同： 传统Infra聚焦于处理海量Web请求（毫秒级、无状态）、保障数据持久化存储、实现分布式服务协调。而AI Infra（尤以大模型为甚）则围绕GPU驱动的模型训练/推理、KV Cache的高效管理、百亿/千亿级参数的分布式执行框架展开。
请求形态迥异： Web请求追求瞬时响应（毫秒级）、天然无状态。大模型（LLM）推理则常承载持续的会话交互（秒级乃至更长，随上下文窗口扩展而递增），需动态维护细粒度的Token级状态（KV Cache）。
技术栈迭代： 熟悉的Kubernetes + Docker堆栈旁，涌现出GPU硬件抽象、vLLM、DeepSpeed、FlashAttention、Triton、NCCL等专为AI设计、名号“高深”的组件。

由此观之，认为传统经验难以直接迁移，确有其表象依据。但这仅仅是“水面之上的冰山”，远非其底层基石。
本质共性：工程核心挑战的永恒回归
拨开“AI专属”的面纱，工程实践的核心命题依然如故：系统设计与资源调度的精妙艺术。 我们面临的，仍是那些传统Infra领域中反复锤炼的同类问题，只是约束条件和优化目标发生了变化：

资源调度： 核心资源从CPU/内存/磁盘IO，转向了更稀缺、更昂贵的GPU显存与算力。
负载处理： 承载对象从HTTP资源请求，变为密集的Prompt请求与大规模训练任务。
核心目标： 高效、稳定、低成本地协调跨节点资源的核心诉求丝毫未变。

概念的映射：经典范式的AI实践
这种延续性，清晰地体现在关键概念的对应关系上：

  
      
          传统 Infra 概念
          AI Infra 对应实践
          核心思想应用
      
  
  
      
          数据分片 (Data Sharding)
          数据并行 (Data Parallelism)
          数据集拆分，多副本并行处理
      
      
          负载均衡 (Load Balancer)
          MoE Router (Mixture of Experts)
          动态分配请求（Token）至专家网络，避免热点
      
      
          操作系统分页 (OS Paging)
          vLLM KV Cache Paging
          虚拟化显存空间，高效管理请求状态
      
  

以vLLM为例： 其核心创新在于将操作系统经典的内存管理机制（分页、交换），创造性地应用于管理LLM推理中关键的KV Cache状态。它如同为LLM定制了一个“显存操作系统”，管理“进程”（推理请求）和“内存页”（KV Cache Blocks），极致优化昂贵显存的利用率。这绝非凭空创造，而是经典系统原理在特定约束下的卓越应用。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://pillumina.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI Infra","item":"https://pillumina.github.io/posts/aiinfra/"},{"@type":"ListItem","position":3,"name":"AI Infra：颠覆性创新，还是经典工程范式的华丽转身？","item":"https://pillumina.github.io/posts/aiinfra/04-aiinfra-thinking/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AI Infra：颠覆性创新，还是经典工程范式的华丽转身？","name":"AI Infra：颠覆性创新，还是经典工程范式的华丽转身？","description":"近期看到一些关于传统基础设施（Traditional Infrastructure）与人工智能基础设施（AI Infrastructure，尤其大模型领域）差异的评论。其核心观点直指两者间的巨大鸿沟：许多精于网络、计算、存储等传统领域的工程师，在面对GPU集群、KV Cache管理、3D并行等全新概念时，常感过往经验难以直接套用，甚至产生踏入一个全然不同技术体系的“割裂感”。\n这些看法颇具代表性，精准捕捉了工程师初探AI Infra时的普遍印象：陌生、高门槛、范式迥异。本文旨在分享我对此的一些初步思考：AI Infra究竟是颠覆传统的全新体系，抑或是既有Infra经验在智能时代的一次深度演化？\n（免责声明：本文纯属个人观点，旨在抛砖引玉，欢迎指正谬误！）\n我的核心论点：AI Infra并非平地起高楼，它实质上是传统Infra工程智慧在新场景下的重构与系统性延展。\n表象差异：新术语与新挑战带来的“视觉冲击” 乍看之下，AI Infra与传统Infra确实分野明显：\n核心任务不同： 传统Infra聚焦于处理海量Web请求（毫秒级、无状态）、保障数据持久化存储、实现分布式服务协调。而AI Infra（尤以大模型为甚）则围绕GPU驱动的模型训练/推理、KV Cache的高效管理、百亿/千亿级参数的分布式执行框架展开。 请求形态迥异： Web请求追求瞬时响应（毫秒级）、天然无状态。大模型（LLM）推理则常承载持续的会话交互（秒级乃至更长，随上下文窗口扩展而递增），需动态维护细粒度的Token级状态（KV Cache）。 技术栈迭代： 熟悉的Kubernetes + Docker堆栈旁，涌现出GPU硬件抽象、vLLM、DeepSpeed、FlashAttention、Triton、NCCL等专为AI设计、名号“高深”的组件。 由此观之，认为传统经验难以直接迁移，确有其表象依据。但这仅仅是“水面之上的冰山”，远非其底层基石。\n本质共性：工程核心挑战的永恒回归 拨开“AI专属”的面纱，工程实践的核心命题依然如故：系统设计与资源调度的精妙艺术。 我们面临的，仍是那些传统Infra领域中反复锤炼的同类问题，只是约束条件和优化目标发生了变化：\n资源调度： 核心资源从CPU/内存/磁盘IO，转向了更稀缺、更昂贵的GPU显存与算力。 负载处理： 承载对象从HTTP资源请求，变为密集的Prompt请求与大规模训练任务。 核心目标： 高效、稳定、低成本地协调跨节点资源的核心诉求丝毫未变。 概念的映射：经典范式的AI实践\n这种延续性，清晰地体现在关键概念的对应关系上：\n传统 Infra 概念 AI Infra 对应实践 核心思想应用 数据分片 (Data Sharding) 数据并行 (Data Parallelism) 数据集拆分，多副本并行处理 负载均衡 (Load Balancer) MoE Router (Mixture of Experts) 动态分配请求（Token）至专家网络，避免热点 操作系统分页 (OS Paging) vLLM KV Cache Paging 虚拟化显存空间，高效管理请求状态 以vLLM为例： 其核心创新在于将操作系统经典的内存管理机制（分页、交换），创造性地应用于管理LLM推理中关键的KV Cache状态。它如同为LLM定制了一个“显存操作系统”，管理“进程”（推理请求）和“内存页”（KV Cache Blocks），极致优化昂贵显存的利用率。这绝非凭空创造，而是经典系统原理在特定约束下的卓越应用。\n","keywords":["AIInfra"],"articleBody":"近期看到一些关于传统基础设施（Traditional Infrastructure）与人工智能基础设施（AI Infrastructure，尤其大模型领域）差异的评论。其核心观点直指两者间的巨大鸿沟：许多精于网络、计算、存储等传统领域的工程师，在面对GPU集群、KV Cache管理、3D并行等全新概念时，常感过往经验难以直接套用，甚至产生踏入一个全然不同技术体系的“割裂感”。\n这些看法颇具代表性，精准捕捉了工程师初探AI Infra时的普遍印象：陌生、高门槛、范式迥异。本文旨在分享我对此的一些初步思考：AI Infra究竟是颠覆传统的全新体系，抑或是既有Infra经验在智能时代的一次深度演化？\n（免责声明：本文纯属个人观点，旨在抛砖引玉，欢迎指正谬误！）\n我的核心论点：AI Infra并非平地起高楼，它实质上是传统Infra工程智慧在新场景下的重构与系统性延展。\n表象差异：新术语与新挑战带来的“视觉冲击” 乍看之下，AI Infra与传统Infra确实分野明显：\n核心任务不同： 传统Infra聚焦于处理海量Web请求（毫秒级、无状态）、保障数据持久化存储、实现分布式服务协调。而AI Infra（尤以大模型为甚）则围绕GPU驱动的模型训练/推理、KV Cache的高效管理、百亿/千亿级参数的分布式执行框架展开。 请求形态迥异： Web请求追求瞬时响应（毫秒级）、天然无状态。大模型（LLM）推理则常承载持续的会话交互（秒级乃至更长，随上下文窗口扩展而递增），需动态维护细粒度的Token级状态（KV Cache）。 技术栈迭代： 熟悉的Kubernetes + Docker堆栈旁，涌现出GPU硬件抽象、vLLM、DeepSpeed、FlashAttention、Triton、NCCL等专为AI设计、名号“高深”的组件。 由此观之，认为传统经验难以直接迁移，确有其表象依据。但这仅仅是“水面之上的冰山”，远非其底层基石。\n本质共性：工程核心挑战的永恒回归 拨开“AI专属”的面纱，工程实践的核心命题依然如故：系统设计与资源调度的精妙艺术。 我们面临的，仍是那些传统Infra领域中反复锤炼的同类问题，只是约束条件和优化目标发生了变化：\n资源调度： 核心资源从CPU/内存/磁盘IO，转向了更稀缺、更昂贵的GPU显存与算力。 负载处理： 承载对象从HTTP资源请求，变为密集的Prompt请求与大规模训练任务。 核心目标： 高效、稳定、低成本地协调跨节点资源的核心诉求丝毫未变。 概念的映射：经典范式的AI实践\n这种延续性，清晰地体现在关键概念的对应关系上：\n传统 Infra 概念 AI Infra 对应实践 核心思想应用 数据分片 (Data Sharding) 数据并行 (Data Parallelism) 数据集拆分，多副本并行处理 负载均衡 (Load Balancer) MoE Router (Mixture of Experts) 动态分配请求（Token）至专家网络，避免热点 操作系统分页 (OS Paging) vLLM KV Cache Paging 虚拟化显存空间，高效管理请求状态 以vLLM为例： 其核心创新在于将操作系统经典的内存管理机制（分页、交换），创造性地应用于管理LLM推理中关键的KV Cache状态。它如同为LLM定制了一个“显存操作系统”，管理“进程”（推理请求）和“内存页”（KV Cache Blocks），极致优化昂贵显存的利用率。这绝非凭空创造，而是经典系统原理在特定约束下的卓越应用。\n基础设施的“三座大山”：Scaling, Sharding, Copying 所有复杂分布式系统的底层挑战，最终都绕不开这三个永恒主题。AI Infra的“新”，在于其前所未有的规模、独特的瓶颈（显存/通信）和严苛的成本压力，使得这些经典挑战呈现出新的面貌与更高的解决难度：\n扩展性 (Scaling)：\n传统： 水平扩展服务器、容器化部署、负载均衡分散流量。 AI： 并行化策略成为生命线——通过数据并行 (Data Parallelism)、模型并行 (Model/Tensor Parallelism)、流水线并行 (Pipeline Parallelism) 的精细组合，在多GPU甚至多机集群上分布和执行庞然大物般的模型负载，支撑超大规模训练与高并发推理。挑战在于通信开销与计算负载的极致平衡。 分片 (Sharding)：\n传统： 数据库按主键/范围分区，提升访问吞吐。 AI： 分片对象变为模型参数、梯度、优化器状态、激活值以及核心的KV Cache。Tensor Parallelism将单层网络切片分布，KV Paging将长上下文状态分块管理。分片策略直接决定了分布式执行的可行性与效率。显存限制使其成为刚需而非优化项。 复制 (Copying)：\n传统： 数据库副本同步、缓存预热、Kafka副本保障可靠性。 AI： 复制代价被指数级放大。例如，Data Parallelism需在各GPU复制完整模型参数进行前向/反向传播（催生了ZeRO等优化技术来_分片_而非全复制参数/梯度/优化器状态）。高效的跨GPU/跨节点通信（依赖NCCL, RDMA） 成为关键瓶颈。带宽与延迟的优化是性能命门。 可见，核心挑战的本质——如何高效、稳健、低成本地协调跨异构节点的资源——从未改变。AI场景的独特约束（显存昂贵、模型巨大、上下文长、通信密集）只是将这些挑战推向了更极致、更脆弱的境地，要求更精巧的工程策略。\n工程之本：量化思维与瓶颈洞察 (Jeff Dean的启示) Google的Jeff Dean提出的“程序员应知的延迟数据”，深刻揭示了量化思维（Quantitative Reasoning）在系统设计中的基石地位。掌握关键操作的近似延迟数量级（如L1缓存访问 vs 内存访问 vs 网络传输 vs 磁盘寻道），是：\n事前设计的关键输入： 用于估算训练时间、推理吞吐、Token延迟，指导架构选型与容量规划。 事后诊断的利器： 当性能未达预期，理解这些基准能快速定位瓶颈所在——是计算(Compute-Bound)、显存带宽(Memory-Bound) 还是通信(Communication-Bound)？ 在AI Infra的映射：\nToken-level KV Cache访问 ≈ GPU HBM全局显存访问延迟 多GPU通信 (NCCL AllReduce等) ≈ 集群内高速网络通信延迟/带宽 跨物理服务器调度 ≈ 数据中心网络延迟 量化思维的普适性： 如同算法复杂度分析（O(n) vs O(n²)），数据库查询计划成本估算，或传统Infra中的延迟认知，在AI Infra中构建并运用类似的心智模型（估算计算FLOPs、显存占用、通信量及对应延迟/带宽）同样至关重要。这是区分优秀工程师的关键——将模糊的“慢”转化为可量化的瓶颈点。\n案例反思： Meta训练LLaMA模型时，GPU故障频发（据传约每几十分钟一次）。这凸显了在极端复杂、长周期运行的AI系统中，强大的日志(logging)、错误追踪(tracing)和性能剖析(profiling)工具对于系统稳定性与可观测性的极端重要性——这正是传统大规模分布式系统运维经验的直接延伸。\n延伸推荐： 李博杰的博客《4090 适合做 AI 训练还是推理？》是量化思维的绝佳范例。它基于具体的数据（模型尺寸、显存需求、通信开销、计算能力）进行严格估算，清晰论证了4090在训练与推理场景下的适用边界。这种基于数据的决策能力，是Infra工程师的核心素养。\n结语：褪去神秘，回归工程本质 当前关于AI Infra的讨论，有时不免笼罩着一层“神秘化”的光环。诚然，LLM的崛起带来了前所未有的模型形态（千亿参数）、交互范式（长会话）和资源瓶颈（显存墙、通信墙）。然而，穿透这些表象，工程要解决的核心命题始终稳固：\n最大化资源利用率 (Optimize Resource Utilization) -\u003e 降低成本 保障服务稳定性 (Ensure Service Stability) 提升吞吐与响应能力 (Maximize Throughput \u0026 Minimize Latency) 这些命题，我们在构建Web服务、数据库、分布式系统的漫长历程中，早已反复求解。AI Infra的革新之处，在于我们需要在GPU主导的计算范式、大模型特有的状态管理（如KV Cache）和高并发推理请求的约束下，对经典解决方案进行系统性重构与深度优化。\n因此，AI Infra的高门槛，其核心不在于对神经网络理论的精通程度，而在于能否将既有的工程能力——系统设计思维、量化分析、问题拆解、性能优化、稳定性保障——无缝迁移并创造性应用于这一充满活力的新领域。\n熟悉网络通信？ 你会发现NCCL的Ring AllReduce拓扑与高性能集群通信设计异曲同工。 深谙缓存与OS内存管理？ KV Cache的重要性及其管理策略（如vLLM）会令你倍感亲切。 构建过服务调度器？ Dynamic Batching 本质上就是经典的流水线并发与批处理思想的再现。 我愈发坚信，AI Infra是传统Infra知识体系在新范式下的深度融合与卓越拓展。它是对旧有挑战在全新尺度与约束下的重新表述（Rephrasing）。\n真正具备竞争力的AI Infra工程师，绝非仅能调参或运行训练/推理脚本，而是那些深刻理解底层系统原理，并能将其与模型特性及业务需求融会贯通的人。这种思维范式的转换（Shift of Thinking）颇具挑战，但一旦建立起传统Infra \u003c-\u003e AI Infra 的概念映射，便会豁然开朗：那些看似“毫不相干”的传统经验，其底层逻辑往往换汤不换药。深厚的传统Infra功底，非但不是累赘，反而是驰骋AI Infra疆域的宝贵基石与独特优势。\n技术浪潮奔涌不息，工程智慧历久弥新。作为从业者，需要对技术有更为本质的认识和了解，才能游刃有余拥抱变化。\n","wordCount":"200","inLanguage":"en","image":"https://pillumina.github.io/imgs/icon_head.png","datePublished":"2025-08-01T10:05:12+08:00","dateModified":"2025-08-01T10:05:12+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://pillumina.github.io/posts/aiinfra/04-aiinfra-thinking/"},"publisher":{"@type":"Organization","name":"CctoctoFX","logo":{"@type":"ImageObject","url":"https://pillumina.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://pillumina.github.io/ accesskey=h title="CctoctoFX (Alt + H)"><img src=https://pillumina.github.io/apple-touch-icon.png alt aria-label=logo height=30>CctoctoFX</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://pillumina.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://pillumina.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://pillumina.github.io/posts/aiinfra/ title="AI Infra"><span>AI Infra</span></a></li><li><a href=https://pillumina.github.io/posts/llmtheory/ title=Thoery><span>Thoery</span></a></li><li><a href=https://pillumina.github.io/posts/programming/ title=Programming><span>Programming</span></a></li><li><a href=https://pillumina.github.io/social/ title=Social><span>Social</span></a></li><li><a href=https://pillumina.github.io/open_courses/ title=Study><span>Study</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://pillumina.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/aiinfra/>AI Infra</a></div><h1 class="post-title entry-hint-parent">AI Infra：颠覆性创新，还是经典工程范式的华丽转身？</h1><div class=post-meta><span title='2025-08-01 10:05:12 +0800 CST'>August 1, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;200 words&nbsp;·&nbsp;Me</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#表象差异新术语与新挑战带来的视觉冲击>表象差异：新术语与新挑战带来的“视觉冲击”</a></li><li><a href=#本质共性工程核心挑战的永恒回归>本质共性：工程核心挑战的永恒回归</a></li><li><a href=#基础设施的三座大山scaling-sharding-copying>基础设施的“三座大山”：Scaling, Sharding, Copying</a></li><li><a href=#工程之本量化思维与瓶颈洞察-jeff-dean的启示>工程之本：量化思维与瓶颈洞察 (Jeff Dean的启示)</a></li><li><a href=#结语褪去神秘回归工程本质>结语：褪去神秘，回归工程本质</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>近期看到一些关于传统基础设施（Traditional Infrastructure）与人工智能基础设施（AI Infrastructure，尤其大模型领域）差异的评论。其核心观点直指两者间的巨大鸿沟：许多精于网络、计算、存储等传统领域的工程师，在面对GPU集群、KV Cache管理、3D并行等全新概念时，常感过往经验难以直接套用，甚至产生踏入一个全然不同技术体系的“割裂感”。</p><p>这些看法颇具代表性，精准捕捉了工程师初探AI Infra时的普遍印象：<strong>陌生、高门槛、范式迥异</strong>。本文旨在分享我对此的一些初步思考：AI Infra究竟是颠覆传统的全新体系，抑或是既有Infra经验在智能时代的一次深度演化？</p><blockquote><p>（<em>免责声明：本文纯属个人观点，旨在抛砖引玉，欢迎指正谬误！</em>）</p></blockquote><p><strong>我的核心论点：AI Infra并非平地起高楼，它实质上是传统Infra工程智慧在新场景下的重构与系统性延展。</strong></p><h3 id=表象差异新术语与新挑战带来的视觉冲击>表象差异：新术语与新挑战带来的“视觉冲击”<a hidden class=anchor aria-hidden=true href=#表象差异新术语与新挑战带来的视觉冲击>#</a></h3><p>乍看之下，AI Infra与传统Infra确实分野明显：</p><ol><li><strong>核心任务不同：</strong> 传统Infra聚焦于处理海量Web请求（毫秒级、无状态）、保障数据持久化存储、实现分布式服务协调。而AI Infra（尤以大模型为甚）则围绕<strong>GPU驱动的模型训练/推理</strong>、<strong>KV Cache的高效管理</strong>、<strong>百亿/千亿级参数的分布式执行框架</strong>展开。</li><li><strong>请求形态迥异：</strong> Web请求追求瞬时响应（毫秒级）、天然无状态。大模型（LLM）推理则常承载<strong>持续的会话交互</strong>（秒级乃至更长，随上下文窗口扩展而递增），需<strong>动态维护细粒度的Token级状态</strong>（KV Cache）。</li><li><strong>技术栈迭代：</strong> 熟悉的Kubernetes + Docker堆栈旁，涌现出GPU硬件抽象、vLLM、DeepSpeed、FlashAttention、Triton、NCCL等<strong>专为AI设计、名号“高深”的组件</strong>。</li></ol><p>由此观之，认为传统经验难以直接迁移，确有其表象依据。但这仅仅是“水面之上的冰山”，远非其底层基石。</p><h3 id=本质共性工程核心挑战的永恒回归>本质共性：工程核心挑战的永恒回归<a hidden class=anchor aria-hidden=true href=#本质共性工程核心挑战的永恒回归>#</a></h3><p><strong>拨开“AI专属”的面纱，工程实践的核心命题依然如故：系统设计与资源调度的精妙艺术。</strong> 我们面临的，仍是那些传统Infra领域中反复锤炼的同类问题，只是约束条件和优化目标发生了变化：</p><ul><li><strong>资源调度：</strong> 核心资源从CPU/内存/磁盘IO，<strong>转向了更稀缺、更昂贵的GPU显存与算力</strong>。</li><li><strong>负载处理：</strong> 承载对象从HTTP资源请求，<strong>变为密集的Prompt请求与大规模训练任务</strong>。</li><li><strong>核心目标：</strong> 高效、稳定、低成本地协调跨节点资源的核心诉求<strong>丝毫未变</strong>。</li></ul><p><strong>概念的映射：经典范式的AI实践</strong></p><p>这种延续性，清晰地体现在关键概念的对应关系上：</p><table><thead><tr><th>传统 Infra 概念</th><th>AI Infra 对应实践</th><th>核心思想应用</th></tr></thead><tbody><tr><td><strong>数据分片 (Data Sharding)</strong></td><td><strong>数据并行 (Data Parallelism)</strong></td><td>数据集拆分，多副本并行处理</td></tr><tr><td><strong>负载均衡 (Load Balancer)</strong></td><td><strong>MoE Router (Mixture of Experts)</strong></td><td>动态分配请求（Token）至专家网络，避免热点</td></tr><tr><td><strong>操作系统分页 (OS Paging)</strong></td><td><strong>vLLM KV Cache Paging</strong></td><td>虚拟化显存空间，高效管理请求状态</td></tr></tbody></table><p><strong>以vLLM为例：</strong> 其核心创新在于将<strong>操作系统经典的内存管理机制（分页、交换）</strong>，创造性地应用于管理LLM推理中关键的<strong>KV Cache状态</strong>。它如同为LLM定制了一个“显存操作系统”，管理“进程”（推理请求）和“内存页”（KV Cache Blocks），极致优化昂贵显存的利用率。这绝非凭空创造，而是<strong>经典系统原理在特定约束下的卓越应用</strong>。</p><h3 id=基础设施的三座大山scaling-sharding-copying>基础设施的“三座大山”：Scaling, Sharding, Copying<a hidden class=anchor aria-hidden=true href=#基础设施的三座大山scaling-sharding-copying>#</a></h3><p>所有复杂分布式系统的底层挑战，最终都绕不开这三个永恒主题。AI Infra的“新”，在于其<strong>前所未有的规模、独特的瓶颈（显存/通信）和严苛的成本压力</strong>，使得这些经典挑战呈现出新的面貌与更高的解决难度：</p><ol><li><p><strong>扩展性 (Scaling)：</strong></p><ul><li><strong>传统：</strong> 水平扩展服务器、容器化部署、负载均衡分散流量。</li><li><strong>AI：</strong> <strong>并行化策略成为生命线</strong>——通过<strong>数据并行 (Data Parallelism)</strong>、<strong>模型并行 (Model/Tensor Parallelism)</strong>、<strong>流水线并行 (Pipeline Parallelism)</strong> 的精细组合，在多GPU甚至多机集群上分布和执行庞然大物般的模型负载，支撑超大规模训练与高并发推理。挑战在于<strong>通信开销与计算负载的极致平衡</strong>。</li></ul></li><li><p><strong>分片 (Sharding)：</strong></p><ul><li><strong>传统：</strong> 数据库按主键/范围分区，提升访问吞吐。</li><li><strong>AI：</strong> 分片对象变为<strong>模型参数、梯度、优化器状态、激活值以及核心的KV Cache</strong>。Tensor Parallelism将单层网络切片分布，KV Paging将长上下文状态分块管理。分片策略直接决定了分布式执行的可行性与效率。<strong>显存限制使其成为刚需而非优化项</strong>。</li></ul></li><li><p><strong>复制 (Copying)：</strong></p><ul><li><strong>传统：</strong> 数据库副本同步、缓存预热、Kafka副本保障可靠性。</li><li><strong>AI：</strong> 复制代价被<strong>指数级放大</strong>。例如，Data Parallelism需在各GPU复制完整模型参数进行前向/反向传播（催生了ZeRO等优化技术来_分片_而非全复制参数/梯度/优化器状态）。高效的<strong>跨GPU/跨节点通信（依赖NCCL, RDMA）</strong> 成为关键瓶颈。<strong>带宽与延迟的优化是性能命门</strong>。</li></ul></li></ol><p><strong>可见，核心挑战的本质——如何高效、稳健、低成本地协调跨异构节点的资源——从未改变。AI场景的独特约束（显存昂贵、模型巨大、上下文长、通信密集）只是将这些挑战推向了更极致、更脆弱的境地，要求更精巧的工程策略。</strong></p><h3 id=工程之本量化思维与瓶颈洞察-jeff-dean的启示>工程之本：量化思维与瓶颈洞察 (Jeff Dean的启示)<a hidden class=anchor aria-hidden=true href=#工程之本量化思维与瓶颈洞察-jeff-dean的启示>#</a></h3><p>Google的Jeff Dean提出的“<a href=https://gist.github.com/jboner/2841832>程序员应知的延迟数据</a>”，深刻揭示了<strong>量化思维（Quantitative Reasoning）在系统设计中的基石地位</strong>。掌握关键操作的近似延迟数量级（如L1缓存访问 vs 内存访问 vs 网络传输 vs 磁盘寻道），是：</p><ol><li><strong>事前设计的关键输入：</strong> 用于估算训练时间、推理吞吐、Token延迟，指导架构选型与容量规划。</li><li><strong>事后诊断的利器：</strong> 当性能未达预期，理解这些基准能快速定位瓶颈所在——是<strong>计算(Compute-Bound)</strong>、<strong>显存带宽(Memory-Bound)</strong> 还是<strong>通信(Communication-Bound)</strong>？</li></ol><p><strong>在AI Infra的映射：</strong></p><ul><li><code>Token-level KV Cache访问</code> ≈ <code>GPU HBM全局显存访问延迟</code></li><li><code>多GPU通信 (NCCL AllReduce等)</code> ≈ <code>集群内高速网络通信延迟/带宽</code></li><li><code>跨物理服务器调度</code> ≈ <code>数据中心网络延迟</code></li></ul><p><strong>量化思维的普适性：</strong> 如同算法复杂度分析（O(n) vs O(n²)），数据库查询计划成本估算，或传统Infra中的延迟认知，<strong>在AI Infra中构建并运用类似的心智模型（估算计算FLOPs、显存占用、通信量及对应延迟/带宽）同样至关重要</strong>。这是区分优秀工程师的关键——<strong>将模糊的“慢”转化为可量化的瓶颈点。</strong></p><p><strong>案例反思：</strong> Meta训练LLaMA模型时，GPU故障频发（据传约每几十分钟一次）。这凸显了在极端复杂、长周期运行的AI系统中，<strong>强大的日志(logging)、错误追踪(tracing)和性能剖析(profiling)工具</strong>对于系统<strong>稳定性与可观测性</strong>的极端重要性——这正是传统大规模分布式系统运维经验的直接延伸。</p><p><strong>延伸推荐：</strong> 李博杰的博客《<a href=https://zhuanlan.zhihu.com/p/655402388>4090 适合做 AI 训练还是推理？</a>》是量化思维的绝佳范例。它基于具体的数据（模型尺寸、显存需求、通信开销、计算能力）进行严格估算，清晰论证了4090在训练与推理场景下的适用边界。<strong>这种基于数据的决策能力，是Infra工程师的核心素养。</strong></p><h3 id=结语褪去神秘回归工程本质>结语：褪去神秘，回归工程本质<a hidden class=anchor aria-hidden=true href=#结语褪去神秘回归工程本质>#</a></h3><p>当前关于AI Infra的讨论，有时不免笼罩着一层“神秘化”的光环。诚然，LLM的崛起带来了前所未有的模型形态（千亿参数）、交互范式（长会话）和资源瓶颈（显存墙、通信墙）。然而，<strong>穿透这些表象，工程要解决的核心命题始终稳固：</strong></p><ul><li><strong>最大化资源利用率 (Optimize Resource Utilization) -> 降低成本</strong></li><li><strong>保障服务稳定性 (Ensure Service Stability)</strong></li><li><strong>提升吞吐与响应能力 (Maximize Throughput & Minimize Latency)</strong></li></ul><p><strong>这些命题，我们在构建Web服务、数据库、分布式系统的漫长历程中，早已反复求解。AI Infra的革新之处，在于我们需要在GPU主导的计算范式、大模型特有的状态管理（如KV Cache）和高并发推理请求的约束下，对经典解决方案进行系统性重构与深度优化。</strong></p><p>因此，AI Infra的高门槛，其核心<strong>不在于对神经网络理论的精通程度</strong>，而在于<strong>能否将既有的工程能力——系统设计思维、量化分析、问题拆解、性能优化、稳定性保障——无缝迁移并创造性应用于这一充满活力的新领域</strong>。</p><ul><li><strong>熟悉网络通信？</strong> 你会发现NCCL的Ring AllReduce拓扑与高性能集群通信设计异曲同工。</li><li><strong>深谙缓存与OS内存管理？</strong> KV Cache的重要性及其管理策略（如vLLM）会令你倍感亲切。</li><li><strong>构建过服务调度器？</strong> Dynamic Batching 本质上就是经典的<strong>流水线并发与批处理</strong>思想的再现。</li></ul><p><strong>我愈发坚信，AI Infra是传统Infra知识体系在新范式下的深度融合与卓越拓展。它是对旧有挑战在全新尺度与约束下的重新表述（Rephrasing）。</strong></p><p>真正具备竞争力的AI Infra工程师，绝非仅能调参或运行训练/推理脚本，而是那些<strong>深刻理解底层系统原理，并能将其与模型特性及业务需求融会贯通的人</strong>。这种思维范式的转换（Shift of Thinking）颇具挑战，但一旦建立起<strong>传统Infra &lt;-> AI Infra</strong> 的概念映射，便会豁然开朗：那些看似“毫不相干”的传统经验，其底层逻辑往往换汤不换药。<strong>深厚的传统Infra功底，非但不是累赘，反而是驰骋AI Infra疆域的宝贵基石与独特优势。</strong></p><p>技术浪潮奔涌不息，工程智慧历久弥新。作为从业者，需要对技术有更为本质的认识和了解，才能游刃有余拥抱变化。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://pillumina.github.io/tags/aiinfra/>AIInfra</a></li></ul><nav class=paginav><a class=prev href=https://pillumina.github.io/posts/aiinfra/07-verl-multiturn-1/><span class=title>« Prev</span><br><span>[VeRL] Multi-Turn RL训练源码走读（1）</span>
</a><a class=next href=https://pillumina.github.io/posts/llmtheory/latex-test/><span class=title>Next »</span><br><span>LaTeX Test Page</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share AI Infra：颠覆性创新，还是经典工程范式的华丽转身？ on x" href="https://x.com/intent/tweet/?text=AI%20Infra%ef%bc%9a%e9%a2%a0%e8%a6%86%e6%80%a7%e5%88%9b%e6%96%b0%ef%bc%8c%e8%bf%98%e6%98%af%e7%bb%8f%e5%85%b8%e5%b7%a5%e7%a8%8b%e8%8c%83%e5%bc%8f%e7%9a%84%e5%8d%8e%e4%b8%bd%e8%bd%ac%e8%ba%ab%ef%bc%9f&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f04-aiinfra-thinking%2f&amp;hashtags=AIInfra"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI Infra：颠覆性创新，还是经典工程范式的华丽转身？ on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f04-aiinfra-thinking%2f&amp;title=AI%20Infra%ef%bc%9a%e9%a2%a0%e8%a6%86%e6%80%a7%e5%88%9b%e6%96%b0%ef%bc%8c%e8%bf%98%e6%98%af%e7%bb%8f%e5%85%b8%e5%b7%a5%e7%a8%8b%e8%8c%83%e5%bc%8f%e7%9a%84%e5%8d%8e%e4%b8%bd%e8%bd%ac%e8%ba%ab%ef%bc%9f&amp;summary=AI%20Infra%ef%bc%9a%e9%a2%a0%e8%a6%86%e6%80%a7%e5%88%9b%e6%96%b0%ef%bc%8c%e8%bf%98%e6%98%af%e7%bb%8f%e5%85%b8%e5%b7%a5%e7%a8%8b%e8%8c%83%e5%bc%8f%e7%9a%84%e5%8d%8e%e4%b8%bd%e8%bd%ac%e8%ba%ab%ef%bc%9f&amp;source=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f04-aiinfra-thinking%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI Infra：颠覆性创新，还是经典工程范式的华丽转身？ on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f04-aiinfra-thinking%2f&title=AI%20Infra%ef%bc%9a%e9%a2%a0%e8%a6%86%e6%80%a7%e5%88%9b%e6%96%b0%ef%bc%8c%e8%bf%98%e6%98%af%e7%bb%8f%e5%85%b8%e5%b7%a5%e7%a8%8b%e8%8c%83%e5%bc%8f%e7%9a%84%e5%8d%8e%e4%b8%bd%e8%bd%ac%e8%ba%ab%ef%bc%9f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI Infra：颠覆性创新，还是经典工程范式的华丽转身？ on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f04-aiinfra-thinking%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI Infra：颠覆性创新，还是经典工程范式的华丽转身？ on whatsapp" href="https://api.whatsapp.com/send?text=AI%20Infra%ef%bc%9a%e9%a2%a0%e8%a6%86%e6%80%a7%e5%88%9b%e6%96%b0%ef%bc%8c%e8%bf%98%e6%98%af%e7%bb%8f%e5%85%b8%e5%b7%a5%e7%a8%8b%e8%8c%83%e5%bc%8f%e7%9a%84%e5%8d%8e%e4%b8%bd%e8%bd%ac%e8%ba%ab%ef%bc%9f%20-%20https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f04-aiinfra-thinking%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI Infra：颠覆性创新，还是经典工程范式的华丽转身？ on telegram" href="https://telegram.me/share/url?text=AI%20Infra%ef%bc%9a%e9%a2%a0%e8%a6%86%e6%80%a7%e5%88%9b%e6%96%b0%ef%bc%8c%e8%bf%98%e6%98%af%e7%bb%8f%e5%85%b8%e5%b7%a5%e7%a8%8b%e8%8c%83%e5%bc%8f%e7%9a%84%e5%8d%8e%e4%b8%bd%e8%bd%ac%e8%ba%ab%ef%bc%9f&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f04-aiinfra-thinking%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share AI Infra：颠覆性创新，还是经典工程范式的华丽转身？ on ycombinator" href="https://news.ycombinator.com/submitlink?t=AI%20Infra%ef%bc%9a%e9%a2%a0%e8%a6%86%e6%80%a7%e5%88%9b%e6%96%b0%ef%bc%8c%e8%bf%98%e6%98%af%e7%bb%8f%e5%85%b8%e5%b7%a5%e7%a8%8b%e8%8c%83%e5%bc%8f%e7%9a%84%e5%8d%8e%e4%b8%bd%e8%bd%ac%e8%ba%ab%ef%bc%9f&u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f04-aiinfra-thinking%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul><div class=related-posts><div class=related-tags><h3>相关文章</h3><ul><li><a href=/posts/aiinfra/01-ascend-cloudmatrix/>昇腾超节点CloudMatrix384论文拆解</a>
<span class=meta>2025-08-07
· 6 min read
· Tags: AIInfra, ascend</span></li></ul></div></div></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://pillumina.github.io/>CctoctoFX</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div class=reading-progress-bar></div><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelector(".reading-progress-bar");if(!t)return;const n=document.querySelector(".post-single");if(!n)return;function s(){const e=n.getBoundingClientRect(),s=e.height,o=window.innerHeight,i=window.scrollY||window.pageYOffset,a=i/(s-o)*100;t.style.width=`${Math.min(100,Math.max(0,a))}%`}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){s(),e=!1}),e=!0)}),s()}),document.addEventListener("DOMContentLoaded",function(){mediumZoom("article img:not(.nozoom)",{margin:24,background:"var(--theme)",scrollOffset:0})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>