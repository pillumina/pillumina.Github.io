<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[SGLang] 后端代码速览 | CctoctoFX</title><meta name=keywords content="inference,sglang"><meta name=description content="本文档为开发者提供 SGLang 后端代码的代码梳理，按照一个请求从输入到最后输出的顺序进行讲解。下图简要介绍了这一流程：

    

具体而言，请求的处理过程如下：


用户启动 Server ，初始化 FastAPI App、TokenizerManager、DetokenizerManager 和 Scheduler，每个组件运行各自的无限事件循环（infinite event loop）。


用户向 FastAPI Server 发送 /v1/chat/completions 请求，Server 通过 v1_chat_completions endpoint 将请求转发到 TokenizerManager。


v1_chat_completions 函数将请求转换为 ChatCompletionRequest，再转换为 GenerateReqInput，并调用 TokenizerManager 的 generate_request 方法。


TokenizerManager 对请求进行 tokenization，并以 Python 对象（pyobj）形式将其转发给 Scheduler，同时调用 TokenizerManager 的 _wait_one_response 方法。


Scheduler 在事件循环 event_loop_normal 中处理请求：

Scheduler 通过 recv_requests 接收请求，调用 process_input_requests 处理输入，通过 handle_generate_request 管理生成请求的逻辑，并将其加入 waiting_queue。
从 waiting_queue 中，Scheduler 使用 get_next_batch_to_run 为即将处理的请求创建 ScheduleBatch。
Scheduler 执行 run_batch 函数，将 ScheduleBatch 转换为 ModelWorkerBatch。
Scheduler 调用 TpModelWorker 的 forward_batch_generation，等待 logits_output 和 next_token_ids。
TpModelWorker 初始化 ForwardBatch，将其转发至 ModelRunner，并等待 logits_output。
ModelRunner 处理 ForwardBatch，调用 forward_extend 执行模型的前向计算（forward pass）。
模型通过 AttentionBackend 加速生成 logits，返回给 ModelRunner，进而返回给 TpModelWorker。
TpModelWorker 从 ModelRunner 接收 logits_output，调用 ModelRunner 的 sample 方法生成 next_token_ids，并将其发送回 Scheduler。
Scheduler 通过 process_batch_result 处理批次结果，使用 tree_cache.cache_finished_req(req) 缓存请求，并通过 check_finished 验证完成状态。对于未完成的请求，Scheduler 继续其事件循环，直到这个请求满足结束条件；对于已完成的请求，则转发到 Scheduler 的 stream_output。
在 stream_output 函数中，Scheduler 处理输出，将其包装成 BatchTokenIDOut，并发送给 DetokenizerManager。



DetokenizerManager 在其事件循环中接收 BatchTokenIDOut，处理后生成 BatchStrOut 并返回给 TokenizerManager。"><meta name=author content="Me"><link rel=canonical href=https://pillumina.github.io/posts/aiinfra/06-sglang-backend/><link crossorigin=anonymous href=/assets/css/stylesheet.9d388901283682bb45dd422fcaa0d0a2054a3c8ff47c9cc6b2baab15508b1b90.css integrity="sha256-nTiJASg2grtF3UIvyqDQogVKPI/0fJzGsrqrFVCLG5A=" rel="preload stylesheet" as=style><link rel=icon href=https://pillumina.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://pillumina.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://pillumina.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://pillumina.github.io/apple-touch-icon.png><link rel=mask-icon href=https://pillumina.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://pillumina.github.io/posts/aiinfra/06-sglang-backend/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>(function(){function t(){return document.querySelector(".post-content")||document.querySelector(".post-single")||document.body}function n(e){return/\$\$[\s\S]+?\$\$|\\\(|\\\)|\\\[|\\\]/.test(e)}function s(e){if(window.__mathjaxLoaded)return;window.__mathjaxLoaded=!0,window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code","tt"],ignoreHtmlClass:"no-math"}};var t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js",t.defer=!0,t.onload=function(){window.MathJax&&window.MathJax.typesetPromise&&window.MathJax.typesetPromise([e]).catch(function(e){console.warn("MathJax typeset error",e)})},document.head.appendChild(t)}function e(){try{if(typeof renderMathInElement=="function"){const e=t();renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,strict:!1,trust:!0,ignoredTags:["script","noscript","style","textarea","pre","code","tt"],ignoredClasses:["no-math"],macros:{"\\boldsymbol":"\\mathbf{#1}","\\bm":"\\mathbf{#1}"}}),setTimeout(function(){n(e.innerHTML)&&s(e)},200)}}catch(e){console.warn("KaTeX render error:",e)}}document.addEventListener("DOMContentLoaded",function(){e(),setTimeout(e,200)}),window.addEventListener("load",function(){setTimeout(e,0)})})()</script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>const config={startOnLoad:!0,theme:"neutral",themeVariables:{lineColor:"#0f0f0f"},flowchart:{useMaxWidth:!1,htmlLabels:!0}};mermaid.initialize(config),window.onload=()=>{window.mermaid.init(0[0],document.querySelectorAll(".language-mermaid"))}</script><link rel=stylesheet href=/css/custom.min.bda7229c4269a242639e058fb11a4782f02f8d77071ba16609befee67cc41c49.css integrity="sha256-vacinEJpokJjngWPsRpHgvAvjXcHG6FmCb7+5nzEHEk="><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]"),n=document.querySelectorAll(".toc a");if(t.length===0||n.length===0)return;const s={};t.forEach(e=>{s[e.id]=e.offsetTop});function i(){const t=window.scrollY+100;let e="";for(const[n,o]of Object.entries(s))if(t>=o)e=n;else break;return e}function o(){const e=i();if(n.forEach(e=>{e.classList.remove("active")}),e){const t=document.querySelector(`.toc a[href="#${e}"]`);t&&t.classList.add("active")}}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){o(),e=!1}),e=!0)}),o()})</script><meta property="og:url" content="https://pillumina.github.io/posts/aiinfra/06-sglang-backend/"><meta property="og:site_name" content="CctoctoFX"><meta property="og:title" content="[SGLang] 后端代码速览"><meta property="og:description" content="本文档为开发者提供 SGLang 后端代码的代码梳理，按照一个请求从输入到最后输出的顺序进行讲解。下图简要介绍了这一流程：
具体而言，请求的处理过程如下：
用户启动 Server ，初始化 FastAPI App、TokenizerManager、DetokenizerManager 和 Scheduler，每个组件运行各自的无限事件循环（infinite event loop）。
用户向 FastAPI Server 发送 /v1/chat/completions 请求，Server 通过 v1_chat_completions endpoint 将请求转发到 TokenizerManager。
v1_chat_completions 函数将请求转换为 ChatCompletionRequest，再转换为 GenerateReqInput，并调用 TokenizerManager 的 generate_request 方法。
TokenizerManager 对请求进行 tokenization，并以 Python 对象（pyobj）形式将其转发给 Scheduler，同时调用 TokenizerManager 的 _wait_one_response 方法。
Scheduler 在事件循环 event_loop_normal 中处理请求：
Scheduler 通过 recv_requests 接收请求，调用 process_input_requests 处理输入，通过 handle_generate_request 管理生成请求的逻辑，并将其加入 waiting_queue。 从 waiting_queue 中，Scheduler 使用 get_next_batch_to_run 为即将处理的请求创建 ScheduleBatch。 Scheduler 执行 run_batch 函数，将 ScheduleBatch 转换为 ModelWorkerBatch。 Scheduler 调用 TpModelWorker 的 forward_batch_generation，等待 logits_output 和 next_token_ids。 TpModelWorker 初始化 ForwardBatch，将其转发至 ModelRunner，并等待 logits_output。 ModelRunner 处理 ForwardBatch，调用 forward_extend 执行模型的前向计算（forward pass）。 模型通过 AttentionBackend 加速生成 logits，返回给 ModelRunner，进而返回给 TpModelWorker。 TpModelWorker 从 ModelRunner 接收 logits_output，调用 ModelRunner 的 sample 方法生成 next_token_ids，并将其发送回 Scheduler。 Scheduler 通过 process_batch_result 处理批次结果，使用 tree_cache.cache_finished_req(req) 缓存请求，并通过 check_finished 验证完成状态。对于未完成的请求，Scheduler 继续其事件循环，直到这个请求满足结束条件；对于已完成的请求，则转发到 Scheduler 的 stream_output。 在 stream_output 函数中，Scheduler 处理输出，将其包装成 BatchTokenIDOut，并发送给 DetokenizerManager。 DetokenizerManager 在其事件循环中接收 BatchTokenIDOut，处理后生成 BatchStrOut 并返回给 TokenizerManager。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-13T10:30:12+08:00"><meta property="article:modified_time" content="2025-08-13T10:30:12+08:00"><meta property="article:tag" content="Inference"><meta property="article:tag" content="Sglang"><meta property="og:image" content="https://pillumina.github.io/imgs/icon_head.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://pillumina.github.io/imgs/icon_head.png"><meta name=twitter:title content="[SGLang] 后端代码速览"><meta name=twitter:description content="本文档为开发者提供 SGLang 后端代码的代码梳理，按照一个请求从输入到最后输出的顺序进行讲解。下图简要介绍了这一流程：

    

具体而言，请求的处理过程如下：


用户启动 Server ，初始化 FastAPI App、TokenizerManager、DetokenizerManager 和 Scheduler，每个组件运行各自的无限事件循环（infinite event loop）。


用户向 FastAPI Server 发送 /v1/chat/completions 请求，Server 通过 v1_chat_completions endpoint 将请求转发到 TokenizerManager。


v1_chat_completions 函数将请求转换为 ChatCompletionRequest，再转换为 GenerateReqInput，并调用 TokenizerManager 的 generate_request 方法。


TokenizerManager 对请求进行 tokenization，并以 Python 对象（pyobj）形式将其转发给 Scheduler，同时调用 TokenizerManager 的 _wait_one_response 方法。


Scheduler 在事件循环 event_loop_normal 中处理请求：

Scheduler 通过 recv_requests 接收请求，调用 process_input_requests 处理输入，通过 handle_generate_request 管理生成请求的逻辑，并将其加入 waiting_queue。
从 waiting_queue 中，Scheduler 使用 get_next_batch_to_run 为即将处理的请求创建 ScheduleBatch。
Scheduler 执行 run_batch 函数，将 ScheduleBatch 转换为 ModelWorkerBatch。
Scheduler 调用 TpModelWorker 的 forward_batch_generation，等待 logits_output 和 next_token_ids。
TpModelWorker 初始化 ForwardBatch，将其转发至 ModelRunner，并等待 logits_output。
ModelRunner 处理 ForwardBatch，调用 forward_extend 执行模型的前向计算（forward pass）。
模型通过 AttentionBackend 加速生成 logits，返回给 ModelRunner，进而返回给 TpModelWorker。
TpModelWorker 从 ModelRunner 接收 logits_output，调用 ModelRunner 的 sample 方法生成 next_token_ids，并将其发送回 Scheduler。
Scheduler 通过 process_batch_result 处理批次结果，使用 tree_cache.cache_finished_req(req) 缓存请求，并通过 check_finished 验证完成状态。对于未完成的请求，Scheduler 继续其事件循环，直到这个请求满足结束条件；对于已完成的请求，则转发到 Scheduler 的 stream_output。
在 stream_output 函数中，Scheduler 处理输出，将其包装成 BatchTokenIDOut，并发送给 DetokenizerManager。



DetokenizerManager 在其事件循环中接收 BatchTokenIDOut，处理后生成 BatchStrOut 并返回给 TokenizerManager。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://pillumina.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI Infra","item":"https://pillumina.github.io/posts/aiinfra/"},{"@type":"ListItem","position":3,"name":"[SGLang] 后端代码速览","item":"https://pillumina.github.io/posts/aiinfra/06-sglang-backend/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[SGLang] 后端代码速览","name":"[SGLang] 后端代码速览","description":"本文档为开发者提供 SGLang 后端代码的代码梳理，按照一个请求从输入到最后输出的顺序进行讲解。下图简要介绍了这一流程：\n具体而言，请求的处理过程如下：\n用户启动 Server ，初始化 FastAPI App、TokenizerManager、DetokenizerManager 和 Scheduler，每个组件运行各自的无限事件循环（infinite event loop）。\n用户向 FastAPI Server 发送 /v1/chat/completions 请求，Server 通过 v1_chat_completions endpoint 将请求转发到 TokenizerManager。\nv1_chat_completions 函数将请求转换为 ChatCompletionRequest，再转换为 GenerateReqInput，并调用 TokenizerManager 的 generate_request 方法。\nTokenizerManager 对请求进行 tokenization，并以 Python 对象（pyobj）形式将其转发给 Scheduler，同时调用 TokenizerManager 的 _wait_one_response 方法。\nScheduler 在事件循环 event_loop_normal 中处理请求：\nScheduler 通过 recv_requests 接收请求，调用 process_input_requests 处理输入，通过 handle_generate_request 管理生成请求的逻辑，并将其加入 waiting_queue。 从 waiting_queue 中，Scheduler 使用 get_next_batch_to_run 为即将处理的请求创建 ScheduleBatch。 Scheduler 执行 run_batch 函数，将 ScheduleBatch 转换为 ModelWorkerBatch。 Scheduler 调用 TpModelWorker 的 forward_batch_generation，等待 logits_output 和 next_token_ids。 TpModelWorker 初始化 ForwardBatch，将其转发至 ModelRunner，并等待 logits_output。 ModelRunner 处理 ForwardBatch，调用 forward_extend 执行模型的前向计算（forward pass）。 模型通过 AttentionBackend 加速生成 logits，返回给 ModelRunner，进而返回给 TpModelWorker。 TpModelWorker 从 ModelRunner 接收 logits_output，调用 ModelRunner 的 sample 方法生成 next_token_ids，并将其发送回 Scheduler。 Scheduler 通过 process_batch_result 处理批次结果，使用 tree_cache.cache_finished_req(req) 缓存请求，并通过 check_finished 验证完成状态。对于未完成的请求，Scheduler 继续其事件循环，直到这个请求满足结束条件；对于已完成的请求，则转发到 Scheduler 的 stream_output。 在 stream_output 函数中，Scheduler 处理输出，将其包装成 BatchTokenIDOut，并发送给 DetokenizerManager。 DetokenizerManager 在其事件循环中接收 BatchTokenIDOut，处理后生成 BatchStrOut 并返回给 TokenizerManager。\n","keywords":["inference","sglang"],"articleBody":"本文档为开发者提供 SGLang 后端代码的代码梳理，按照一个请求从输入到最后输出的顺序进行讲解。下图简要介绍了这一流程：\n具体而言，请求的处理过程如下：\n用户启动 Server ，初始化 FastAPI App、TokenizerManager、DetokenizerManager 和 Scheduler，每个组件运行各自的无限事件循环（infinite event loop）。\n用户向 FastAPI Server 发送 /v1/chat/completions 请求，Server 通过 v1_chat_completions endpoint 将请求转发到 TokenizerManager。\nv1_chat_completions 函数将请求转换为 ChatCompletionRequest，再转换为 GenerateReqInput，并调用 TokenizerManager 的 generate_request 方法。\nTokenizerManager 对请求进行 tokenization，并以 Python 对象（pyobj）形式将其转发给 Scheduler，同时调用 TokenizerManager 的 _wait_one_response 方法。\nScheduler 在事件循环 event_loop_normal 中处理请求：\nScheduler 通过 recv_requests 接收请求，调用 process_input_requests 处理输入，通过 handle_generate_request 管理生成请求的逻辑，并将其加入 waiting_queue。 从 waiting_queue 中，Scheduler 使用 get_next_batch_to_run 为即将处理的请求创建 ScheduleBatch。 Scheduler 执行 run_batch 函数，将 ScheduleBatch 转换为 ModelWorkerBatch。 Scheduler 调用 TpModelWorker 的 forward_batch_generation，等待 logits_output 和 next_token_ids。 TpModelWorker 初始化 ForwardBatch，将其转发至 ModelRunner，并等待 logits_output。 ModelRunner 处理 ForwardBatch，调用 forward_extend 执行模型的前向计算（forward pass）。 模型通过 AttentionBackend 加速生成 logits，返回给 ModelRunner，进而返回给 TpModelWorker。 TpModelWorker 从 ModelRunner 接收 logits_output，调用 ModelRunner 的 sample 方法生成 next_token_ids，并将其发送回 Scheduler。 Scheduler 通过 process_batch_result 处理批次结果，使用 tree_cache.cache_finished_req(req) 缓存请求，并通过 check_finished 验证完成状态。对于未完成的请求，Scheduler 继续其事件循环，直到这个请求满足结束条件；对于已完成的请求，则转发到 Scheduler 的 stream_output。 在 stream_output 函数中，Scheduler 处理输出，将其包装成 BatchTokenIDOut，并发送给 DetokenizerManager。 DetokenizerManager 在其事件循环中接收 BatchTokenIDOut，处理后生成 BatchStrOut 并返回给 TokenizerManager。\nTokenizerManager 在其事件循环中接收结果，通过 handle_loop 处理并更新内部状态，然后将响应返回给Server 。\nFastAPI Server 最后封装完成的响应并将其返回给用户。\n💡 多模态请求处理：对于包含图像、视频等多模态内容的请求详细处理流程，请参考 SGLang 多模态请求生命周期：以 Qwen2.5-VL 为例的架构级深度解析。\n本文基于 SGLang v0.4.0 版本的代码编写。\n注意：本文档仍在编写中，以下部分将在后续加入：\n基于 Attention Backend 的 Radix Cache 管理。 get_next_batch_to_run：如何为每批次请求提取和写入 KV 缓存。 get_model_worker_batch。 write_req_to_token_pool_triton。 使用 CUDA Graphs 优化 Attention Backend。 重叠调度策略（overlap scheduling）。 启动 Server（launch Sever） SGLang 提供 SRT（SGLang Runtime）Server 用于服务 HTTP 请求以及一个不依赖 HTTP 协议的离线推理引擎。核心函数 launch_server 和 launch_engine 均定义在 server.py 中。其中，launch_engine 函数负责初始化核心 SRT Server 的组件。\n设置 logging、Server 参数、CUDA/NCCL 环境变量以及进程间通信端口，配置 model 和 tokenizer。 如果 dp_size \u003e 1，运行 run_data_parallel_controller_process 以启动多个 data parallel replicas；否则，在每个 tp_rank 上，以子进程的方式初始化一个 Scheduler，处理来自 TokenizerManager 的请求，并且管理 KV Cache。 在 Engine 主进程中运行 TokenizerManager，并以子进程形式运行 DetokenizerManager：前者负责 tokenize requests 并发送给 Scheduler，后者将 Scheduler 返回的 token ids 转换为文本，发送回 Server 前端。需要注意的是，在多节点推理中（例如，在两个节点上使用 共计 16 张 H100 部署 Llama 3.1 405B），TokenizerManager 和 DetokenizerManager 仅在第一个节点运行。 如果指定了 chat template，则将其启动，随后等待 Scheduler 进程发出全部进程准备就绪的信号，并且 Scheduler 的配置信息。 需要注意的是，在 0.4.0 版本中，DataParallelController 用于在 data parallel replicas 之间以 round-robin （轮询）方式调度请求。未来，我们计划将其更换为 SGLang Router 来实现多个 replica 之间的调度。\n转发请求 (Forward Requests From Server) Server 使用 FastAPI 应用定义 API endpoint，通过 v1_chat_completions 将 /v1/chat/completions 请求转发至 TokenizerManager。\n从 raw_request 中解析 JSON 数据为 ChatCompletionRequest，将其转换为 GenerateReqInput，并通过 v1_chat_generate_request 配置 sampling_params。 调用 TokenizerManager 的 generate_request 方法并等待返回。得到返回后，根据 stream 参数处理流式（streaming）或非流式（non-streaming）响应。 对于流式响应，使用 generate_stream_resp 逐步处理 generate_request 的输出；对于非流式响应，等待异步返回的处理结果并通过 v1_chat_generate_response 转换为 ChatCompletionResponse。 TokenizerManager 生成请求（Generate Request In TokenizerManager） TokenizerManager 由Server 主进程中的 launch_server 初始化，用于对请求进行 tokenization。\nInitialization 设置 ZeroMQ 进行进程间通信，包括 TokenizerManager 与 DetokenizerManager 和 Scheduler 交互的 socket。 配置 server_args，启用 metrics，并初始化 model_config、tokenizer 以及多模态图像处理器的 placeholders。 generate_request 如果 TokenizerManager 的事件循环尚未初始化，则在此创建。 如果模型权重正在通过 update_weights_from_disk 或 update_weights_from_distributed 更新参数，则暂停处理。 验证请求类型是否与模型的 is_generation 设置匹配。 使用 normalize_batch_and_arguments 对请求进行归一化/标准化，以管理批处理、并行采样和默认参数。 对单个请求，通过 _tokenize_one_request 进行 tokenization，将请求发送至 Scheduler，并通过 _wait_one_response 等待响应。 对批处理请求，通过 _handle_batch_request 方法进行处理：tokenize 输入、管理并行采样、与 Scheduler 交互，并在流式和非流式模式下生成响应。 Scheduler 接收请求以及处理批次 (Scheduler Receive Requests and Process Batches) 这张图给出了 Scheduler 的概览：\nScheduler 作为 Server 的子进程运行，通过 run_scheduler_process 初始化，并通过 event_loop_normal 或 event_loop_overlap 执行无限的事件循环。\nInitialization 配置 ZeroMQ 用于与 TokenizerManager 的通信。 设置 server_args、port_args、model_config、sessions，并根据重叠调度（overlap scheduling）的方式初始化 TpModelWorker 或 TpModelWorkerClient。 初始化分词器和处理器，使用 ChunkCache 或 RadixCache 进行缓存管理，配置 SchedulePolicy。 配置 chunk prefill 参数，并为 constraint decoding 请求初始化 GrammarBackend。 Event Loop Scheduler 不断执行由 process_input_requests、get_next_batch_to_run、run_batch 和 process_batch_result 构成的无限事件循环。\nprocess_input_requests 遍历接收到的请求，识别其类型并将其分派给相应的处理函数。\nget_next_batch_to_run 尽可能将 last_batch 与 running_batch 合并，并通过 get_new_batch_prefill 优先处理 prefill batch。 如果没有 prefill batch，则更新用于 decode batch 的 running_batch，包括过滤请求、管理显存并调整解码参数。 run_batch 对于生成模型，使用 TpModelWorker 的 forward_batch_generation 生成新的 token，或在空闲状态中使用 forward_batch_idle，并将结果返回至 event_loop_normal。 对于嵌入或奖励模型，执行 forward_batch_embedding，并返回 embeddings。 process_batch_result 在执行完 run_batch 后，Scheduler 在 event_loop_normal 中处理批量结果：\nDecode 模式：处理输出，更新请求状态，处理标记和概率数据，管理内存，并记录统计信息。 Extend 模式：处理预填充结果，处理输入标记，并为进一步解码或嵌入做准备。 已完成的请求通过 cache_finished_req 缓存，并流式传输到 DetokenizerManager。未完成的请求会被更新，并循环回 get_next_batch_to_run 进行进一步处理，直至完成。 需要注意的是，LLM 推理按照计算特性不同，通常分为 Prefill 和 Decode 阶段。对于 Prefill 和 Decode 的概念，可以参考 HuggingFace 的这篇文章。而在 SGLang 中，大多数情况下使用的是 extend mode，而非 prefill mode。Prefill 模式为新请求初始化 KV-Cache，通常使用 Paged KV-Cache。而 Extend 模式则利用 Ragged Tensors 增量更新现有的 KV-Cache，效率更高，这使其非常适合 SGLang 面向的长序列或多轮对话请求。\nTpModelWorker 管理 forward pass 和 token sampling (TpModelWorker Manage Forward and Token Sampling) TpModelWorker 负责管理 ModelRunner 的 forward pass 和 token sampling 操作，从而完成由 Scheduler 调度的批次请求。\nInitialization 初始化 tokenizer、模型配置和 ModelRunner。 配置设备信息和 memory pool。 forward_batch_generation 创建 ForwardBatch，通过 ModelRunner 的 forward 计算 logits，并使用 ModelRunner 的 sample 采样得到下一个 token。 将 logits_output 和 next_token_ids 返回给 Scheduler，用于 Scheduler 的 process_batch_result。 forward_batch_embedding 创建一个 ForwardBatch，通过 ModelRunner 的 forward 获取 logits_output 和 embeddings。 embedding 请求不需要采样，因此跳过 ModelRunner 的 sample 过程，直接将 embeddings 返回给 Scheduler。 ModelRunner 管理模型执行 (ModelRunner Manages Model Execution) ModelRunner 初始化 AttentionBackend 并管理加载的模型，以执行 generation 和 embedding 任务的 forward pass。\nInitialization 初始化分布式环境，加载模型，启动 tensor parallel，并设置 memory pool 和 AttentionBackend。\nforward forward 函数根据 forward_mode 决定适当的前向模式来处理批次：\nforward_decode：初始化 forward metadata 并调用模型的 forward，传入 input IDs 和 position。 forward_extend：初始化 forward metadata 并调用模型的 forward 进行 generation 或 embedding 任务。 forward_idle：当前向模式为空闲时，管理空闲的前向传递。 Model 加载权重并执行前向传递 (Model Load Weights and Perform Forward) ModelRunner 的 self.model 是 Model class 的一个实例。所有 支持的模型 都可以在 python/sglang/srt/models 中找到。我们以 Qwen2ForCausalLM 为例。\nQwen2ForCausalLM 的结构如下：\nmodel：用于前向传递的权重。 embed_tokens：将 input_ids 转换为 embeddings。 lm_head：将 hidden states 映射回 vocabulary space。 logits_processor：处理 logits 以便进一步 sampling 或者 normalization。 pooler：用于提取 embeddings 或计算 rewards 的 pooling 机制。 forward Qwen2ForCausalLM 中的 forward 函数处理 input IDs，生成用于预测下一个 token 的 logits，或生成用于奖励/嵌入请求的 embeddings：\n使用 embed_tokens 将 input_ids 转换为 embeddings。将 embeddings 依次通过多个 Qwen2DecoderLayer 层完成 forward pass。 如果 get_embedding 为 True，则通过 pooler 返回 embeddings；否则，使用 logits_processor 计算 logits 并返回。 SGLang 对模型推理的加速主要来自于 forward_batch 与 AttentionBackend 之间的交互。\nAttentionBackend 加速模型前向传递 (AttentionBackend Accelerate Model Forward) SGLang 支持多个 Attention Backends，这些 backends 加速模型的 forward pass 和 key-value cache reuse。我们以 FlashInferBackend 为例。\nInitialization 配置 sliding window 和 cross-attention 场景的 wrappers。 分配必要的 buffers 和 key-value 索引。 为高效的注意力计算准备 forward metadata。 集成 CUDA Graphs 支持以优化执行路径。 init_forward_metadata decode mode：使用 indices_updater_decode 更新 decode 的索引，并设置 forward_metadata 以使用 decode_wrappers。 extend mode：根据 token 和 wrappers 的数量确定是否需要 ragged forward，随后使用 indices_updater_prefill 更新索引。 分配 metadata：设置 forward_metadata，为 ragged forward 和 prefix extension 设置 flags。 forward_extend 和 forward_decode 对 forward_extend，根据 ragged 或者 paged attention，选择合适的 wrapper。对 forward_decode，选择 decode wrapper。 计算 attention，管理 key-value cache，并返回 reshaped 后的输出。 DetokenizerManager 进行解码并发送回 TokenizerManager (DetokenizerManager Detokenize and Send to TokenizerManager) DetokenizerManager 在 launch_server 中被初始化为 Server 的子进程，用于将 Scheduler 返回的 token ids 转换为文本，并发送回 TokenizerManager。\nInitialization 设置 ZMQ communication socket 和 tokenizer。使用 LimitedCapacityDict 管理 decode status。\nevent_loop 和 trim_eos 接收来自 Scheduler 的处理请求，直接转发 BatchEmbeddingOut 或处理 BatchTokenIDOut 进行 detokenization。 将 token ID 拆分为 read_ids 和 surr_ids。使用 batch_decode 将 token ID 转换为文本。更新 DecodeStatus，包括新的 offsets 和 detokenized text。 在序列的停止处整理输出，将 detokenized text 与 metadata 合并成 BatchStrOut，并发送回 TokenizerManager。 FastAPI 整理并输出 (FastAPI Wraps the Output) DetokenizerManager 通过 ZeroMQ 将 BatchStrOut 发送到 TokenizerManager。 TokenizerManager 更新请求状态并为 FastAPI 准备 detokenized text。 最后，在 FastAPI 中，对于流式传输，使用异步生成器和 StreamingResponse 将响应发送给用户。 对于非流式传输，收集并使用 ORJSONResponse 发送完整响应，并返回给用户。 ","wordCount":"885","inLanguage":"en","image":"https://pillumina.github.io/imgs/icon_head.png","datePublished":"2025-08-13T10:30:12+08:00","dateModified":"2025-08-13T10:30:12+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://pillumina.github.io/posts/aiinfra/06-sglang-backend/"},"publisher":{"@type":"Organization","name":"CctoctoFX","logo":{"@type":"ImageObject","url":"https://pillumina.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://pillumina.github.io/ accesskey=h title="CctoctoFX (Alt + H)"><img src=https://pillumina.github.io/apple-touch-icon.png alt aria-label=logo height=30>CctoctoFX</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://pillumina.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://pillumina.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://pillumina.github.io/posts/aiinfra/ title="AI Infra"><span>AI Infra</span></a></li><li><a href=https://pillumina.github.io/posts/llmtheory/ title=Thoery><span>Thoery</span></a></li><li><a href=https://pillumina.github.io/posts/programming/ title=Programming><span>Programming</span></a></li><li><a href=https://pillumina.github.io/social/ title=Social><span>Social</span></a></li><li><a href=https://pillumina.github.io/open_courses/ title=Study><span>Study</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://pillumina.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/aiinfra/>AI Infra</a></div><h1 class="post-title entry-hint-parent">[SGLang] 后端代码速览</h1><div class=post-meta><span title='2025-08-13 10:30:12 +0800 CST'>August 13, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;885 words&nbsp;·&nbsp;Me</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#启动-serverlaunch-sever>启动 Server（launch Sever）</a></li><li><a href=#转发请求-forward-requests-from-server>转发请求 (Forward Requests From Server)</a></li><li><a href=#tokenizermanager-生成请求generate-request-in-tokenizermanager>TokenizerManager 生成请求（Generate Request In TokenizerManager）</a><ul><li><a href=#initialization><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/tokenizer_manager.py#L88>Initialization</a></a></li><li><a href=#generate_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/tokenizer_manager.py#L173>generate_request</a></a></li></ul></li><li><a href=#scheduler-接收请求以及处理批次-scheduler-receive-requests-and-process-batches>Scheduler 接收请求以及处理批次 (Scheduler Receive Requests and Process Batches)</a><ul><li><a href=#initialization-1><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/scheduler.py#L97>Initialization</a></a></li><li><a href=#event-loop><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/scheduler.py#L376>Event Loop</a></a></li><li><a href=#process_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/scheduler.py#L508>process_input_requests</a></a></li><li><a href=#get_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/scheduler.py#L768>get_next_batch_to_run</a></a></li><li><a href=#run_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/scheduler.py#L956>run_batch</a></a></li><li><a href=#process_-1><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/scheduler.py#L987>process_batch_result</a></a></li></ul></li><li><a href=#tpmodelworker-管理-forward-pass-和-token-sampling-tpmodelworker-manage-forward-and-token-sampling>TpModelWorker 管理 forward pass 和 token sampling (TpModelWorker Manage Forward and Token Sampling)</a><ul><li><a href=#initialization-2><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/tp_worker.py#L40>Initialization</a></a></li><li><a href=#forward_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/tp_worker.py#L148>forward_batch_generation</a></a></li><li><a href=#forward_-1><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/tp_worker.py#L160>forward_batch_embedding</a></a></li></ul></li><li><a href=#modelrunner-管理模型执行-modelrunner-manages-model-execution>ModelRunner 管理模型执行 (ModelRunner Manages Model Execution)</a><ul><li><a href=#initialization-3><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/model_executor/model_runner.py#L66>Initialization</a></a></li><li><a href=#forward><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/model_executor/model_runner.py#L675>forward</a></a></li></ul></li><li><a href=#model-加载权重并执行前向传递-model-load-weights-and-perform-forward>Model 加载权重并执行前向传递 (Model Load Weights and Perform Forward)</a><ul><li><a href=#forward-1><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/models/qwen2.py#L289>forward</a></a></li></ul></li><li><a href=#attentionbackend-加速模型前向传递-attentionbackend-accelerate-model-forward>AttentionBackend 加速模型前向传递 (AttentionBackend Accelerate Model Forward)</a><ul><li><a href=#initialization-4><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/layers/attention/flashinfer_backend.py#L48>Initialization</a></a></li><li><a href=#init_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/layers/attention/flashinfer_backend.py#L130>init_forward_metadata</a></a></li><li><a href=#forward_-和-forward_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/layers/attention/flashinfer_backend.py#L223>forward_extend</a> 和 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/layers/attention/flashinfer_backend.py#L277>forward_decode</a></a></li></ul></li><li><a href=#detokenizermanager-进行解码并发送回-tokenizermanager-detokenizermanager-detokenize-and-send-to-tokenizermanager>DetokenizerManager 进行解码并发送回 TokenizerManager (DetokenizerManager Detokenize and Send to TokenizerManager)</a><ul><li><a href=#initialization-5><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/detokenizer_manager.py#L53>Initialization</a></a></li><li><a href=#event_-和-trim_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/detokenizer_manager.py#L93>event_loop</a> 和 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/detokenizer_manager.py#L78>trim_eos</a></a></li></ul></li><li><a href=#fastapi-整理并输出-fastapi-wraps-the-output><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/server.py#L287>FastAPI 整理并输出 (FastAPI Wraps the Output)</a></a></li></ul></nav></div></details></div><div class=post-content><p>本文档为开发者提供 SGLang 后端代码的代码梳理，按照一个请求从输入到最后输出的顺序进行讲解。下图简要介绍了这一流程：</p><div style="text-align:center;width:100%;margin:0 auto"><img src=https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/raw/main/sglang/code-walk-through/sglang-architecture.svg alt="SGLang 架构图" style=width:100%;height:auto></div><p>具体而言，请求的处理过程如下：</p><ol><li><p>用户启动 Server ，初始化 FastAPI App、TokenizerManager、DetokenizerManager 和 Scheduler，每个组件运行各自的无限事件循环（infinite event loop）。</p></li><li><p>用户向 FastAPI Server 发送 <code>/v1/chat/completions</code> 请求，Server 通过 <code>v1_chat_completions</code> endpoint 将请求转发到 TokenizerManager。</p></li><li><p><code>v1_chat_completions</code> 函数将请求转换为 <code>ChatCompletionRequest</code>，再转换为 <code>GenerateReqInput</code>，并调用 TokenizerManager 的 <code>generate_request</code> 方法。</p></li><li><p>TokenizerManager 对请求进行 tokenization，并以 Python 对象（<code>pyobj</code>）形式将其转发给 Scheduler，同时调用 TokenizerManager 的 <code>_wait_one_response</code> 方法。</p></li><li><p>Scheduler 在事件循环 <code>event_loop_normal</code> 中处理请求：</p><ul><li>Scheduler 通过 <code>recv_requests</code> 接收请求，调用 <code>process_input_requests</code> 处理输入，通过 <code>handle_generate_request</code> 管理生成请求的逻辑，并将其加入 <code>waiting_queue</code>。</li><li>从 <code>waiting_queue</code> 中，Scheduler 使用 <code>get_next_batch_to_run</code> 为即将处理的请求创建 <code>ScheduleBatch</code>。</li><li>Scheduler 执行 <code>run_batch</code> 函数，将 <code>ScheduleBatch</code> 转换为 <code>ModelWorkerBatch</code>。</li><li>Scheduler 调用 TpModelWorker 的 <code>forward_batch_generation</code>，等待 <code>logits_output</code> 和 <code>next_token_ids</code>。</li><li>TpModelWorker 初始化 <code>ForwardBatch</code>，将其转发至 ModelRunner，并等待 <code>logits_output</code>。</li><li>ModelRunner 处理 <code>ForwardBatch</code>，调用 <code>forward_extend</code> 执行模型的前向计算（forward pass）。</li><li>模型通过 <code>AttentionBackend</code> 加速生成 logits，返回给 ModelRunner，进而返回给 TpModelWorker。</li><li>TpModelWorker 从 ModelRunner 接收 <code>logits_output</code>，调用 ModelRunner 的 <code>sample</code> 方法生成 <code>next_token_ids</code>，并将其发送回 Scheduler。</li><li>Scheduler 通过 <code>process_batch_result</code> 处理批次结果，使用 <code>tree_cache.cache_finished_req(req)</code> 缓存请求，并通过 <code>check_finished</code> 验证完成状态。对于未完成的请求，Scheduler 继续其事件循环，直到这个请求满足结束条件；对于已完成的请求，则转发到 Scheduler 的 <code>stream_output</code>。</li><li>在 <code>stream_output</code> 函数中，Scheduler 处理输出，将其包装成 <code>BatchTokenIDOut</code>，并发送给 DetokenizerManager。</li></ul></li><li><p>DetokenizerManager 在其事件循环中接收 <code>BatchTokenIDOut</code>，处理后生成 <code>BatchStrOut</code> 并返回给 TokenizerManager。</p></li><li><p>TokenizerManager 在其事件循环中接收结果，通过 <code>handle_loop</code> 处理并更新内部状态，然后将响应返回给Server 。</p></li><li><p>FastAPI Server 最后封装完成的响应并将其返回给用户。</p></li></ol><blockquote><p><strong>💡 多模态请求处理</strong>：对于包含图像、视频等多模态内容的请求详细处理流程，请参考 <a href=multimodal_request_lifecycle.md>SGLang 多模态请求生命周期：以 Qwen2.5-VL 为例的架构级深度解析</a>。</p></blockquote><p>本文基于 <a href=https://github.com/sgl-project/sglang/tree/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751>SGLang v0.4.0</a> 版本的代码编写。</p><p><strong>注意：本文档仍在编写中，以下部分将在后续加入：</strong></p><ol><li>基于 Attention Backend 的 Radix Cache 管理。</li><li><code>get_next_batch_to_run</code>：如何为每批次请求提取和写入 KV 缓存。</li><li><code>get_model_worker_batch</code>。</li><li><code>write_req_to_token_pool_triton</code>。</li><li>使用 CUDA Graphs 优化 Attention Backend。</li><li>重叠调度策略（overlap scheduling）。</li></ol><h2 id=启动-serverlaunch-sever>启动 Server（launch Sever）<a hidden class=anchor aria-hidden=true href=#启动-serverlaunch-sever>#</a></h2><p>SGLang 提供 SRT（SGLang Runtime）Server 用于<a href=https://sgl-project.github.io/backend/send_request.html>服务 HTTP 请求</a>以及一个不依赖 HTTP 协议的<a href=https://sgl-project.github.io/backend/offline_engine_api.html>离线推理引擎</a>。核心函数 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/server.py#L507><code>launch_server</code></a> 和 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/server.py#L418><code>launch_engine</code></a> 均定义在 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/server.py>server.py</a> 中。其中，<code>launch_engine</code> 函数负责初始化核心 SRT Server 的组件。</p><ol><li>设置 logging、Server 参数、CUDA/NCCL 环境变量以及进程间通信端口，配置 model 和 tokenizer。</li><li>如果 <code>dp_size > 1</code>，运行 <code>run_data_parallel_controller_process</code> 以启动多个 data parallel replicas；否则，在每个 <code>tp_rank</code> 上，以子进程的方式初始化一个 Scheduler，处理来自 TokenizerManager 的请求，并且管理 KV Cache。</li><li>在 Engine 主进程中运行 TokenizerManager，并以子进程形式运行 DetokenizerManager：前者负责 tokenize requests 并发送给 Scheduler，后者将 Scheduler 返回的 token ids 转换为文本，发送回 Server 前端。需要注意的是，在多节点推理中（例如，在两个节点上使用 共计 16 张 H100 部署 Llama 3.1 405B），TokenizerManager 和 DetokenizerManager 仅在第一个节点运行。</li><li>如果指定了 chat template，则将其启动，随后等待 Scheduler 进程发出全部进程准备就绪的信号，并且 Scheduler 的配置信息。</li></ol><p>需要注意的是，在 0.4.0 版本中，<a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/data_parallel_controller.py#L52>DataParallelController</a> 用于在 data parallel replicas 之间以 round-robin （轮询）方式调度请求。未来，我们计划将其更换为 <a href=https://sgl-project.github.io/router/router.html>SGLang Router</a> 来实现多个 replica 之间的调度。</p><h2 id=转发请求-forward-requests-from-server>转发请求 (Forward Requests From Server)<a hidden class=anchor aria-hidden=true href=#转发请求-forward-requests-from-server>#</a></h2><p>Server 使用 FastAPI 应用定义 API endpoint，通过 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/openai_api/adapter.py#L1101>v1_chat_completions</a> 将 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/server.py#L354><code>/v1/chat/completions</code></a> 请求转发至 TokenizerManager。</p><ol><li>从 <code>raw_request</code> 中解析 JSON 数据为 <code>ChatCompletionRequest</code>，将其转换为 <code>GenerateReqInput</code>，并通过 <code>v1_chat_generate_request</code> 配置 <code>sampling_params</code>。</li><li>调用 TokenizerManager 的 <code>generate_request</code> 方法并等待返回。得到返回后，根据 <code>stream</code> 参数处理流式（streaming）或非流式（non-streaming）响应。</li><li>对于流式响应，使用 <code>generate_stream_resp</code> 逐步处理 <code>generate_request</code> 的输出；对于非流式响应，等待异步返回的处理结果并通过 <code>v1_chat_generate_response</code> 转换为 <code>ChatCompletionResponse</code>。</li></ol><h2 id=tokenizermanager-生成请求generate-request-in-tokenizermanager>TokenizerManager 生成请求（Generate Request In TokenizerManager）<a hidden class=anchor aria-hidden=true href=#tokenizermanager-生成请求generate-request-in-tokenizermanager>#</a></h2><p><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/tokenizer_manager.py#L88>TokenizerManager</a> 由Server 主进程中的 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/server.py#L507><code>launch_server</code></a> 初始化，用于对请求进行 tokenization。</p><h3 id=initialization><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/tokenizer_manager.py#L88>Initialization</a><a hidden class=anchor aria-hidden=true href=#initialization>#</a></h3><ol><li>设置 <a href=https://libzmq.readthedocs.io/en/latest/>ZeroMQ</a> 进行进程间通信，包括 TokenizerManager 与 DetokenizerManager 和 Scheduler 交互的 socket。</li><li>配置 <code>server_args</code>，启用 <code>metrics</code>，并初始化 <code>model_config</code>、<code>tokenizer</code> 以及多模态图像处理器的 placeholders。</li></ol><h3 id=generate_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/tokenizer_manager.py#L173>generate_request</a><a hidden class=anchor aria-hidden=true href=#generate_>#</a></h3><ol><li>如果 TokenizerManager 的事件循环尚未初始化，则在此创建。</li><li>如果模型权重正在通过 <code>update_weights_from_disk</code> 或 <code>update_weights_from_distributed</code> 更新参数，则暂停处理。</li><li>验证请求类型是否与模型的 <code>is_generation</code> 设置匹配。</li><li>使用 <code>normalize_batch_and_arguments</code> 对请求进行归一化/标准化，以管理批处理、并行采样和默认参数。</li><li>对单个请求，通过 <code>_tokenize_one_request</code> 进行 tokenization，将请求发送至 Scheduler，并通过 <code>_wait_one_response</code> 等待响应。</li><li>对批处理请求，通过 <code>_handle_batch_request</code> 方法进行处理：tokenize 输入、管理并行采样、与 Scheduler 交互，并在流式和非流式模式下生成响应。</li></ol><h2 id=scheduler-接收请求以及处理批次-scheduler-receive-requests-and-process-batches>Scheduler 接收请求以及处理批次 (Scheduler Receive Requests and Process Batches)<a hidden class=anchor aria-hidden=true href=#scheduler-接收请求以及处理批次-scheduler-receive-requests-and-process-batches>#</a></h2><p>这张图给出了 Scheduler 的概览：<br><img alt=sglang_scheduler loading=lazy src=https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/raw/main/sglang/code-walk-through/sglang_scheduler.svg data-zoomable></p><p><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/scheduler.py#L97>Scheduler</a> 作为 Server 的子进程运行，通过 <code>run_scheduler_process</code> 初始化，并通过 <code>event_loop_normal</code> 或 <code>event_loop_overlap</code> 执行无限的事件循环。</p><h3 id=initialization-1><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/scheduler.py#L97>Initialization</a><a hidden class=anchor aria-hidden=true href=#initialization-1>#</a></h3><ol><li>配置 <a href=https://libzmq.readthedocs.io/en/latest/>ZeroMQ</a> 用于与 TokenizerManager 的通信。</li><li>设置 <code>server_args</code>、<code>port_args</code>、<code>model_config</code>、<code>sessions</code>，并根据重叠调度（overlap scheduling）的方式初始化 TpModelWorker 或 TpModelWorkerClient。</li><li>初始化分词器和处理器，使用 ChunkCache 或 RadixCache 进行缓存管理，配置 SchedulePolicy。</li><li>配置 chunk prefill 参数，并为 constraint decoding 请求初始化 GrammarBackend。</li></ol><h3 id=event-loop><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/scheduler.py#L376>Event Loop</a><a hidden class=anchor aria-hidden=true href=#event-loop>#</a></h3><p>Scheduler 不断执行由 <code>process_input_requests</code>、<code>get_next_batch_to_run</code>、<code>run_batch</code> 和 <code>process_batch_result</code> 构成的无限事件循环。</p><h3 id=process_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/scheduler.py#L508>process_input_requests</a><a hidden class=anchor aria-hidden=true href=#process_>#</a></h3><p>遍历接收到的请求，识别其类型并将其分派给相应的处理函数。</p><h3 id=get_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/scheduler.py#L768>get_next_batch_to_run</a><a hidden class=anchor aria-hidden=true href=#get_>#</a></h3><ol><li>尽可能将 <code>last_batch</code> 与 <code>running_batch</code> 合并，并通过 <code>get_new_batch_prefill</code> 优先处理 prefill batch。</li><li>如果没有 prefill batch，则更新用于 decode batch 的 <code>running_batch</code>，包括过滤请求、管理显存并调整解码参数。</li></ol><h3 id=run_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/scheduler.py#L956>run_batch</a><a hidden class=anchor aria-hidden=true href=#run_>#</a></h3><ol><li>对于生成模型，使用 TpModelWorker 的 <code>forward_batch_generation</code> 生成新的 token，或在空闲状态中使用 <code>forward_batch_idle</code>，并将结果返回至 <code>event_loop_normal</code>。</li><li>对于嵌入或奖励模型，执行 <code>forward_batch_embedding</code>，并返回 embeddings。</li></ol><h3 id=process_-1><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/scheduler.py#L987>process_batch_result</a><a hidden class=anchor aria-hidden=true href=#process_-1>#</a></h3><p>在执行完 <code>run_batch</code> 后，Scheduler 在 <code>event_loop_normal</code> 中处理批量结果：</p><ol><li><strong>Decode 模式</strong>：处理输出，更新请求状态，处理标记和概率数据，管理内存，并记录统计信息。</li><li><strong>Extend 模式</strong>：处理预填充结果，处理输入标记，并为进一步解码或嵌入做准备。</li><li>已完成的请求通过 <code>cache_finished_req</code> 缓存，并流式传输到 DetokenizerManager。未完成的请求会被更新，并循环回 <code>get_next_batch_to_run</code> 进行进一步处理，直至完成。</li></ol><p>需要注意的是，LLM 推理按照计算特性不同，通常分为 Prefill 和 Decode 阶段。对于 Prefill 和 Decode 的概念，可以参考 HuggingFace 的<a href=https://huggingface.co/blog/martinigoyanes/llm-inference-at-scale-with-tgi>这篇文章</a>。而在 SGLang 中，大多数情况下使用的是 <a href=https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/layers/attention/flashinfer_backend.py>extend mode</a>，而非 prefill mode。Prefill 模式为新请求初始化 KV-Cache，通常使用 Paged KV-Cache。而 Extend 模式则利用 Ragged Tensors 增量更新现有的 KV-Cache，效率更高，这使其非常适合 SGLang 面向的长序列或多轮对话请求。</p><h2 id=tpmodelworker-管理-forward-pass-和-token-sampling-tpmodelworker-manage-forward-and-token-sampling>TpModelWorker 管理 forward pass 和 token sampling (TpModelWorker Manage Forward and Token Sampling)<a hidden class=anchor aria-hidden=true href=#tpmodelworker-管理-forward-pass-和-token-sampling-tpmodelworker-manage-forward-and-token-sampling>#</a></h2><p><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/tp_worker.py#L40>TpModelWorker</a> 负责管理 ModelRunner 的 forward pass 和 token sampling 操作，从而完成由 Scheduler 调度的批次请求。</p><h3 id=initialization-2><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/tp_worker.py#L40>Initialization</a><a hidden class=anchor aria-hidden=true href=#initialization-2>#</a></h3><ol><li>初始化 tokenizer、模型配置和 ModelRunner。</li><li>配置设备信息和 memory pool。</li></ol><h3 id=forward_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/tp_worker.py#L148>forward_batch_generation</a><a hidden class=anchor aria-hidden=true href=#forward_>#</a></h3><ol><li>创建 <code>ForwardBatch</code>，通过 ModelRunner 的 <code>forward</code> 计算 logits，并使用 ModelRunner 的 <code>sample</code> 采样得到下一个 token。</li><li>将 <code>logits_output</code> 和 <code>next_token_ids</code> 返回给 Scheduler，用于 Scheduler 的 <code>process_batch_result</code>。</li></ol><h3 id=forward_-1><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/tp_worker.py#L160>forward_batch_embedding</a><a hidden class=anchor aria-hidden=true href=#forward_-1>#</a></h3><ol><li>创建一个 <code>ForwardBatch</code>，通过 ModelRunner 的 <code>forward</code> 获取 <code>logits_output</code> 和 <code>embeddings</code>。</li><li>embedding 请求不需要采样，因此跳过 ModelRunner 的 <code>sample</code> 过程，直接将 <code>embeddings</code> 返回给 Scheduler。</li></ol><h2 id=modelrunner-管理模型执行-modelrunner-manages-model-execution>ModelRunner 管理模型执行 (ModelRunner Manages Model Execution)<a hidden class=anchor aria-hidden=true href=#modelrunner-管理模型执行-modelrunner-manages-model-execution>#</a></h2><p><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/model_executor/model_runner.py#L66>ModelRunner</a> 初始化 AttentionBackend 并管理加载的模型，以执行 generation 和 embedding 任务的 forward pass。</p><h3 id=initialization-3><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/model_executor/model_runner.py#L66>Initialization</a><a hidden class=anchor aria-hidden=true href=#initialization-3>#</a></h3><p>初始化分布式环境，加载模型，启动 tensor parallel，并设置 memory pool 和 AttentionBackend。</p><h3 id=forward><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/model_executor/model_runner.py#L675>forward</a><a hidden class=anchor aria-hidden=true href=#forward>#</a></h3><p><code>forward</code> 函数根据 <code>forward_mode</code> 决定适当的前向模式来处理批次：</p><ol><li><code>forward_decode</code>：初始化 forward metadata 并调用模型的 <code>forward</code>，传入 input IDs 和 position。</li><li><code>forward_extend</code>：初始化 forward metadata 并调用模型的 <code>forward</code> 进行 generation 或 embedding 任务。</li><li><code>forward_idle</code>：当前向模式为空闲时，管理空闲的前向传递。</li></ol><h2 id=model-加载权重并执行前向传递-model-load-weights-and-perform-forward>Model 加载权重并执行前向传递 (Model Load Weights and Perform Forward)<a hidden class=anchor aria-hidden=true href=#model-加载权重并执行前向传递-model-load-weights-and-perform-forward>#</a></h2><p>ModelRunner 的 <code>self.model</code> 是 Model class 的一个实例。所有 <a href=https://sgl-project.github.io/references/supported_models.html>支持的模型</a> 都可以在 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/models>python/sglang/srt/models</a> 中找到。我们以 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/models/qwen2.py#L269>Qwen2ForCausalLM</a> 为例。</p><p><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/models/qwen2.py#L269><code>Qwen2ForCausalLM</code></a> 的结构如下：</p><ul><li><code>model</code>：用于前向传递的权重。</li><li><code>embed_tokens</code>：将 <code>input_ids</code> 转换为 <code>embeddings</code>。</li><li><code>lm_head</code>：将 hidden states 映射回 vocabulary space。</li><li><code>logits_processor</code>：处理 <code>logits</code> 以便进一步 sampling 或者 normalization。</li><li><code>pooler</code>：用于提取 embeddings 或计算 rewards 的 pooling 机制。</li></ul><h3 id=forward-1><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/models/qwen2.py#L289>forward</a><a hidden class=anchor aria-hidden=true href=#forward-1>#</a></h3><p><code>Qwen2ForCausalLM</code> 中的 <code>forward</code> 函数处理 input IDs，生成用于预测下一个 token 的 logits，或生成用于奖励/嵌入请求的 embeddings：</p><ol><li>使用 <code>embed_tokens</code> 将 <code>input_ids</code> 转换为 embeddings。将 embeddings 依次通过多个 Qwen2DecoderLayer 层完成 forward pass。</li><li>如果 <code>get_embedding</code> 为 True，则通过 <code>pooler</code> 返回 embeddings；否则，使用 <code>logits_processor</code> 计算 <code>logits</code> 并返回。</li></ol><p>SGLang 对模型推理的加速主要来自于 <code>forward_batch</code> 与 AttentionBackend 之间的交互。</p><h2 id=attentionbackend-加速模型前向传递-attentionbackend-accelerate-model-forward>AttentionBackend 加速模型前向传递 (AttentionBackend Accelerate Model Forward)<a hidden class=anchor aria-hidden=true href=#attentionbackend-加速模型前向传递-attentionbackend-accelerate-model-forward>#</a></h2><p>SGLang 支持多个 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/layers/attention>Attention Backends</a>，这些 backends 加速模型的 forward pass 和 key-value cache reuse。我们以 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/layers/attention/flashinfer_backend.py>FlashInferBackend</a> 为例。</p><h3 id=initialization-4><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/layers/attention/flashinfer_backend.py#L48>Initialization</a><a hidden class=anchor aria-hidden=true href=#initialization-4>#</a></h3><ol><li>配置 sliding window 和 cross-attention 场景的 wrappers。</li><li>分配必要的 buffers 和 key-value 索引。</li><li>为高效的注意力计算准备 forward metadata。</li><li>集成 CUDA Graphs 支持以优化执行路径。</li></ol><h3 id=init_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/layers/attention/flashinfer_backend.py#L130>init_forward_metadata</a><a hidden class=anchor aria-hidden=true href=#init_>#</a></h3><ol><li>decode mode：使用 <code>indices_updater_decode</code> 更新 decode 的索引，并设置 <code>forward_metadata</code> 以使用 <code>decode_wrappers</code>。</li><li>extend mode：根据 token 和 wrappers 的数量确定是否需要 ragged forward，随后使用 <code>indices_updater_prefill</code> 更新索引。</li><li>分配 metadata：设置 <code>forward_metadata</code>，为 ragged forward 和 prefix extension 设置 flags。</li></ol><h3 id=forward_-和-forward_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/layers/attention/flashinfer_backend.py#L223>forward_extend</a> 和 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/layers/attention/flashinfer_backend.py#L277>forward_decode</a><a hidden class=anchor aria-hidden=true href=#forward_-和-forward_>#</a></h3><ol><li>对 <code>forward_extend</code>，根据 ragged 或者 paged attention，选择合适的 wrapper。对 <code>forward_decode</code>，选择 decode wrapper。</li><li>计算 attention，管理 key-value cache，并返回 reshaped 后的输出。</li></ol><h2 id=detokenizermanager-进行解码并发送回-tokenizermanager-detokenizermanager-detokenize-and-send-to-tokenizermanager>DetokenizerManager 进行解码并发送回 TokenizerManager (DetokenizerManager Detokenize and Send to TokenizerManager)<a hidden class=anchor aria-hidden=true href=#detokenizermanager-进行解码并发送回-tokenizermanager-detokenizermanager-detokenize-and-send-to-tokenizermanager>#</a></h2><p><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/detokenizer_manager.py#L53>DetokenizerManager</a> 在 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/server.py#L507><code>launch_server</code></a> 中被初始化为 Server 的子进程，用于将 Scheduler 返回的 token ids 转换为文本，并发送回 TokenizerManager。</p><h3 id=initialization-5><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/detokenizer_manager.py#L53>Initialization</a><a hidden class=anchor aria-hidden=true href=#initialization-5>#</a></h3><p>设置 ZMQ communication socket 和 tokenizer。使用 <code>LimitedCapacityDict</code> 管理 decode status。</p><h3 id=event_-和-trim_><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/detokenizer_manager.py#L93>event_loop</a> 和 <a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/managers/detokenizer_manager.py#L78>trim_eos</a><a hidden class=anchor aria-hidden=true href=#event_-和-trim_>#</a></h3><ol><li>接收来自 Scheduler 的处理请求，直接转发 <code>BatchEmbeddingOut</code> 或处理 <code>BatchTokenIDOut</code> 进行 detokenization。</li><li>将 token ID 拆分为 <code>read_ids</code> 和 <code>surr_ids</code>。使用 <code>batch_decode</code> 将 token ID 转换为文本。更新 <code>DecodeStatus</code>，包括新的 offsets 和 detokenized text。</li><li>在序列的停止处整理输出，将 detokenized text 与 metadata 合并成 <code>BatchStrOut</code>，并发送回 TokenizerManager。</li></ol><h2 id=fastapi-整理并输出-fastapi-wraps-the-output><a href=https://github.com/sgl-project/sglang/blob/f8b0326934bacb7a7d4eba68fb6eddebaa6ff751/python/sglang/srt/server.py#L287>FastAPI 整理并输出 (FastAPI Wraps the Output)</a><a hidden class=anchor aria-hidden=true href=#fastapi-整理并输出-fastapi-wraps-the-output>#</a></h2><ol><li>DetokenizerManager 通过 <a href=https://libzmq.readthedocs.io/en/latest/>ZeroMQ</a> 将 <code>BatchStrOut</code> 发送到 TokenizerManager。</li><li>TokenizerManager 更新请求状态并为 FastAPI 准备 detokenized text。</li><li>最后，在 FastAPI 中，对于流式传输，使用异步生成器和 <code>StreamingResponse</code> 将响应发送给用户。</li><li>对于非流式传输，收集并使用 <code>ORJSONResponse</code> 发送完整响应，并返回给用户。</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://pillumina.github.io/tags/inference/>Inference</a></li><li><a href=https://pillumina.github.io/tags/sglang/>Sglang</a></li></ul><nav class=paginav><a class=prev href=https://pillumina.github.io/posts/aiinfra/05-verl-params/><span class=title>« Prev</span><br><span>[VeRL] 参数速览</span>
</a><a class=next href=https://pillumina.github.io/posts/llmtheory/2-moe/><span class=title>Next »</span><br><span>MoE环游记：2、深入负载均衡</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [SGLang] 后端代码速览 on x" href="https://x.com/intent/tweet/?text=%5bSGLang%5d%20%e5%90%8e%e7%ab%af%e4%bb%a3%e7%a0%81%e9%80%9f%e8%a7%88&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f06-sglang-backend%2f&amp;hashtags=inference%2csglang"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [SGLang] 后端代码速览 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f06-sglang-backend%2f&amp;title=%5bSGLang%5d%20%e5%90%8e%e7%ab%af%e4%bb%a3%e7%a0%81%e9%80%9f%e8%a7%88&amp;summary=%5bSGLang%5d%20%e5%90%8e%e7%ab%af%e4%bb%a3%e7%a0%81%e9%80%9f%e8%a7%88&amp;source=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f06-sglang-backend%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [SGLang] 后端代码速览 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f06-sglang-backend%2f&title=%5bSGLang%5d%20%e5%90%8e%e7%ab%af%e4%bb%a3%e7%a0%81%e9%80%9f%e8%a7%88"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [SGLang] 后端代码速览 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f06-sglang-backend%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [SGLang] 后端代码速览 on whatsapp" href="https://api.whatsapp.com/send?text=%5bSGLang%5d%20%e5%90%8e%e7%ab%af%e4%bb%a3%e7%a0%81%e9%80%9f%e8%a7%88%20-%20https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f06-sglang-backend%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [SGLang] 后端代码速览 on telegram" href="https://telegram.me/share/url?text=%5bSGLang%5d%20%e5%90%8e%e7%ab%af%e4%bb%a3%e7%a0%81%e9%80%9f%e8%a7%88&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f06-sglang-backend%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [SGLang] 后端代码速览 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5bSGLang%5d%20%e5%90%8e%e7%ab%af%e4%bb%a3%e7%a0%81%e9%80%9f%e8%a7%88&u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f06-sglang-backend%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul><div class=related-posts><div class=related-series><h3>同系列文章</h3><ul></ul></div><div class=related-tags><h3>相关文章</h3><ul><li><a href=/posts/aiinfra/09-verl-agentloop/>[VeRL] AgentLoop源码走读</a>
<span class=meta>2025-08-14
· 15 min read
· Tags: framework, verl, sglang</span></li><li><a href=/posts/aiinfra/08-verl-multiturn-2/>[VeRL] Multi-Turn RL训练源码走读（2）</a>
<span class=meta>2025-08-03
· 27 min read
· Tags: framework, verl, sglang</span></li><li><a href=/posts/aiinfra/07-verl-multiturn-1/>[VeRL] Multi-Turn RL训练源码走读（1）</a>
<span class=meta>2025-08-03
· 27 min read
· Tags: framework, verl, sglang</span></li></ul></div></div></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://pillumina.github.io/>CctoctoFX</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div class=reading-progress-bar></div><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelector(".reading-progress-bar");if(!t)return;const n=document.querySelector(".post-single");if(!n)return;function s(){const e=n.getBoundingClientRect(),s=e.height,o=window.innerHeight,i=window.scrollY||window.pageYOffset,a=i/(s-o)*100;t.style.width=`${Math.min(100,Math.max(0,a))}%`}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){s(),e=!1}),e=!0)}),s()}),document.addEventListener("DOMContentLoaded",function(){mediumZoom("article img:not(.nozoom)",{margin:24,background:"var(--theme)",scrollOffset:0})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>