<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>昇腾超节点CloudMatrix384论文拆解 | CctoctoFX</title><meta name=keywords content><meta name=description content="
6.19发布的CloudMatrix384论文拆解，从宏观到基础概念
核心指标和计算方式
TPOT (Time Per Output Token)

公式： $$TPOT= \frac{Decode总耗时}{生成Token数量}$$
测量方式： 从第一个输出Token开始计时，到生成结束（含MoE通信/KV读取）
为什么重要： 直接决定用户体验（如Chatbot响应速度），论文要求 <50ms（严格模式<15ms）
深层意义： 反映系统通信+计算综合能力，EP320下TPOT=42ms证明UB网络突破MoE通信墙

计算效率 (Tokens/s per TFLOPS)

公式： $$计算效率=\frac {吞吐量(tokens/s)} {NPU峰值算力(TFLOPS)}$$​
论文数据：


  
      
          阶段
          值
          对比基准
      
  
  
      
          Prefill
          4.45
          超NVIDIA H100+SGLang(3.8)
      
      
          Decode
          1.29
          超NVIDIA H800+DeepSeek(0.9)
      
  


为什么重要： 揭示硬件利用率，1.0以上表明软硬件协同极致优化
深层意义： Decode阶段1.29 → 昇腾910的Cube引擎利用率达 86%（传统GPU仅60%)

缓存访问延迟 (KV Cache Access Latency)

公式： $$延迟=TMMU_{查询}+TUB_{传输}+TDRAM_{读取}​$$
论文数据：


  
      
          场景
          延迟
          对比传统
      
  
  
      
          本地HBM命中
          0.2μs
          -
      
      
          远程DRAM访问(UB)
          1.5μs
          >10μs (PCIe+IB)
      
  


为什么重要： 长上下文推理中70%时间花在KV缓存访问
深层意义： UB统一内存将远程访问性能提升至近本地水平，支撑百万Token上下文。

专家并行扩展性 (EP Degree)

定义：单个MoE层可分布的专家数量
论文突破：EP320（每个昇腾Die托管1个专家）
支撑公式： $$可扩展性=\frac {UB总带宽}{单个专家通信需求}$$ $$EPmax=\frac {384×392GB/s} {8B/token×10^6token/s}=320$$
为什么重要： EP>100时传统网络崩溃，EP320证明UB突破通信可扩展性极限

INT8量化收益

公式：$$ 加速比=\frac {FP16吞吐}{INT8吞吐}×精度保持率$$
论文数据：

吞吐提升：1.8倍
精度损失：<0.5%（16个基准测试）


为什么重要： Decode阶段内存带宽减少50%，解决NPU的“内存墙”问题

QA辅助理解
为什么用TPOT而非QPS？

TPOT剥离Batch Size影响，纯粹衡量单次生成效率
更直观反映SLA（用户感知的延迟）

为什么强调计算效率而非绝对吞吐？

排除工艺优势（7nm vs 5nm），聚焦架构创新价值
1.29 tokens/s/TFLOPS → 证明UB+LEP设计优于NVLink+GPU

为什么测量远程DRAM访问延迟？

验证内存池化的实际效果，这是打破“内存墙”的核心
1.5μs延迟 → 实现“全集群如单机”的硬件基础

超节点架构
三级网络平面的物理隔离
硬件隔离原理"><meta name=author content="Me"><link rel=canonical href=https://pillumina.github.io/posts/aiinfra/01-ascend-cloudmatrix/><link crossorigin=anonymous href=/assets/css/stylesheet.9d388901283682bb45dd422fcaa0d0a2054a3c8ff47c9cc6b2baab15508b1b90.css integrity="sha256-nTiJASg2grtF3UIvyqDQogVKPI/0fJzGsrqrFVCLG5A=" rel="preload stylesheet" as=style><link rel=icon href=https://pillumina.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://pillumina.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://pillumina.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://pillumina.github.io/apple-touch-icon.png><link rel=mask-icon href=https://pillumina.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://pillumina.github.io/posts/aiinfra/01-ascend-cloudmatrix/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>const config={startOnLoad:!0,theme:"neutral",themeVariables:{lineColor:"#0f0f0f"},flowchart:{useMaxWidth:!1,htmlLabels:!0}};mermaid.initialize(config),window.onload=()=>{window.mermaid.init(0[0],document.querySelectorAll(".language-mermaid"))}</script><link rel=stylesheet href=/css/custom.min.7ca191baf9a98cba901e2771d1f5485af2e39a950ce60a50254e72e853eb373d.css integrity="sha256-fKGRuvmpjLqQHidx0fVIWvLjmpUM5gpQJU5y6FPrNz0="><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]"),n=document.querySelectorAll(".toc a");if(t.length===0||n.length===0)return;const s={};t.forEach(e=>{s[e.id]=e.offsetTop});function i(){const t=window.scrollY+100;let e="";for(const[n,o]of Object.entries(s))if(t>=o)e=n;else break;return e}function o(){const e=i();if(n.forEach(e=>{e.classList.remove("active")}),e){const t=document.querySelector(`.toc a[href="#${e}"]`);t&&t.classList.add("active")}}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){o(),e=!1}),e=!0)}),o()})</script><meta property="og:url" content="https://pillumina.github.io/posts/aiinfra/01-ascend-cloudmatrix/"><meta property="og:site_name" content="CctoctoFX"><meta property="og:title" content="昇腾超节点CloudMatrix384论文拆解"><meta property="og:description" content=" 6.19发布的CloudMatrix384论文拆解，从宏观到基础概念
核心指标和计算方式 TPOT (Time Per Output Token) 公式： $$TPOT= \frac{Decode总耗时}{生成Token数量}$$ 测量方式： 从第一个输出Token开始计时，到生成结束（含MoE通信/KV读取） 为什么重要： 直接决定用户体验（如Chatbot响应速度），论文要求 <50ms（严格模式<15ms） 深层意义： 反映系统通信+计算综合能力，EP320下TPOT=42ms证明UB网络突破MoE通信墙 计算效率 (Tokens/s per TFLOPS) 公式： $$计算效率=\frac {吞吐量(tokens/s)} {NPU峰值算力(TFLOPS)}$$​ 论文数据： 阶段 值 对比基准 Prefill 4.45 超NVIDIA H100+SGLang(3.8) Decode 1.29 超NVIDIA H800+DeepSeek(0.9) 为什么重要： 揭示硬件利用率，1.0以上表明软硬件协同极致优化 深层意义： Decode阶段1.29 → 昇腾910的Cube引擎利用率达 86%（传统GPU仅60%) 缓存访问延迟 (KV Cache Access Latency) 公式： $$延迟=TMMU_{查询}+TUB_{传输}+TDRAM_{读取}​$$ 论文数据： 场景 延迟 对比传统 本地HBM命中 0.2μs - 远程DRAM访问(UB) 1.5μs >10μs (PCIe+IB) 为什么重要： 长上下文推理中70%时间花在KV缓存访问 深层意义： UB统一内存将远程访问性能提升至近本地水平，支撑百万Token上下文。 专家并行扩展性 (EP Degree) 定义：单个MoE层可分布的专家数量 论文突破：EP320（每个昇腾Die托管1个专家） 支撑公式： $$可扩展性=\frac {UB总带宽}{单个专家通信需求}$$ $$EPmax=\frac {384×392GB/s} {8B/token×10^6token/s}=320$$ 为什么重要： EP>100时传统网络崩溃，EP320证明UB突破通信可扩展性极限 INT8量化收益 公式：$$ 加速比=\frac {FP16吞吐}{INT8吞吐}×精度保持率$$ 论文数据： 吞吐提升：1.8倍 精度损失：<0.5%（16个基准测试） 为什么重要： Decode阶段内存带宽减少50%，解决NPU的“内存墙”问题 QA辅助理解 为什么用TPOT而非QPS？ TPOT剥离Batch Size影响，纯粹衡量单次生成效率 更直观反映SLA（用户感知的延迟） 为什么强调计算效率而非绝对吞吐？ 排除工艺优势（7nm vs 5nm），聚焦架构创新价值 1.29 tokens/s/TFLOPS → 证明UB+LEP设计优于NVLink+GPU 为什么测量远程DRAM访问延迟？ 验证内存池化的实际效果，这是打破“内存墙”的核心 1.5μs延迟 → 实现“全集群如单机”的硬件基础 超节点架构 三级网络平面的物理隔离 硬件隔离原理"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-07T10:40:12+08:00"><meta property="article:modified_time" content="2025-08-07T10:40:12+08:00"><meta property="og:image" content="https://pillumina.github.io/imgs/icon_head.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://pillumina.github.io/imgs/icon_head.png"><meta name=twitter:title content="昇腾超节点CloudMatrix384论文拆解"><meta name=twitter:description content="
6.19发布的CloudMatrix384论文拆解，从宏观到基础概念
核心指标和计算方式
TPOT (Time Per Output Token)

公式： $$TPOT= \frac{Decode总耗时}{生成Token数量}$$
测量方式： 从第一个输出Token开始计时，到生成结束（含MoE通信/KV读取）
为什么重要： 直接决定用户体验（如Chatbot响应速度），论文要求 <50ms（严格模式<15ms）
深层意义： 反映系统通信+计算综合能力，EP320下TPOT=42ms证明UB网络突破MoE通信墙

计算效率 (Tokens/s per TFLOPS)

公式： $$计算效率=\frac {吞吐量(tokens/s)} {NPU峰值算力(TFLOPS)}$$​
论文数据：


  
      
          阶段
          值
          对比基准
      
  
  
      
          Prefill
          4.45
          超NVIDIA H100+SGLang(3.8)
      
      
          Decode
          1.29
          超NVIDIA H800+DeepSeek(0.9)
      
  


为什么重要： 揭示硬件利用率，1.0以上表明软硬件协同极致优化
深层意义： Decode阶段1.29 → 昇腾910的Cube引擎利用率达 86%（传统GPU仅60%)

缓存访问延迟 (KV Cache Access Latency)

公式： $$延迟=TMMU_{查询}+TUB_{传输}+TDRAM_{读取}​$$
论文数据：


  
      
          场景
          延迟
          对比传统
      
  
  
      
          本地HBM命中
          0.2μs
          -
      
      
          远程DRAM访问(UB)
          1.5μs
          >10μs (PCIe+IB)
      
  


为什么重要： 长上下文推理中70%时间花在KV缓存访问
深层意义： UB统一内存将远程访问性能提升至近本地水平，支撑百万Token上下文。

专家并行扩展性 (EP Degree)

定义：单个MoE层可分布的专家数量
论文突破：EP320（每个昇腾Die托管1个专家）
支撑公式： $$可扩展性=\frac {UB总带宽}{单个专家通信需求}$$ $$EPmax=\frac {384×392GB/s} {8B/token×10^6token/s}=320$$
为什么重要： EP>100时传统网络崩溃，EP320证明UB突破通信可扩展性极限

INT8量化收益

公式：$$ 加速比=\frac {FP16吞吐}{INT8吞吐}×精度保持率$$
论文数据：

吞吐提升：1.8倍
精度损失：<0.5%（16个基准测试）


为什么重要： Decode阶段内存带宽减少50%，解决NPU的“内存墙”问题

QA辅助理解
为什么用TPOT而非QPS？

TPOT剥离Batch Size影响，纯粹衡量单次生成效率
更直观反映SLA（用户感知的延迟）

为什么强调计算效率而非绝对吞吐？

排除工艺优势（7nm vs 5nm），聚焦架构创新价值
1.29 tokens/s/TFLOPS → 证明UB+LEP设计优于NVLink+GPU

为什么测量远程DRAM访问延迟？

验证内存池化的实际效果，这是打破“内存墙”的核心
1.5μs延迟 → 实现“全集群如单机”的硬件基础

超节点架构
三级网络平面的物理隔离
硬件隔离原理"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://pillumina.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI Infra","item":"https://pillumina.github.io/posts/aiinfra/"},{"@type":"ListItem","position":3,"name":"昇腾超节点CloudMatrix384论文拆解","item":"https://pillumina.github.io/posts/aiinfra/01-ascend-cloudmatrix/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"昇腾超节点CloudMatrix384论文拆解","name":"昇腾超节点CloudMatrix384论文拆解","description":" 6.19发布的CloudMatrix384论文拆解，从宏观到基础概念\n核心指标和计算方式 TPOT (Time Per Output Token) 公式： $$TPOT= \\frac{Decode总耗时}{生成Token数量}$$ 测量方式： 从第一个输出Token开始计时，到生成结束（含MoE通信/KV读取） 为什么重要： 直接决定用户体验（如Chatbot响应速度），论文要求 \u0026lt;50ms（严格模式\u0026lt;15ms） 深层意义： 反映系统通信+计算综合能力，EP320下TPOT=42ms证明UB网络突破MoE通信墙 计算效率 (Tokens/s per TFLOPS) 公式： $$计算效率=\\frac {吞吐量(tokens/s)} {NPU峰值算力(TFLOPS)}$$​ 论文数据： 阶段 值 对比基准 Prefill 4.45 超NVIDIA H100+SGLang(3.8) Decode 1.29 超NVIDIA H800+DeepSeek(0.9) 为什么重要： 揭示硬件利用率，1.0以上表明软硬件协同极致优化 深层意义： Decode阶段1.29 → 昇腾910的Cube引擎利用率达 86%（传统GPU仅60%) 缓存访问延迟 (KV Cache Access Latency) 公式： $$延迟=TMMU_{查询}+TUB_{传输}+TDRAM_{读取}​$$ 论文数据： 场景 延迟 对比传统 本地HBM命中 0.2μs - 远程DRAM访问(UB) 1.5μs \u0026gt;10μs (PCIe+IB) 为什么重要： 长上下文推理中70%时间花在KV缓存访问 深层意义： UB统一内存将远程访问性能提升至近本地水平，支撑百万Token上下文。 专家并行扩展性 (EP Degree) 定义：单个MoE层可分布的专家数量 论文突破：EP320（每个昇腾Die托管1个专家） 支撑公式： $$可扩展性=\\frac {UB总带宽}{单个专家通信需求}$$ $$EPmax=\\frac {384×392GB/s} {8B/token×10^6token/s}=320$$ 为什么重要： EP\u0026gt;100时传统网络崩溃，EP320证明UB突破通信可扩展性极限 INT8量化收益 公式：$$ 加速比=\\frac {FP16吞吐}{INT8吞吐}×精度保持率$$ 论文数据： 吞吐提升：1.8倍 精度损失：\u0026lt;0.5%（16个基准测试） 为什么重要： Decode阶段内存带宽减少50%，解决NPU的“内存墙”问题 QA辅助理解 为什么用TPOT而非QPS？ TPOT剥离Batch Size影响，纯粹衡量单次生成效率 更直观反映SLA（用户感知的延迟） 为什么强调计算效率而非绝对吞吐？ 排除工艺优势（7nm vs 5nm），聚焦架构创新价值 1.29 tokens/s/TFLOPS → 证明UB+LEP设计优于NVLink+GPU 为什么测量远程DRAM访问延迟？ 验证内存池化的实际效果，这是打破“内存墙”的核心 1.5μs延迟 → 实现“全集群如单机”的硬件基础 超节点架构 三级网络平面的物理隔离 硬件隔离原理\n","keywords":[],"articleBody":" 6.19发布的CloudMatrix384论文拆解，从宏观到基础概念\n核心指标和计算方式 TPOT (Time Per Output Token) 公式： $$TPOT= \\frac{Decode总耗时}{生成Token数量}$$ 测量方式： 从第一个输出Token开始计时，到生成结束（含MoE通信/KV读取） 为什么重要： 直接决定用户体验（如Chatbot响应速度），论文要求 \u003c50ms（严格模式\u003c15ms） 深层意义： 反映系统通信+计算综合能力，EP320下TPOT=42ms证明UB网络突破MoE通信墙 计算效率 (Tokens/s per TFLOPS) 公式： $$计算效率=\\frac {吞吐量(tokens/s)} {NPU峰值算力(TFLOPS)}$$​ 论文数据： 阶段 值 对比基准 Prefill 4.45 超NVIDIA H100+SGLang(3.8) Decode 1.29 超NVIDIA H800+DeepSeek(0.9) 为什么重要： 揭示硬件利用率，1.0以上表明软硬件协同极致优化 深层意义： Decode阶段1.29 → 昇腾910的Cube引擎利用率达 86%（传统GPU仅60%) 缓存访问延迟 (KV Cache Access Latency) 公式： $$延迟=TMMU_{查询}+TUB_{传输}+TDRAM_{读取}​$$ 论文数据： 场景 延迟 对比传统 本地HBM命中 0.2μs - 远程DRAM访问(UB) 1.5μs \u003e10μs (PCIe+IB) 为什么重要： 长上下文推理中70%时间花在KV缓存访问 深层意义： UB统一内存将远程访问性能提升至近本地水平，支撑百万Token上下文。 专家并行扩展性 (EP Degree) 定义：单个MoE层可分布的专家数量 论文突破：EP320（每个昇腾Die托管1个专家） 支撑公式： $$可扩展性=\\frac {UB总带宽}{单个专家通信需求}$$ $$EPmax=\\frac {384×392GB/s} {8B/token×10^6token/s}=320$$ 为什么重要： EP\u003e100时传统网络崩溃，EP320证明UB突破通信可扩展性极限 INT8量化收益 公式：$$ 加速比=\\frac {FP16吞吐}{INT8吞吐}×精度保持率$$ 论文数据： 吞吐提升：1.8倍 精度损失：\u003c0.5%（16个基准测试） 为什么重要： Decode阶段内存带宽减少50%，解决NPU的“内存墙”问题 QA辅助理解 为什么用TPOT而非QPS？ TPOT剥离Batch Size影响，纯粹衡量单次生成效率 更直观反映SLA（用户感知的延迟） 为什么强调计算效率而非绝对吞吐？ 排除工艺优势（7nm vs 5nm），聚焦架构创新价值 1.29 tokens/s/TFLOPS → 证明UB+LEP设计优于NVLink+GPU 为什么测量远程DRAM访问延迟？ 验证内存池化的实际效果，这是打破“内存墙”的核心 1.5μs延迟 → 实现“全集群如单机”的硬件基础 超节点架构 三级网络平面的物理隔离 硬件隔离原理\ngraph TD subgraph Ascend节点 NPU[NPU计算单元] --\u003e|直连| L1_SW[L1 UB交换芯片] CPU[Kunpeng CPU] --\u003e|直连| L1_SW L1_SW --\u003e|专用光纤| L2_SW[L2 UB交换芯片] NPU --\u003e|专用SerDes| RDMA_NIC[RDMA网卡] RDMA_NIC --\u003e|RoCE协议| External_RDMA[外部RDMA网络] CPU --\u003e|PCIe| Qingtian[Qingtian DPU] Qingtian --\u003e|以太网| VPC_Switch[VPC交换机] end 隔离关键点：\n物理链路分离：\nUB平面：NPU/CPU → L1交换芯片 → 专用光纤 → L2交换芯片（通信机柜） RDMA平面：NPU → 板载RDMA SerDes接口 → 外部RoCE网络 VPC平面：CPU → Qingtian DPU → 标准以太网交换机 协议栈隔离：\nUB协议：自定义帧格式（128B载荷+8B路由头），硬件加速 RDMA协议：RoCEv2（兼容InfiniBand生态） VPC协议：TCP/IP + UBoE扩展 流量管控：\nMatrixLink组件：在Qingtian DPU上实现QoS策略 UB平面：最高优先级（保障MoE/KV缓存流量） RDMA平面：中等优先级（训练/推理数据同步） VPC平面：最低优先级（管理/存储流量） 三级平面分层设计价值 解决的关键问题对比\n挑战场景 UB平面解决方案 RDMA平面解决方案 VPC平面解决方案 MoE的Token分发(All-to-All) 全互联拓扑延迟\u003c1.2μs 不适用 不适用 跨节点KV缓存同步 本地化优先（延迟最优） 200Gbps带宽同步 不适用 模型冷启动加载 从内存池直接加载（1.5μs） 从远端存储加载（\u003e10ms） 从对象存储加载（\u003e100ms） 用户请求接入 不适用 不适用 以太网/IP协议接入 分层逻辑： 性能敏感层（UB）： 承载95%的AI内部通信流量 硬件级隔离保障亚微秒延迟 扩展兼容层（RDMA）： 解决超节点间数据同步 兼容行业标准（RoCEv2） 生态接入层（VPC）： 无缝对接现有云设施 通过UBoE逐步增强性能 超节点物理部署架构 graph TB subgraph 超节点CloudMatrix384 subgraph 12个计算机柜 subgraph 节点1 NPU1_0 --\u003e L1_SW1[L1 UB芯片] ... --\u003e L1_SW1 L1_SW1 --\u003e|平面1| L2_SW1[L2 UB交换机] L1_SW1 --\u003e|平面7| L2_SW7 end subgraph 节点48 NPU48_0 --\u003e L1_SW48 L1_SW48 --\u003e|平面1| L2_SW1 L1_SW48 --\u003e|平面7| L2_SW7 end end subgraph 4个通信机柜 L2_SW1[L2 UB交换机组] --\u003e|全互联| 所有L1芯片 L2_SW7[L2 UB交换机组] --\u003e|全互联| 所有L1芯片 end RDMA_NIC组[RDMA网卡集群] --\u003e|RoCE| 跨超节点网络 Qingtian_DPU组 --\u003e|以太网| 数据中心核心交换机 end 关键说明：\n计算部分：\n48个节点（384 NPU + 192 CPU）部署在12个机柜 每个节点含7个L1 UB交换芯片（共48×7=336个L1芯片） 通信部分：\n4个专用通信机柜部署L2 UB交换机组 7个独立子平面 × 16个L2芯片 = 112个L2交换机 每个L1芯片直连16个L2芯片（总链路数=336×16=5,376） 扩展接口：\nRDMA出口：NPU直连的RoCE网卡集群 VPC出口：CPU连接的Qingtian DPU集群 逻辑全局架构图（硬件+软件视图） graph LR subgraph 逻辑资源池 direction TB NPU_Pool[NPU计算池] --\u003e|UB访问| Memory_Pool[全局内存池] CPU_Pool[CPU资源池] --\u003e|UB访问| Memory_Pool Memory_Pool --\u003e|存储| KV_Cache[分布式KV缓存] NPU_Pool --\u003e|RDMA| Other_Supernode[其他超节点] CPU_Pool --\u003e|VPC| DC_Network[数据中心网络] end subgraph 软件服务层 Prefill[Prefill集群] --\u003e|读取| KV_Cache Decode[Decode集群] --\u003e|更新| KV_Cache Cache_Mgr[缓存管理器] --\u003e|调度| KV_Cache MatrixResource --\u003e|资源编排| NPU_Pool MatrixLink --\u003e|网络策略| Memory_Pool end 架构解析：\n硬件抽象层：\n全局内存池：物理分散的NPU HBM + CPU DRAM → 逻辑统一地址空间 动态资源池：NPU/CPU按需组成Prefill/Decode集群（如EP320） 网络平面映射：\nUB平面：承载内存池访问/MoE通信（图中实线） RDMA平面：跨超节点KV同步（虚线） VPC平面：外部服务接入（点划线） 软件控制层：\nMatrixResource：拓扑感知的资源编排（如动态分配160 NPU给Decode） MatrixLink：UB网络QoS保障（优先MoE流量） UB统一总线理解 UB网络核心设计目标 解决传统分布式系统的通信瓶颈：\n问题：MoE模型中的Token分发（Token Dispatch）和专家输出合并（Expert Output Combination）需高频All-to-All通信，传统树状网络（如InfiniBand）因多级转发导致延迟\u003e3μs。 目标：通过全互联直连拓扑实现亚微秒级延迟（\u003c1μs）和接近0带宽衰减。 UB网络物理架构与数据流动 硬件拓扑结构 graph TD subgraph 超节点内通信 A[Ascend 910节点] --\u003e|L1 UB交换芯片| B[L2 UB交换平面] B --\u003e|全互联| C[其他Ascend节点] end L1交换层：每个Ascend 910节点（含8 NPU）配备7个L1 UB交换芯片。 L2交换层：超节点分为7个独立子平面，每个子平面含16个L2 UB交换芯片。 连接方式： 每个L1芯片直连对应子平面的所有16个L2芯片（避免带宽阻塞）。 带宽保障：节点内部带宽（392 GB/s） = L1→L2总带宽（392 GB/s × 7）。 数据流动示例\n假设NPU0需将Token发送至专家255（位于NPU255）： sequenceDiagram participant NPU0 as NPU0（源） participant L1 as L1交换芯片（节点内） participant L2 as L2交换芯片（子平面） participant NPU255 as NPU255（目标） NPU0-\u003e\u003eL1: 发送Token数据（含专家ID=255） L1-\u003e\u003eL2: 根据专家ID选择子平面（如平面#3） L2-\u003e\u003eNPU255: 直连转发至目标NPU NPU255-\u003e\u003eL2: 返回确认信号 L2-\u003e\u003eL1: 回传确认 L1-\u003e\u003eNPU0: 完成传输 关键优化： 零路由表查询：硬件预配置专家ID与L2子平面的映射关系，避免软件寻址开销。 物理直连：L1→L2、L2→目标NPU均为点到点光纤直连，无中间交换机。 通信协议栈与硬件加速 UB协议栈分层设计 层级 功能 物理层 112 Gbps SerDes接口，64B/66B编码，支持392 GB/s单向带宽 链路层 硬件事务拆分（Chunking）和重组，支持最大8KB报文 传输层 零拷贝DMA引擎，绕过CPU和OS内核（类似GPUDirect RDMA） 应用层 融合通信算子（如MoE的Token Dispatch/Combination封装为单条硬件指令） MoE通信的硬件融合 传统流程 graph LR A[Token分发] --\u003e B[专家计算] --\u003e C[输出合并] 需两次独立的All-to-All通信（延迟翻倍）\nUB优化流程 graph LR A[Token分发 + 输出合并] --\u003e|单次融合操作| B[专家计算] 硬件指令：昇腾910内置MOE_FUSED_COMM指令，将两次通信合并为一次。 带宽节省：Token元数据（专家ID）与输出张量共享同一缓存区。 关键场景性能对比 ALL-to-ALL延迟（256 NPU） 架构 延迟（μs） 瓶颈原因 InfiniBand HDR 12.3 多级交换机转发 + 协议栈开销 UB网络 1.2 物理直连 + 硬件融合通信 KV 缓存访问流程 flowchart TD A[NPU需读取历史KV块] --\u003e B{本地HBM命中？} B -- 未命中 --\u003e C[通过UB访问远程DRAM池] C --\u003e D[从CPU内存池直接读取] D --\u003e E[数据返回NPU] 性能优势： 远程DRAM访问延迟≈1.5μs（对比InfiniBand的\u003e10μs）。 带宽利用率\u003e90%（传统方案\u003c50%）。 创新点 1. 全互联拓扑的本质 物理层：用两级星型结构（L1局部全连 + L2分组全连）逼近FullMesh性能，避免O(N²)链路复杂度。 协议层：将通信模式抽象为硬件指令（如ALL_TO_ALL），由NPU通信引擎直接执行。 2. 与NVIDIA方案的对比 特性 NVIDIA NVLink/Switch Huawei UB 拓扑 单节点全连，跨节点树状 超节点内全互联 延迟 节点内0.5μs，跨节点\u003e3μs 全节点\u003c1μs MoE支持 EP144需复杂流水线掩盖延迟 原生支持EP320 内存池化 仅GPU显存共享 NPU+CPU全局内存池 UB网络数据流动全貌 graph LR subgraph CloudMatrix384超节点 direction TB subgraph Node1[Ascend节点1] NPU1_0 --\u003e L1_1[L1交换芯片] NPU1_1 --\u003e L1_1 ... --\u003e L1_1 L1_1 --\u003e|平面1| L2_1[L2交换芯片群] L1_1 --\u003e|平面2| L2_2 L1_1 --\u003e|平面7| L2_7 end subgraph Node48[Ascend节点48] NPU48_0 --\u003e L1_48 ... L1_48 --\u003e|平面1| L2_1 L1_48 --\u003e|平面7| L2_7 end L2_1 --\u003e NPU48_0 L2_7 --\u003e NPU1_0 end 箭头方向：数据通过L1/L2芯片跨节点直连。 带宽保障：每个L1芯片的7条链路独立工作，总带宽=7×392 GB/s。 QA辅助理解 什么是星型拓扑？UB网络是纯星型吗？ 经典星型拓扑：所有节点连接至中心交换机，优点是布线简单，缺点是中心节点易成瓶颈（单点故障/带宽限制）。 UB的改良设计： graph TB subgraph L2子平面 L2_SW1[L2交换机1] L2_SW2[L2交换机2] end NPU0 --\u003e|直连| L1_SW[节点内L1交换机] L1_SW --\u003e|全互联| L2_SW1 L1_SW --\u003e|全互联| L2_SW2 两级星型混合：节点内8 NPU → 7个L1交换机（星型），L1交换机 → 16个L2交换机（全互联）。 优势：用O(N)布线复杂度逼近FullMesh性能，避免中心瓶颈（L2分散负载）。 为什么UB能实现“跨节点延迟\u003c1μs”？ 协议硬化 + 物理直连\n步骤 传统InfiniBand UB优化 数据发出 NPU → 驱动 → 内核协议栈 NPU直连SerDes接口 跨节点路由 多级交换机转发（3-5跳） L1→L2单跳直达（物理直连） 数据接收 内核协议栈 → 驱动 → NPU 直写NPU缓存（DMA引擎） 典型延迟 \u003e3μs 0.8-1.2μs 关键创新： 跳过操作系统协议栈（类似RDMA但更底层） 交换机无路由查找（预配置专家ID→平面映射） 融合通信算子如何减少MoE通信开销？ 合并两次All-to-All为单次硬件事务\n1 2 3 4 5 6 // 昇腾910硬件指令 moe_fused_comm( input_tokens, expert_mask, // 携带路由信息 output_buffer // 预留专家输出空间 ); 效果：单次通信完成分发+合并，延迟降至5μs内（EP320实测）。 什么是“零拷贝DMA”？UB如何实现？ 内存直接访问，跳过CPU复制\n传统流程（以KV缓存读取为例）： 远程DRAM → 网卡 → 主机内存 → PCIe → NPU内存（3次拷贝） UB零拷贝： 远程DRAM → UB网络 → NPU计算单元 硬件支持： NPU集成DMA引擎，直接发起远程内存读写请求 UB交换机维护全局内存地址表（类似CC-NUMA） 带宽利用率：从**\u003c50%（InfiniBand）提升至\u003e90%** 为什么需要7个L2子平面？ 匹配Ascend 910芯片接口，避免带宽阻塞\n数学关系： 单NPU带宽需求：392 GB/s 单L1交换机上行带宽 = 7链路 × 56 GB/s = 392 GB/s 每个L1需直连7组L2才能满足带宽（7×56=392） 容错设计： 任一L2子平面故障不影响其他平面通信 动态路由切换（MatrixLink组件） 分布式KV缓存在UB上如何工作？ 全局内存池 + 硬件一致性协议\nflowchart LR NPU1 --\u003e|UB读请求| CachePool[CPU DRAM池] CachePool --\u003e|返回数据| NPU1 NPU2 --\u003e|并行写| CachePool 一致性保障： 硬件级MESI协议（缓存行锁定） 写冲突时L2交换机仲裁（优先本地化写入） 性能对比：\n场景 传统方案延迟 UB延迟 跨节点读缓存 10μs+ 1.5 UB如何支持动态负载？ 资源池化 + 软件定义拓扑\n硬件基础： NPU/CPU/内存物理解耦 UB网络虚拟化（VXLAN类似技术） 动态调整示例： 1 2 3 4 # 突发流量时扩容Decode集群 if request_queue \u003e threshold: assign_new_npus(\"Decode\", 40) # 从空闲池分配40 NPU update_routing_table() # 更新专家路由表 效果：10秒内扩容EP40组，TPOT仍\u003c50ms UB与NVIDIA NVLink本质区别？ 全集群统一总线 vs 节点内互联\n维度 NVLink/Switch UB 拓扑范围 单节点（8 GPU） 384 NPU全域 内存模型 仅GPU显存共享 CPU+NPU全局池化 扩展方式 树状扩展（带宽衰减） 全互联扩展 延迟一致性 跨节点\u003e3μs 全域\u003c1μs 为什么需要专用L1/L2交换芯片？ 解决SerDes电气限制 + 协议卸载\n电气层： 单SerDes链路极限56 GB/s → 需多链路并行 L1芯片聚合8 NPU流量（392 GB/s） 协议层： 卸载路由计算、CRC校验、重传机制 NPU无需处理通信协议（算力100%用于AI） 融合算子具体如何节省带宽？ 元数据与有效载荷合并编码\n传统两次通信： Token分发：发送{token_data, expert_id} 输出合并：发送{expert_output, token_id} 总数据量：2×(数据+ID) UB融合通信： 单次发送{token_data, expert_id, output_buffer_addr} 节省：减少50% ID字段传输，带宽需求下降35% UB如何保障大规模组网的信号完整性？ 光电混合 + 时钟同步技术\n挑战：384 NPU全互联需数万链路，电信号衰减严重 华为方案： 关键路径用光纤替代铜缆（L2交换机间） 全局时钟分发网络（误差\u003c5ps） 接收端自适应均衡（CTLE+DFE） 为什么说UB是“AI原生网络”？ 硬件指令级AI通信原语\n1 2 3 4 5 6 // Ascend 910指令集扩展 ascend_moe_dispatch( expert_mask, // 门控网络输出 token_ptr, // 输入token地址 ub_channel_id // 指定UB虚拟通道 ); 与AI计算流水线深度耦合： 通信操作作为AI计算图的一部分编译 立方体引擎(Cube)执行FFN时自动触发通信 对比传统网络：需CPU调度MPI库，上下文切换开销大 UB全局内存理解 基础概念：KV缓存的核心挑战 问题本质 KV缓存：存储Transformer历史注意力状态（Key/Value向量），用于长上下文推理 挑战： 百万token上下文需TB级内存 → NPU本地HBM（仅32GB）无法容纳 传统方案：跨节点复制数据 → 带宽瓶颈 \u0026 高延迟 传统方案（NVIDIA架构） graph LR A[GPU0] --\u003e|NVLink| B[GPU1本地HBM] B --\u003e|PCIe| C[CPU内存] C --\u003e|InfiniBand| D[远程节点内存] 访问流程： 缺失的KV块从远程节点复制到本地HBM GPU计算单元读取本地HBM 痛点： 高延迟：跨节点复制链路过长（\u003e10μs） 低带宽：InfiniBand带宽（200Gbps） « HBM带宽（3TB/s） 数据孤岛：GPU显存无法直接共享 华为UB全局内存的硬件基础 物理架构：三级资源池化 graph TB subgraph CloudMatrix384 NPU_HBM[NPU HBM] --\u003e UB CPU_DRAM[CPU DRAM池] --\u003e UB UB --\u003e|统一编址| Global_Memory[全局内存空间] end 核心组件： NPU HBM：昇腾910双Die共32GB，带宽3.2TB/s（本地高速缓存） CPU DRAM池：192鲲鹏CPU的TB级内存（主存储池） UB网络：392GB/s全互联，延迟\u003c1μs（数据通路） 地址映射硬件（类似CC-NUMA） flowchart TD NPU[NPU计算单元] --\u003e MMU[内存管理单元] MMU --\u003e|本地地址？| Local{地址检查} Local --\u003e|是| HBM[本地HBM] Local --\u003e|否| UB_Interface[UB网络接口] UB_Interface --\u003e|全局地址| UB_Switch[UB交换机] UB_Switch --\u003e|路由| Target[目标内存设备] 关键硬件： 全局地址表：存储在UB交换机中，记录物理内存位置 零拷贝DMA：NPU直接发起远程内存读写，无需CPU参与 缓存一致性协议（硬件加速MESI） 挑战：多NPU并发修改同一KV块 解决方案： UB交换机内置一致性引擎 基于物理地址的缓存行锁定（64B粒度） 写冲突时优先本地化处理（Locality-aware） 软件层实现：全局内存访问原理 内存分配（软件API） 1 2 3 4 5 6 7 8 // 在DRAM池分配分布式KV缓存 kv_cache_t* kv_buf = ub_mem_alloc_pooled( UBMEM_DRAM_POOL, // 内存池类型 1024 * 1024 * 1024, // 1GB空间 KV_CACHE_TAG // 标记为KV缓存 ); // NPU直接映射到地址空间 void* npu_virt_addr = ub_map_remote(kv_buf); 关键操作： ub_mem_alloc_pooled：从DRAM池分配物理内存 ub_map_remote：将远程内存映射到NPU虚拟地址空间 KV缓存读取流程 sequenceDiagram NPU-\u003e\u003eUB_MMU: 发起KV块读取(虚拟地址0xFFFF1000) UB_MMU-\u003e\u003eGlobal_Addr_Table: 查询物理位置(Node15, DRAM_Offset0x8000) Global_Addr_Table--\u003e\u003eUB_MMU: 返回目标地址 UB_MMU-\u003e\u003eUB_Switch: 发送DMA读请求 UB_Switch-\u003e\u003eNode15_DRAM: 读取数据 Node15_DRAM-\u003e\u003eUB_Switch: 返回数据 UB_Switch-\u003e\u003eNPU: 直写NPU寄存器 性能关键： 全程无CPU参与 数据不经过本地HBM（避免复制） 延迟仅 1.5μs（对比NVIDIA \u003e10μs） 缓存服务架构 graph TD Prefill[Prefill NPU] --\u003e|写入新KV块| Cache[DRAM缓存池] Decode[Decode NPU] --\u003e|读取历史块| Cache Cache_Manager[缓存管理器] --\u003e|元数据维护| Cache 去中心化设计： 无全局调度器 → 各NPU通过UB自主访问 缓存位置对软件透明 与NVIDIA方案的对比 架构差异全景图 维度 NVIDIA方案 华为UB方案 硬件基础 GPU显存隔离 + 分层网络 NPU/CPU内存池 + UB全互联 远程访问 显式复制（PCIe→IB→HBM） 直接内存映射（零拷贝） 地址空间 每GPU独立虚拟地址 全域统一虚拟地址 调度约束 请求需路由到持有KV块的GPU 任意NPU可直接访问任意KV块 典型延迟 \u003e10μs（跨节点） 1.5μs（全域） 峰值带宽 ≤200Gbps（InfiniBand） 392GB/s（单NPU单向） 工程实现关键创新 硬件层：DRAM-NPU带宽均衡 问题：DRAM带宽（256GB/s）\u003c NPU需求（392GB/s） 解决方案： 分布式条带化：将KV块切分存储到多CPU内存 并发访问：NPU同时从8个DRAM节点读取 协议层：轻量级一致性控制 优化点： KV缓存只读居多 → 放宽一致性要求 写操作仅限Prefill NPU → 减少协议开销 软件层：缓存感知的KV布局 1 2 3 4 5 6 7 8 // 按Attention Head分片存储 for (int head=0; head\u003cnum_heads; head++) { ub_mem_store(kv_cache, head * head_size, // 按Head偏移 data, UB_CACHE_AWARE // 标记为缓存友好布局 ); } 效果： 单次读取可获取完整Attention Head数据 减少随机访问导致的缓存行浪费 QA辅助理解 UB全局内存的硬件工作原理? 三级硬件协同\n1. NPU内存管理单元（MMU）\n接收虚拟地址请求 查询全局地址表（存储在UB交换机） 2. UB交换机\n维护全局地址→物理位置映射 路由请求到目标设备（DRAM或HBM） 3. DMA引擎\n在目标设备执行直接内存访问 数据通过UB网络直送请求方NPU 软件层如何使用UB全局内存？（开发者视角） 透明化API + 缓存优化\n开发者API示例\n1 2 3 4 5 6 7 8 9 10 // 分配分布式KV缓存 kv_cache_t* kv_buf = ub_mem_alloc_pooled( UBMEM_DRAM_POOL, // 指定DRAM池 1024*1024*1024, // 1GB空间 KV_CACHE_TAG // 标记为KV缓存 ); // NPU直接映射远程内存 void* npu_vaddr = ub_map_remote(kv_buf); // 像本地内存一样使用 attention_compute(npu_vaddr + offset); 缓存感知优化\n按Attention Head分片存储： 1 2 for head_idx in range(num_heads): store_kv_block(kv_cache, head_idx * head_size, data) 单次读取获取完整Head数据，减少随机访问 与NVIDIA方案性能对比？数据差异多大？ 数量级提升\n指标 NVIDIA H800 Huawei UB 提升倍数 远程读延迟 \u003e10 μs 1.5 μs 6.7× 有效带宽利用率 \u003c50% \u003e90% 1.8× KV缓存容量 单节点限制 数十TB ∞ 一致性管理开销 软件实现（高开销） 硬件MESI 10×效率 UB如何处理多NPU并发修改同一缓存？ 硬件加速的MESI协议\n缓存行锁定：64B为粒度，写操作时独占锁定 仲裁策略： 读操作：并行执行（无冲突） 写操作：UB交换机按物理位置优先级仲裁 本地节点优先 → 减少网络传输 冲突案例： NPU A和C同时请求写同一KV块 UB交换机检测冲突，赋予Node_A优先权 NPU A完成写后通知NPU C失效旧副本 注意：KV缓存以只读为主（Decode阶段），写冲突率\u003c0.1%。\n为什么CPU DRAM池比NPU HBM更适合存储KV缓存？ 容量与成本的完美平衡\n介质 优势 劣势 适用场景 NPU HBM 超高速（3.2TB/s） 容量小（32GB/NPU） 热点缓存 CPU DRAM 超大容量（TB级） 较慢（256GB/s） 主KV存储池 智能分层： 活跃KV缓存 → 自动迁移至NPU HBM 历史KV缓存 → 存储在CPU DRAM池 ","wordCount":"1211","inLanguage":"en","image":"https://pillumina.github.io/imgs/icon_head.png","datePublished":"2025-08-07T10:40:12+08:00","dateModified":"2025-08-07T10:40:12+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://pillumina.github.io/posts/aiinfra/01-ascend-cloudmatrix/"},"publisher":{"@type":"Organization","name":"CctoctoFX","logo":{"@type":"ImageObject","url":"https://pillumina.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://pillumina.github.io/ accesskey=h title="CctoctoFX (Alt + H)"><img src=https://pillumina.github.io/apple-touch-icon.png alt aria-label=logo height=30>CctoctoFX</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://pillumina.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://pillumina.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://pillumina.github.io/posts/aiinfra title="AI Infra"><span>AI Infra</span></a></li><li><a href=https://pillumina.github.io/posts/programming title=Programming><span>Programming</span></a></li><li><a href=https://pillumina.github.io/social title=Social><span>Social</span></a></li><li><a href=https://pillumina.github.io/open_courses title=Study><span>Study</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://pillumina.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/aiinfra/>AI Infra</a></div><h1 class="post-title entry-hint-parent">昇腾超节点CloudMatrix384论文拆解</h1><div class=post-meta><span title='2025-08-07 10:40:12 +0800 CST'>August 7, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1211 words&nbsp;·&nbsp;Me</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#核心指标和计算方式>核心指标和计算方式</a><ul><li><a href=#tpot-time-per-output-token><strong>TPOT (Time Per Output Token)</strong></a></li><li><a href=#计算效率-tokenss-per-tflops><strong>计算效率 (Tokens/s per TFLOPS)</strong></a></li><li><a href=#缓存访问延迟-kv-cache-access-latency><strong>缓存访问延迟 (KV Cache Access Latency)</strong></a></li><li><a href=#专家并行扩展性-ep-degree><strong>专家并行扩展性 (EP Degree)</strong></a></li><li><a href=#int8量化收益><strong>INT8量化收益</strong></a></li><li><a href=#qa辅助理解>QA辅助理解</a></li></ul></li><li><a href=#超节点架构>超节点架构</a><ul><li><a href=#三级网络平面的物理隔离>三级网络平面的物理隔离</a></li><li><a href=#三级平面分层设计价值>三级平面分层设计价值</a></li><li><a href=#超节点物理部署架构>超节点物理部署架构</a></li><li><a href=#逻辑全局架构图硬件软件视图>逻辑全局架构图（硬件+软件视图）</a></li></ul></li><li><a href=#ub统一总线理解>UB统一总线理解</a><ul><li><a href=#ub网络核心设计目标>UB网络核心设计目标</a></li><li><a href=#ub网络物理架构与数据流动>UB网络物理架构与数据流动</a></li><li><a href=#通信协议栈与硬件加速>通信协议栈与硬件加速</a></li><li><a href=#关键场景性能对比>关键场景性能对比</a></li><li><a href=#创新点>创新点</a></li><li><a href=#ub网络数据流动全貌>UB网络数据流动全貌</a></li><li><a href=#qa辅助理解-1>QA辅助理解</a></li></ul></li><li><a href=#ub全局内存理解>UB全局内存理解</a><ul><li><a href=#基础概念kv缓存的核心挑战>基础概念：KV缓存的核心挑战</a></li><li><a href=#华为ub全局内存的硬件基础>华为UB全局内存的硬件基础</a></li><li><a href=#软件层实现全局内存访问原理>软件层实现：全局内存访问原理</a></li><li><a href=#与nvidia方案的对比>与NVIDIA方案的对比</a></li><li><a href=#工程实现关键创新>工程实现关键创新</a></li><li><a href=#qa辅助理解-2>QA辅助理解</a></li></ul></li></ul></nav></div></details></div><div class=post-content><blockquote><p>6.19发布的CloudMatrix384论文拆解，从宏观到基础概念</p></blockquote><h2 id=核心指标和计算方式>核心指标和计算方式<a hidden class=anchor aria-hidden=true href=#核心指标和计算方式>#</a></h2><h3 id=tpot-time-per-output-token><strong>TPOT (Time Per Output Token)</strong><a hidden class=anchor aria-hidden=true href=#tpot-time-per-output-token>#</a></h3><ul><li><strong>公式</strong>： $$TPOT= \frac{Decode总耗时}{生成Token数量}$$</li><li><strong>测量方式</strong>： 从第一个输出Token开始计时，到生成结束（含MoE通信/KV读取）</li><li><strong>为什么重要</strong>： 直接决定用户体验（如Chatbot响应速度），论文要求 <strong>&lt;50ms</strong>（严格模式&lt;15ms）</li><li><strong>深层意义</strong>： 反映<strong>系统通信+计算综合能力</strong>，EP320下TPOT=42ms证明UB网络突破MoE通信墙</li></ul><h3 id=计算效率-tokenss-per-tflops><strong>计算效率 (Tokens/s per TFLOPS)</strong><a hidden class=anchor aria-hidden=true href=#计算效率-tokenss-per-tflops>#</a></h3><ul><li><strong>公式</strong>： $$计算效率=\frac {吞吐量(tokens/s)} {NPU峰值算力(TFLOPS)}$$​</li><li><strong>论文数据</strong>：</li></ul><table><thead><tr><th>阶段</th><th>值</th><th>对比基准</th></tr></thead><tbody><tr><td>Prefill</td><td>4.45</td><td>超NVIDIA H100+SGLang(3.8)</td></tr><tr><td>Decode</td><td>1.29</td><td>超NVIDIA H800+DeepSeek(0.9)</td></tr></tbody></table><ul><li><strong>为什么重要</strong>： 揭示<strong>硬件利用率</strong>，1.0以上表明软硬件协同极致优化</li><li><strong>深层意义</strong>： Decode阶段1.29 → 昇腾910的Cube引擎利用率达 <strong>86%</strong>（传统GPU仅60%)</li></ul><h3 id=缓存访问延迟-kv-cache-access-latency><strong>缓存访问延迟 (KV Cache Access Latency)</strong><a hidden class=anchor aria-hidden=true href=#缓存访问延迟-kv-cache-access-latency>#</a></h3><ul><li><strong>公式</strong>： $$延迟=TMMU_{查询}+TUB_{传输}+TDRAM_{读取}​$$</li><li><strong>论文数据</strong>：</li></ul><table><thead><tr><th>场景</th><th>延迟</th><th>对比传统</th></tr></thead><tbody><tr><td>本地HBM命中</td><td>0.2μs</td><td>-</td></tr><tr><td>远程DRAM访问(UB)</td><td>1.5μs</td><td>>10μs (PCIe+IB)</td></tr></tbody></table><ul><li><strong>为什么重要</strong>： 长上下文推理中<strong>70%时间花在KV缓存访问</strong></li><li><strong>深层意义</strong>： UB统一内存将远程访问性能提升至<strong>近本地水平</strong>，支撑百万Token上下文。</li></ul><h3 id=专家并行扩展性-ep-degree><strong>专家并行扩展性 (EP Degree)</strong><a hidden class=anchor aria-hidden=true href=#专家并行扩展性-ep-degree>#</a></h3><ul><li><strong>定义</strong>：单个MoE层可分布的专家数量</li><li><strong>论文突破</strong>：<strong>EP320</strong>（每个昇腾Die托管1个专家）</li><li><strong>支撑公式</strong>： $$可扩展性=\frac {UB总带宽}{单个专家通信需求}$$ $$EPmax=\frac {384×392GB/s} {8B/token×10^6token/s}=320$$</li><li><strong>为什么重要</strong>： EP>100时传统网络崩溃，EP320证明UB突破通信可扩展性极限</li></ul><h3 id=int8量化收益><strong>INT8量化收益</strong><a hidden class=anchor aria-hidden=true href=#int8量化收益>#</a></h3><ul><li><strong>公式</strong>：$$ 加速比=\frac {FP16吞吐}{INT8吞吐}×精度保持率$$</li><li><strong>论文数据</strong>：<ul><li>吞吐提升：<strong>1.8倍</strong></li><li>精度损失：<strong>&lt;0.5%</strong>（16个基准测试）</li></ul></li><li><strong>为什么重要</strong>： Decode阶段<strong>内存带宽减少50%</strong>，解决NPU的“内存墙”问题</li></ul><h3 id=qa辅助理解>QA辅助理解<a hidden class=anchor aria-hidden=true href=#qa辅助理解>#</a></h3><h4 id=为什么用tpot而非qps><strong>为什么用TPOT而非QPS？</strong><a hidden class=anchor aria-hidden=true href=#为什么用tpot而非qps>#</a></h4><ul><li>TPOT剥离Batch Size影响，<strong>纯粹衡量单次生成效率</strong></li><li>更直观反映SLA（用户感知的延迟）</li></ul><h4 id=为什么强调计算效率而非绝对吞吐><strong>为什么强调计算效率而非绝对吞吐？</strong><a hidden class=anchor aria-hidden=true href=#为什么强调计算效率而非绝对吞吐>#</a></h4><ul><li>排除工艺优势（7nm vs 5nm），<strong>聚焦架构创新价值</strong></li><li>1.29 tokens/s/TFLOPS → 证明UB+LEP设计优于NVLink+GPU</li></ul><h4 id=为什么测量远程dram访问延迟><strong>为什么测量远程DRAM访问延迟？</strong><a hidden class=anchor aria-hidden=true href=#为什么测量远程dram访问延迟>#</a></h4><ul><li>验证<strong>内存池化</strong>的实际效果，这是打破“内存墙”的核心</li><li>1.5μs延迟 → 实现“全集群如单机”的硬件基础</li></ul><h2 id=超节点架构>超节点架构<a hidden class=anchor aria-hidden=true href=#超节点架构>#</a></h2><h3 id=三级网络平面的物理隔离>三级网络平面的物理隔离<a hidden class=anchor aria-hidden=true href=#三级网络平面的物理隔离>#</a></h3><p><strong>硬件隔离原理</strong></p><pre class=mermaid>
  graph TD
    subgraph Ascend节点
        NPU[NPU计算单元] --&gt;|直连| L1_SW[L1 UB交换芯片]
        CPU[Kunpeng CPU] --&gt;|直连| L1_SW
        L1_SW --&gt;|专用光纤| L2_SW[L2 UB交换芯片]
        
        NPU --&gt;|专用SerDes| RDMA_NIC[RDMA网卡]
        RDMA_NIC --&gt;|RoCE协议| External_RDMA[外部RDMA网络]
        
        CPU --&gt;|PCIe| Qingtian[Qingtian DPU]
        Qingtian --&gt;|以太网| VPC_Switch[VPC交换机]
    end
</pre><p><strong>隔离关键点</strong>：</p><ol><li><p><strong>物理链路分离</strong>：</p><ul><li><strong>UB平面</strong>：NPU/CPU → L1交换芯片 → 专用光纤 → L2交换芯片（通信机柜）</li><li><strong>RDMA平面</strong>：NPU → 板载RDMA SerDes接口 → 外部RoCE网络</li><li><strong>VPC平面</strong>：CPU → Qingtian DPU → 标准以太网交换机</li></ul></li><li><p><strong>协议栈隔离</strong>：</p><ul><li><strong>UB协议</strong>：自定义帧格式（128B载荷+8B路由头），硬件加速</li><li><strong>RDMA协议</strong>：RoCEv2（兼容InfiniBand生态）</li><li><strong>VPC协议</strong>：TCP/IP + UBoE扩展</li></ul></li><li><p><strong>流量管控</strong>：</p><ul><li><strong>MatrixLink组件</strong>：在Qingtian DPU上实现QoS策略<ul><li>UB平面：最高优先级（保障MoE/KV缓存流量）</li><li>RDMA平面：中等优先级（训练/推理数据同步）</li><li>VPC平面：最低优先级（管理/存储流量）</li></ul></li></ul></li></ol><h3 id=三级平面分层设计价值>三级平面分层设计价值<a hidden class=anchor aria-hidden=true href=#三级平面分层设计价值>#</a></h3><p><strong>解决的关键问题对比</strong></p><table><thead><tr><th><strong>挑战场景</strong></th><th>UB平面解决方案</th><th>RDMA平面解决方案</th><th>VPC平面解决方案</th></tr></thead><tbody><tr><td>MoE的Token分发(All-to-All)</td><td>全互联拓扑延迟&lt;1.2μs</td><td>不适用</td><td>不适用</td></tr><tr><td>跨节点KV缓存同步</td><td>本地化优先（延迟最优）</td><td>200Gbps带宽同步</td><td>不适用</td></tr><tr><td>模型冷启动加载</td><td>从内存池直接加载（1.5μs）</td><td>从远端存储加载（>10ms）</td><td>从对象存储加载（>100ms）</td></tr><tr><td>用户请求接入</td><td>不适用</td><td>不适用</td><td>以太网/IP协议接入</td></tr><tr><td><strong>分层逻辑</strong>：</td><td></td><td></td><td></td></tr></tbody></table><ol><li><strong>性能敏感层（UB）</strong>：<ul><li>承载95%的AI内部通信流量</li><li>硬件级隔离保障亚微秒延迟</li></ul></li><li><strong>扩展兼容层（RDMA）</strong>：<ul><li>解决超节点间数据同步</li><li>兼容行业标准（RoCEv2）</li></ul></li><li><strong>生态接入层（VPC）</strong>：<ul><li>无缝对接现有云设施</li><li>通过UBoE逐步增强性能</li></ul></li></ol><h3 id=超节点物理部署架构>超节点物理部署架构<a hidden class=anchor aria-hidden=true href=#超节点物理部署架构>#</a></h3><pre class=mermaid>
  graph TB
    subgraph 超节点CloudMatrix384
        subgraph 12个计算机柜
            subgraph 节点1
                NPU1_0 --&gt; L1_SW1[L1 UB芯片]
                ... --&gt; L1_SW1
                L1_SW1 --&gt;|平面1| L2_SW1[L2 UB交换机]
                L1_SW1 --&gt;|平面7| L2_SW7
            end
            subgraph 节点48
                NPU48_0 --&gt; L1_SW48
                L1_SW48 --&gt;|平面1| L2_SW1
                L1_SW48 --&gt;|平面7| L2_SW7
            end
        end
        
        subgraph 4个通信机柜
            L2_SW1[L2 UB交换机组] --&gt;|全互联| 所有L1芯片
            L2_SW7[L2 UB交换机组] --&gt;|全互联| 所有L1芯片
        end
        
        RDMA_NIC组[RDMA网卡集群] --&gt;|RoCE| 跨超节点网络
        Qingtian_DPU组 --&gt;|以太网| 数据中心核心交换机
    end
</pre><p><strong>关键说明</strong>：</p><ol><li><p><strong>计算部分</strong>：</p><ul><li>48个节点（384 NPU + 192 CPU）部署在12个机柜</li><li>每个节点含7个L1 UB交换芯片（共48×7=336个L1芯片）</li></ul></li><li><p><strong>通信部分</strong>：</p><ul><li>4个专用通信机柜部署L2 UB交换机组</li><li>7个独立子平面 × 16个L2芯片 = 112个L2交换机</li><li>每个L1芯片直连16个L2芯片（总链路数=336×16=5,376）</li></ul></li><li><p><strong>扩展接口</strong>：</p><ul><li><strong>RDMA出口</strong>：NPU直连的RoCE网卡集群</li><li><strong>VPC出口</strong>：CPU连接的Qingtian DPU集群</li></ul></li></ol><h3 id=逻辑全局架构图硬件软件视图>逻辑全局架构图（硬件+软件视图）<a hidden class=anchor aria-hidden=true href=#逻辑全局架构图硬件软件视图>#</a></h3><pre class=mermaid>
  graph LR
    subgraph 逻辑资源池
        direction TB
        NPU_Pool[NPU计算池] --&gt;|UB访问| Memory_Pool[全局内存池]
        CPU_Pool[CPU资源池] --&gt;|UB访问| Memory_Pool
        Memory_Pool --&gt;|存储| KV_Cache[分布式KV缓存]
        
        NPU_Pool --&gt;|RDMA| Other_Supernode[其他超节点]
        CPU_Pool --&gt;|VPC| DC_Network[数据中心网络]
    end
    
    subgraph 软件服务层
        Prefill[Prefill集群] --&gt;|读取| KV_Cache
        Decode[Decode集群] --&gt;|更新| KV_Cache
        Cache_Mgr[缓存管理器] --&gt;|调度| KV_Cache
        
        MatrixResource --&gt;|资源编排| NPU_Pool
        MatrixLink --&gt;|网络策略| Memory_Pool
    end
</pre><p><strong>架构解析</strong>：</p><ol><li><p><strong>硬件抽象层</strong>：</p><ul><li><strong>全局内存池</strong>：物理分散的NPU HBM + CPU DRAM → 逻辑统一地址空间</li><li><strong>动态资源池</strong>：NPU/CPU按需组成Prefill/Decode集群（如EP320）</li></ul></li><li><p><strong>网络平面映射</strong>：</p><ul><li><strong>UB平面</strong>：承载内存池访问/MoE通信（图中实线）</li><li><strong>RDMA平面</strong>：跨超节点KV同步（虚线）</li><li><strong>VPC平面</strong>：外部服务接入（点划线）</li></ul></li><li><p><strong>软件控制层</strong>：</p><ul><li><strong>MatrixResource</strong>：拓扑感知的资源编排（如动态分配160 NPU给Decode）</li><li><strong>MatrixLink</strong>：UB网络QoS保障（优先MoE流量）</li></ul></li></ol><h2 id=ub统一总线理解>UB统一总线理解<a hidden class=anchor aria-hidden=true href=#ub统一总线理解>#</a></h2><h3 id=ub网络核心设计目标>UB网络核心设计目标<a hidden class=anchor aria-hidden=true href=#ub网络核心设计目标>#</a></h3><p><strong>解决传统分布式系统的通信瓶颈</strong>：</p><ul><li><strong>问题</strong>：MoE模型中的Token分发（Token Dispatch）和专家输出合并（Expert Output Combination）需高频All-to-All通信，传统树状网络（如InfiniBand）因多级转发导致延迟>3μs。</li><li><strong>目标</strong>：通过<strong>全互联直连拓扑</strong>实现亚微秒级延迟（&lt;1μs）和接近0带宽衰减。</li></ul><h3 id=ub网络物理架构与数据流动>UB网络物理架构与数据流动<a hidden class=anchor aria-hidden=true href=#ub网络物理架构与数据流动>#</a></h3><ol><li>硬件拓扑结构</li></ol><pre class=mermaid>
  graph TD 
  subgraph 超节点内通信
   A[Ascend 910节点] --&gt;|L1 UB交换芯片| B[L2 UB交换平面] 
   B --&gt;|全互联| C[其他Ascend节点] 
  end
</pre><ul><li><strong>L1交换层</strong>：每个Ascend 910节点（含8 NPU）配备<strong>7个L1 UB交换芯片</strong>。</li><li><strong>L2交换层</strong>：超节点分为<strong>7个独立子平面</strong>，每个子平面含16个L2 UB交换芯片。</li><li><strong>连接方式</strong>：<ul><li>每个L1芯片直连<strong>对应子平面的所有16个L2芯片</strong>（避免带宽阻塞）。</li><li><strong>带宽保障</strong>：节点内部带宽（392 GB/s） = L1→L2总带宽（392 GB/s × 7）。</li></ul></li></ul><ol start=2><li>数据流动示例<br>假设NPU0需将Token发送至专家255（位于NPU255）：</li></ol><pre class=mermaid>
  sequenceDiagram 
  participant NPU0 as NPU0（源） 
  participant L1 as L1交换芯片（节点内）
   participant L2 as L2交换芯片（子平面） 
   participant NPU255 as NPU255（目标） 
   
   NPU0-&gt;&gt;L1: 发送Token数据（含专家ID=255）
   L1-&gt;&gt;L2: 根据专家ID选择子平面（如平面#3） 
   L2-&gt;&gt;NPU255: 直连转发至目标NPU 
   NPU255-&gt;&gt;L2: 返回确认信号 
   L2-&gt;&gt;L1: 回传确认 
   L1-&gt;&gt;NPU0: 完成传输
</pre><ul><li><strong>关键优化</strong>：<ul><li><strong>零路由表查询</strong>：硬件预配置专家ID与L2子平面的映射关系，避免软件寻址开销。</li><li><strong>物理直连</strong>：L1→L2、L2→目标NPU均为点到点光纤直连，无中间交换机。</li></ul></li></ul><h3 id=通信协议栈与硬件加速>通信协议栈与硬件加速<a hidden class=anchor aria-hidden=true href=#通信协议栈与硬件加速>#</a></h3><h4 id=ub协议栈分层设计>UB协议栈分层设计<a hidden class=anchor aria-hidden=true href=#ub协议栈分层设计>#</a></h4><table><thead><tr><th><strong>层级</strong></th><th><strong>功能</strong></th></tr></thead><tbody><tr><td><strong>物理层</strong></td><td>112 Gbps SerDes接口，64B/66B编码，支持392 GB/s单向带宽</td></tr><tr><td><strong>链路层</strong></td><td>硬件事务拆分（Chunking）和重组，支持最大8KB报文</td></tr><tr><td><strong>传输层</strong></td><td>零拷贝DMA引擎，绕过CPU和OS内核（类似GPUDirect RDMA）</td></tr><tr><td><strong>应用层</strong></td><td>融合通信算子（如MoE的Token Dispatch/Combination封装为单条硬件指令）</td></tr></tbody></table><h4 id=moe通信的硬件融合>MoE通信的硬件融合<a hidden class=anchor aria-hidden=true href=#moe通信的硬件融合>#</a></h4><ol><li>传统流程</li></ol><pre class=mermaid>
  graph LR
  A[Token分发] --&gt; B[专家计算] --&gt; C[输出合并]
</pre><p>需两次独立的All-to-All通信（延迟翻倍）</p><ol start=2><li>UB优化流程</li></ol><pre class=mermaid>
  graph LR 
  A[Token分发 + 输出合并] --&gt;|单次融合操作| B[专家计算]
</pre><ul><li><strong>硬件指令</strong>：昇腾910内置<code>MOE_FUSED_COMM</code>指令，将两次通信合并为一次。</li><li><strong>带宽节省</strong>：Token元数据（专家ID）与输出张量共享同一缓存区。</li></ul><h3 id=关键场景性能对比>关键场景性能对比<a hidden class=anchor aria-hidden=true href=#关键场景性能对比>#</a></h3><h4 id=all-to-all延迟256-npu>ALL-to-ALL延迟（256 NPU）<a hidden class=anchor aria-hidden=true href=#all-to-all延迟256-npu>#</a></h4><table><thead><tr><th><strong>架构</strong></th><th>延迟（μs）</th><th>瓶颈原因</th></tr></thead><tbody><tr><td>InfiniBand HDR</td><td>12.3</td><td>多级交换机转发 + 协议栈开销</td></tr><tr><td><strong>UB网络</strong></td><td><strong>1.2</strong></td><td>物理直连 + 硬件融合通信</td></tr></tbody></table><h4 id=kv-缓存访问流程>KV 缓存访问流程<a hidden class=anchor aria-hidden=true href=#kv-缓存访问流程>#</a></h4><pre class=mermaid>
  flowchart TD     
  A[NPU需读取历史KV块] --&gt; B{本地HBM命中？}    
  B -- 未命中 --&gt; C[通过UB访问远程DRAM池]     
  C --&gt; D[从CPU内存池直接读取]     
  D --&gt; E[数据返回NPU]
</pre><ul><li><strong>性能优势</strong>：<ul><li>远程DRAM访问延迟≈1.5μs（对比InfiniBand的>10μs）。</li><li>带宽利用率>90%（传统方案&lt;50%）。</li></ul></li></ul><h3 id=创新点>创新点<a hidden class=anchor aria-hidden=true href=#创新点>#</a></h3><h4 id=1全互联拓扑的本质>1. <strong>全互联拓扑的本质</strong><a hidden class=anchor aria-hidden=true href=#1全互联拓扑的本质>#</a></h4><ul><li><strong>物理层</strong>：用<strong>两级星型结构</strong>（L1局部全连 + L2分组全连）逼近FullMesh性能，避免O(N²)链路复杂度。</li><li><strong>协议层</strong>：将通信模式抽象为硬件指令（如<code>ALL_TO_ALL</code>），由NPU通信引擎直接执行。</li></ul><h4 id=2与nvidia方案的对比>2. <strong>与NVIDIA方案的对比</strong><a hidden class=anchor aria-hidden=true href=#2与nvidia方案的对比>#</a></h4><table><thead><tr><th><strong>特性</strong></th><th>NVIDIA NVLink/Switch</th><th>Huawei UB</th></tr></thead><tbody><tr><td>拓扑</td><td>单节点全连，跨节点树状</td><td>超节点内全互联</td></tr><tr><td>延迟</td><td>节点内0.5μs，跨节点>3μs</td><td>全节点&lt;1μs</td></tr><tr><td>MoE支持</td><td>EP144需复杂流水线掩盖延迟</td><td><strong>原生支持EP320</strong></td></tr><tr><td>内存池化</td><td>仅GPU显存共享</td><td>NPU+CPU全局内存池</td></tr></tbody></table><h3 id=ub网络数据流动全貌>UB网络数据流动全貌<a hidden class=anchor aria-hidden=true href=#ub网络数据流动全貌>#</a></h3><pre class=mermaid>
  graph LR
    subgraph CloudMatrix384超节点
        direction TB
        subgraph Node1[Ascend节点1]
            NPU1_0 --&gt; L1_1[L1交换芯片]
            NPU1_1 --&gt; L1_1
            ... --&gt; L1_1
            L1_1 --&gt;|平面1| L2_1[L2交换芯片群]
            L1_1 --&gt;|平面2| L2_2
            L1_1 --&gt;|平面7| L2_7
        end

        subgraph Node48[Ascend节点48]
            NPU48_0 --&gt; L1_48
            ...
            L1_48 --&gt;|平面1| L2_1
            L1_48 --&gt;|平面7| L2_7
        end

        L2_1 --&gt; NPU48_0
        L2_7 --&gt; NPU1_0
    end
</pre><ul><li><strong>箭头方向</strong>：数据通过L1/L2芯片跨节点直连。</li><li><strong>带宽保障</strong>：每个L1芯片的7条链路独立工作，总带宽=7×392 GB/s。</li></ul><h3 id=qa辅助理解-1>QA辅助理解<a hidden class=anchor aria-hidden=true href=#qa辅助理解-1>#</a></h3><h4 id=什么是星型拓扑ub网络是纯星型吗><strong>什么是星型拓扑？UB网络是纯星型吗？</strong><a hidden class=anchor aria-hidden=true href=#什么是星型拓扑ub网络是纯星型吗>#</a></h4><ul><li><strong>经典星型拓扑</strong>：所有节点连接至中心交换机，优点是布线简单，缺点是中心节点易成瓶颈（单点故障/带宽限制）。</li><li><strong>UB的改良设计</strong>：</li></ul><pre class=mermaid>
  graph TB 
 subgraph L2子平面
  L2_SW1[L2交换机1] 
  L2_SW2[L2交换机2] 
 end 

NPU0 --&gt;|直连| L1_SW[节点内L1交换机]
L1_SW --&gt;|全互联| L2_SW1 
L1_SW --&gt;|全互联| L2_SW2
</pre><ul><li><strong>两级星型混合</strong>：节点内8 NPU → 7个L1交换机（星型），L1交换机 → 16个L2交换机（全互联）。</li><li><strong>优势</strong>：用<strong>O(N)布线复杂度</strong>逼近FullMesh性能，避免中心瓶颈（L2分散负载）。</li></ul><h4 id=为什么ub能实现跨节点延迟1μs><strong>为什么UB能实现“跨节点延迟&lt;1μs”？</strong><a hidden class=anchor aria-hidden=true href=#为什么ub能实现跨节点延迟1μs>#</a></h4><p><strong>协议硬化 + 物理直连</strong></p><table><thead><tr><th><strong>步骤</strong></th><th><strong>传统InfiniBand</strong></th><th><strong>UB优化</strong></th></tr></thead><tbody><tr><td>数据发出</td><td>NPU → 驱动 → 内核协议栈</td><td>NPU直连SerDes接口</td></tr><tr><td>跨节点路由</td><td>多级交换机转发（3-5跳）</td><td>L1→L2单跳直达（物理直连）</td></tr><tr><td>数据接收</td><td>内核协议栈 → 驱动 → NPU</td><td>直写NPU缓存（DMA引擎）</td></tr><tr><td><strong>典型延迟</strong></td><td>>3μs</td><td><strong>0.8-1.2μs</strong></td></tr></tbody></table><ul><li><strong>关键创新</strong>：<ul><li>跳过操作系统协议栈（类似RDMA但更底层）</li><li>交换机无路由查找（预配置专家ID→平面映射）</li></ul></li></ul><h4 id=融合通信算子如何减少moe通信开销><strong>融合通信算子如何减少MoE通信开销？</strong><a hidden class=anchor aria-hidden=true href=#融合通信算子如何减少moe通信开销>#</a></h4><p><strong>合并两次All-to-All为单次硬件事务</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>//</span> <span class=n>昇腾910硬件指令</span>
</span></span><span class=line><span class=cl><span class=n>moe_fused_comm</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=n>input_tokens</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>		<span class=n>expert_mask</span><span class=p>,</span> <span class=o>//</span> <span class=n>携带路由信息</span>
</span></span><span class=line><span class=cl>		<span class=n>output_buffer</span> <span class=o>//</span> <span class=n>预留专家输出空间</span> 
</span></span><span class=line><span class=cl>		<span class=p>);</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>效果</strong>：单次通信完成分发+合并，延迟降至<strong>5μs内</strong>（EP320实测）。</li></ul><h4 id=什么是零拷贝dmaub如何实现><strong>什么是“零拷贝DMA”？UB如何实现？</strong><a hidden class=anchor aria-hidden=true href=#什么是零拷贝dmaub如何实现>#</a></h4><p><strong>内存直接访问，跳过CPU复制</strong></p><ul><li><strong>传统流程</strong>（以KV缓存读取为例）： <code>远程DRAM → 网卡 → 主机内存 → PCIe → NPU内存</code>（3次拷贝）</li><li><strong>UB零拷贝</strong>： <code>远程DRAM → UB网络 → NPU计算单元</code><ul><li><strong>硬件支持</strong>：<ul><li>NPU集成DMA引擎，直接发起远程内存读写请求</li><li>UB交换机维护全局内存地址表（类似CC-NUMA）</li></ul></li></ul></li><li><strong>带宽利用率</strong>：从**&lt;50%<strong>（InfiniBand）提升至</strong>>90%**</li></ul><h4 id=为什么需要7个l2子平面><strong>为什么需要7个L2子平面？</strong><a hidden class=anchor aria-hidden=true href=#为什么需要7个l2子平面>#</a></h4><p><strong>匹配Ascend 910芯片接口，避免带宽阻塞</strong></p><ul><li><strong>数学关系</strong>：<ul><li>单NPU带宽需求：392 GB/s</li><li>单L1交换机上行带宽 = 7链路 × 56 GB/s = 392 GB/s</li><li><strong>每个L1需直连7组L2</strong>才能满足带宽（7×56=392）</li></ul></li><li><strong>容错设计</strong>：<ul><li>任一L2子平面故障不影响其他平面通信</li><li>动态路由切换（MatrixLink组件）</li></ul></li></ul><h4 id=分布式kv缓存在ub上如何工作><strong>分布式KV缓存在UB上如何工作？</strong><a hidden class=anchor aria-hidden=true href=#分布式kv缓存在ub上如何工作>#</a></h4><p><strong>全局内存池 + 硬件一致性协议</strong></p><pre class=mermaid>
  flowchart LR 
  NPU1 --&gt;|UB读请求| CachePool[CPU DRAM池] 
  CachePool --&gt;|返回数据| NPU1
  NPU2 --&gt;|并行写| CachePool
</pre><ul><li><strong>一致性保障</strong>：<ul><li>硬件级MESI协议（缓存行锁定）</li><li>写冲突时L2交换机仲裁（优先本地化写入）</li></ul></li></ul><p><strong>性能对比</strong>：</p><table><thead><tr><th>场景</th><th>传统方案延迟</th><th>UB延迟</th></tr></thead><tbody><tr><td>跨节点读缓存</td><td>10μs+</td><td><strong>1.5</strong></td></tr></tbody></table><h4 id=ub如何支持动态负载><strong>UB如何支持动态负载？</strong><a hidden class=anchor aria-hidden=true href=#ub如何支持动态负载>#</a></h4><p><strong>资源池化 + 软件定义拓扑</strong></p><ul><li><strong>硬件基础</strong>：<ul><li>NPU/CPU/内存物理解耦</li><li>UB网络虚拟化（VXLAN类似技术）</li></ul></li><li><strong>动态调整示例</strong>：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 突发流量时扩容Decode集群 </span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>request_queue</span> <span class=o>&gt;</span> <span class=n>threshold</span><span class=p>:</span> 
</span></span><span class=line><span class=cl>	<span class=n>assign_new_npus</span><span class=p>(</span><span class=s2>&#34;Decode&#34;</span><span class=p>,</span> <span class=mi>40</span><span class=p>)</span> <span class=c1># 从空闲池分配40 NPU </span>
</span></span><span class=line><span class=cl>	<span class=n>update_routing_table</span><span class=p>()</span> <span class=c1># 更新专家路由表</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>效果</strong>：10秒内扩容EP40组，TPOT仍&lt;50ms</li></ul><h4 id=ub与nvidia-nvlink本质区别><strong>UB与NVIDIA NVLink本质区别？</strong><a hidden class=anchor aria-hidden=true href=#ub与nvidia-nvlink本质区别>#</a></h4><p><strong>全集群统一总线 vs 节点内互联</strong></p><table><thead><tr><th><strong>维度</strong></th><th>NVLink/Switch</th><th>UB</th></tr></thead><tbody><tr><td>拓扑范围</td><td>单节点（8 GPU）</td><td><strong>384 NPU全域</strong></td></tr><tr><td>内存模型</td><td>仅GPU显存共享</td><td><strong>CPU+NPU全局池化</strong></td></tr><tr><td>扩展方式</td><td>树状扩展（带宽衰减）</td><td>全互联扩展</td></tr><tr><td>延迟一致性</td><td>跨节点>3μs</td><td><strong>全域&lt;1μs</strong></td></tr></tbody></table><h4 id=为什么需要专用l1l2交换芯片><strong>为什么需要专用L1/L2交换芯片？</strong><a hidden class=anchor aria-hidden=true href=#为什么需要专用l1l2交换芯片>#</a></h4><p><strong>解决SerDes电气限制 + 协议卸载</strong></p><ul><li><strong>电气层</strong>：<ul><li>单SerDes链路极限56 GB/s → 需多链路并行</li><li>L1芯片聚合8 NPU流量（392 GB/s）</li></ul></li><li><strong>协议层</strong>：<ul><li>卸载路由计算、CRC校验、重传机制</li><li>NPU无需处理通信协议（算力100%用于AI）</li></ul></li></ul><h4 id=融合算子具体如何节省带宽><strong>融合算子具体如何节省带宽？</strong><a hidden class=anchor aria-hidden=true href=#融合算子具体如何节省带宽>#</a></h4><p><strong>元数据与有效载荷合并编码</strong></p><ul><li><strong>传统两次通信</strong>：<ul><li>Token分发：发送<code>{token_data, expert_id}</code></li><li>输出合并：发送<code>{expert_output, token_id}</code> <strong>总数据量</strong>：<code>2×(数据+ID)</code></li></ul></li><li><strong>UB融合通信</strong>：<ul><li>单次发送<code>{token_data, expert_id, output_buffer_addr}</code> <strong>节省</strong>：减少50% ID字段传输，带宽需求<strong>下降35%</strong></li></ul></li></ul><h4 id=ub如何保障大规模组网的信号完整性><strong>UB如何保障大规模组网的信号完整性？</strong><a hidden class=anchor aria-hidden=true href=#ub如何保障大规模组网的信号完整性>#</a></h4><p><strong>光电混合 + 时钟同步技术</strong></p><ul><li><strong>挑战</strong>：384 NPU全互联需数万链路，电信号衰减严重</li><li><strong>华为方案</strong>：<ul><li>关键路径用光纤替代铜缆（L2交换机间）</li><li>全局时钟分发网络（误差&lt;5ps）</li><li>接收端自适应均衡（CTLE+DFE）</li></ul></li></ul><h4 id=为什么说ub是ai原生网络><strong>为什么说UB是“AI原生网络”？</strong><a hidden class=anchor aria-hidden=true href=#为什么说ub是ai原生网络>#</a></h4><p><strong>硬件指令级AI通信原语</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Ascend 910指令集扩展 
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>ascend_moe_dispatch</span><span class=p>(</span> 
</span></span><span class=line><span class=cl>	<span class=n>expert_mask</span><span class=p>,</span> <span class=c1>// 门控网络输出 
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=n>token_ptr</span><span class=p>,</span> <span class=c1>// 输入token地址 
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=n>ub_channel_id</span> <span class=c1>// 指定UB虚拟通道 
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=p>);</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>与AI计算流水线深度耦合</strong>：<ul><li>通信操作作为AI计算图的一部分编译</li><li>立方体引擎(Cube)执行FFN时自动触发通信</li></ul></li><li><strong>对比传统网络</strong>：需CPU调度MPI库，上下文切换开销大</li></ul><h2 id=ub全局内存理解>UB全局内存理解<a hidden class=anchor aria-hidden=true href=#ub全局内存理解>#</a></h2><h3 id=基础概念kv缓存的核心挑战>基础概念：KV缓存的核心挑战<a hidden class=anchor aria-hidden=true href=#基础概念kv缓存的核心挑战>#</a></h3><h4 id=问题本质>问题本质<a hidden class=anchor aria-hidden=true href=#问题本质>#</a></h4><ul><li><strong>KV缓存</strong>：存储Transformer历史注意力状态（Key/Value向量），用于长上下文推理</li><li><strong>挑战</strong>：<ul><li>百万token上下文需TB级内存 → NPU本地HBM（仅32GB）无法容纳</li><li>传统方案：跨节点复制数据 → <strong>带宽瓶颈 & 高延迟</strong></li></ul></li></ul><h4 id=传统方案nvidia架构>传统方案（NVIDIA架构）<a hidden class=anchor aria-hidden=true href=#传统方案nvidia架构>#</a></h4><pre class=mermaid>
  graph LR 
  A[GPU0] --&gt;|NVLink| B[GPU1本地HBM] 
  B --&gt;|PCIe| C[CPU内存] 
  C --&gt;|InfiniBand| D[远程节点内存]
</pre><ul><li><strong>访问流程</strong>：<ol><li>缺失的KV块从远程节点复制到本地HBM</li><li>GPU计算单元读取本地HBM</li></ol></li><li><strong>痛点</strong>：<ul><li><strong>高延迟</strong>：跨节点复制链路过长（>10μs）</li><li><strong>低带宽</strong>：InfiniBand带宽（200Gbps） &#171; HBM带宽（3TB/s）</li><li><strong>数据孤岛</strong>：GPU显存无法直接共享</li></ul></li></ul><h3 id=华为ub全局内存的硬件基础>华为UB全局内存的硬件基础<a hidden class=anchor aria-hidden=true href=#华为ub全局内存的硬件基础>#</a></h3><h4 id=物理架构三级资源池化>物理架构：三级资源池化<a hidden class=anchor aria-hidden=true href=#物理架构三级资源池化>#</a></h4><pre class=mermaid>
  graph TB 
  subgraph CloudMatrix384 
    NPU_HBM[NPU HBM] --&gt; UB 
    CPU_DRAM[CPU DRAM池] --&gt; UB 
    UB --&gt;|统一编址| Global_Memory[全局内存空间] 
  end
</pre><ul><li><strong>核心组件</strong>：<ul><li><strong>NPU HBM</strong>：昇腾910双Die共32GB，带宽3.2TB/s（本地高速缓存）</li><li><strong>CPU DRAM池</strong>：192鲲鹏CPU的TB级内存（主存储池）</li><li><strong>UB网络</strong>：392GB/s全互联，延迟&lt;1μs（数据通路）</li></ul></li></ul><h4 id=地址映射硬件类似cc-numa>地址映射硬件（类似CC-NUMA）<a hidden class=anchor aria-hidden=true href=#地址映射硬件类似cc-numa>#</a></h4><pre class=mermaid>
  flowchart TD 
  NPU[NPU计算单元] --&gt; MMU[内存管理单元] 
  MMU --&gt;|本地地址？| Local{地址检查} 
  Local --&gt;|是| HBM[本地HBM] 
  Local --&gt;|否| UB_Interface[UB网络接口] 
  UB_Interface --&gt;|全局地址| UB_Switch[UB交换机] 
  UB_Switch --&gt;|路由| Target[目标内存设备]
</pre><ul><li><strong>关键硬件</strong>：<ul><li><strong>全局地址表</strong>：存储在UB交换机中，记录物理内存位置</li><li><strong>零拷贝DMA</strong>：NPU直接发起远程内存读写，无需CPU参与</li></ul></li></ul><h4 id=缓存一致性协议硬件加速mesi>缓存一致性协议（硬件加速MESI）<a hidden class=anchor aria-hidden=true href=#缓存一致性协议硬件加速mesi>#</a></h4><ul><li><strong>挑战</strong>：多NPU并发修改同一KV块</li><li><strong>解决方案</strong>：<ul><li>UB交换机内置<strong>一致性引擎</strong></li><li>基于物理地址的缓存行锁定（64B粒度）</li><li>写冲突时优先本地化处理（Locality-aware）</li></ul></li></ul><h3 id=软件层实现全局内存访问原理>软件层实现：全局内存访问原理<a hidden class=anchor aria-hidden=true href=#软件层实现全局内存访问原理>#</a></h3><h4 id=内存分配软件api>内存分配（软件API）<a hidden class=anchor aria-hidden=true href=#内存分配软件api>#</a></h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=c1>// 在DRAM池分配分布式KV缓存 
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>kv_cache_t</span><span class=o>*</span> <span class=n>kv_buf</span> <span class=o>=</span> <span class=nf>ub_mem_alloc_pooled</span><span class=p>(</span> 
</span></span><span class=line><span class=cl>	<span class=n>UBMEM_DRAM_POOL</span><span class=p>,</span> <span class=c1>// 内存池类型 
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span><span class=p>,</span> <span class=c1>// 1GB空间 
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=n>KV_CACHE_TAG</span> <span class=c1>// 标记为KV缓存 
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=p>);</span> 
</span></span><span class=line><span class=cl><span class=c1>// NPU直接映射到地址空间 
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>void</span><span class=o>*</span> <span class=n>npu_virt_addr</span> <span class=o>=</span> <span class=nf>ub_map_remote</span><span class=p>(</span><span class=n>kv_buf</span><span class=p>);</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>关键操作</strong>：<ul><li><code>ub_mem_alloc_pooled</code>：从DRAM池分配物理内存</li><li><code>ub_map_remote</code>：将远程内存映射到NPU虚拟地址空间</li></ul></li></ul><h4 id=kv缓存读取流程>KV缓存读取流程<a hidden class=anchor aria-hidden=true href=#kv缓存读取流程>#</a></h4><pre class=mermaid>
  sequenceDiagram 
  NPU-&gt;&gt;UB_MMU: 发起KV块读取(虚拟地址0xFFFF1000) 
  UB_MMU-&gt;&gt;Global_Addr_Table: 查询物理位置(Node15, DRAM_Offset0x8000)  
  Global_Addr_Table--&gt;&gt;UB_MMU: 返回目标地址 
  UB_MMU-&gt;&gt;UB_Switch: 发送DMA读请求 
  UB_Switch-&gt;&gt;Node15_DRAM: 读取数据 
  Node15_DRAM-&gt;&gt;UB_Switch: 返回数据 
  UB_Switch-&gt;&gt;NPU: 直写NPU寄存器
</pre><ul><li><strong>性能关键</strong>：<ul><li>全程无CPU参与</li><li>数据不经过本地HBM（避免复制）</li><li>延迟仅 <strong>1.5μs</strong>（对比NVIDIA >10μs）</li></ul></li></ul><h4 id=缓存服务架构>缓存服务架构<a hidden class=anchor aria-hidden=true href=#缓存服务架构>#</a></h4><pre class=mermaid>
  graph TD 
  Prefill[Prefill NPU] --&gt;|写入新KV块| Cache[DRAM缓存池] 
  Decode[Decode NPU] --&gt;|读取历史块| Cache 
  Cache_Manager[缓存管理器] --&gt;|元数据维护| Cache
</pre><ul><li><strong>去中心化设计</strong>：<ul><li>无全局调度器 → 各NPU通过UB自主访问</li><li>缓存位置对软件透明</li></ul></li></ul><h3 id=与nvidia方案的对比>与NVIDIA方案的对比<a hidden class=anchor aria-hidden=true href=#与nvidia方案的对比>#</a></h3><h4 id=架构差异全景图>架构差异全景图<a hidden class=anchor aria-hidden=true href=#架构差异全景图>#</a></h4><table><thead><tr><th><strong>维度</strong></th><th>NVIDIA方案</th><th>华为UB方案</th></tr></thead><tbody><tr><td><strong>硬件基础</strong></td><td>GPU显存隔离 + 分层网络</td><td>NPU/CPU内存池 + UB全互联</td></tr><tr><td><strong>远程访问</strong></td><td>显式复制（PCIe→IB→HBM）</td><td>直接内存映射（零拷贝）</td></tr><tr><td><strong>地址空间</strong></td><td>每GPU独立虚拟地址</td><td>全域统一虚拟地址</td></tr><tr><td><strong>调度约束</strong></td><td>请求需路由到持有KV块的GPU</td><td>任意NPU可直接访问任意KV块</td></tr><tr><td><strong>典型延迟</strong></td><td>>10μs（跨节点）</td><td><strong>1.5μs</strong>（全域）</td></tr><tr><td><strong>峰值带宽</strong></td><td>≤200Gbps（InfiniBand）</td><td><strong>392GB/s</strong>（单NPU单向）</td></tr></tbody></table><h3 id=工程实现关键创新>工程实现关键创新<a hidden class=anchor aria-hidden=true href=#工程实现关键创新>#</a></h3><h4 id=硬件层dram-npu带宽均衡>硬件层：DRAM-NPU带宽均衡<a hidden class=anchor aria-hidden=true href=#硬件层dram-npu带宽均衡>#</a></h4><ul><li><strong>问题</strong>：DRAM带宽（256GB/s）&lt; NPU需求（392GB/s）</li><li><strong>解决方案</strong>：<ul><li><strong>分布式条带化</strong>：将KV块切分存储到多CPU内存</li><li><strong>并发访问</strong>：NPU同时从8个DRAM节点读取</li></ul></li></ul><h4 id=协议层轻量级一致性控制>协议层：轻量级一致性控制<a hidden class=anchor aria-hidden=true href=#协议层轻量级一致性控制>#</a></h4><ul><li><strong>优化点</strong>：<ul><li>KV缓存<strong>只读居多</strong> → 放宽一致性要求</li><li>写操作仅限Prefill NPU → 减少协议开销</li></ul></li></ul><h4 id=软件层缓存感知的kv布局>软件层：缓存感知的KV布局<a hidden class=anchor aria-hidden=true href=#软件层缓存感知的kv布局>#</a></h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=c1>// 按Attention Head分片存储 
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>head</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>head</span><span class=o>&lt;</span><span class=n>num_heads</span><span class=p>;</span> <span class=n>head</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=nf>ub_mem_store</span><span class=p>(</span><span class=n>kv_cache</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>	<span class=n>head</span> <span class=o>*</span> <span class=n>head_size</span><span class=p>,</span> <span class=c1>// 按Head偏移 
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=n>data</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>	<span class=n>UB_CACHE_AWARE</span> <span class=c1>// 标记为缓存友好布局 
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=p>);</span> 
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>效果</strong>：<ul><li>单次读取可获取完整Attention Head数据</li><li>减少随机访问导致的缓存行浪费</li></ul></li></ul><h3 id=qa辅助理解-2>QA辅助理解<a hidden class=anchor aria-hidden=true href=#qa辅助理解-2>#</a></h3><h4 id=ub全局内存的硬件工作原理>UB全局内存的硬件工作原理?<a hidden class=anchor aria-hidden=true href=#ub全局内存的硬件工作原理>#</a></h4><p><strong>三级硬件协同</strong></p><p>1. <strong>NPU内存管理单元（MMU）</strong></p><ul><li>接收虚拟地址请求</li><li>查询<strong>全局地址表</strong>（存储在UB交换机）</li></ul><p>2. <strong>UB交换机</strong></p><ul><li>维护全局地址→物理位置映射</li><li>路由请求到目标设备（DRAM或HBM）</li></ul><p>3. <strong>DMA引擎</strong></p><ul><li>在目标设备执行直接内存访问</li><li>数据通过UB网络直送请求方NPU</li></ul><h4 id=软件层如何使用ub全局内存开发者视角>软件层如何使用UB全局内存？（开发者视角）<a hidden class=anchor aria-hidden=true href=#软件层如何使用ub全局内存开发者视角>#</a></h4><p><strong>透明化API + 缓存优化</strong></p><p>开发者API示例</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=c1>// 分配分布式KV缓存
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>kv_cache_t</span><span class=o>*</span> <span class=n>kv_buf</span> <span class=o>=</span> <span class=nf>ub_mem_alloc_pooled</span><span class=p>(</span> 
</span></span><span class=line><span class=cl>	<span class=n>UBMEM_DRAM_POOL</span><span class=p>,</span> <span class=c1>// 指定DRAM池 
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=mi>1024</span><span class=o>*</span><span class=mi>1024</span><span class=o>*</span><span class=mi>1024</span><span class=p>,</span> <span class=c1>// 1GB空间
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=n>KV_CACHE_TAG</span> <span class=c1>// 标记为KV缓存 
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// NPU直接映射远程内存 
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>void</span><span class=o>*</span> <span class=n>npu_vaddr</span> <span class=o>=</span> <span class=nf>ub_map_remote</span><span class=p>(</span><span class=n>kv_buf</span><span class=p>);</span> 
</span></span><span class=line><span class=cl><span class=c1>// 像本地内存一样使用 
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=nf>attention_compute</span><span class=p>(</span><span class=n>npu_vaddr</span> <span class=o>+</span> <span class=n>offset</span><span class=p>);</span>
</span></span></code></pre></td></tr></table></div></div><p>缓存感知优化</p><ul><li><strong>按Attention Head分片存储</strong>：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>head_idx</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_heads</span><span class=p>):</span> 
</span></span><span class=line><span class=cl>	<span class=n>store_kv_block</span><span class=p>(</span><span class=n>kv_cache</span><span class=p>,</span> <span class=n>head_idx</span> <span class=o>*</span> <span class=n>head_size</span><span class=p>,</span> <span class=n>data</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>单次读取获取完整Head数据，减少随机访问</li></ul><h4 id=与nvidia方案性能对比数据差异多大>与NVIDIA方案性能对比？数据差异多大？<a hidden class=anchor aria-hidden=true href=#与nvidia方案性能对比数据差异多大>#</a></h4><p><strong>数量级提升</strong></p><table><thead><tr><th><strong>指标</strong></th><th>NVIDIA H800</th><th>Huawei UB</th><th>提升倍数</th></tr></thead><tbody><tr><td>远程读延迟</td><td>>10 μs</td><td><strong>1.5 μs</strong></td><td>6.7×</td></tr><tr><td>有效带宽利用率</td><td>&lt;50%</td><td><strong>>90%</strong></td><td>1.8×</td></tr><tr><td>KV缓存容量</td><td>单节点限制</td><td><strong>数十TB</strong></td><td>∞</td></tr><tr><td>一致性管理开销</td><td>软件实现（高开销）</td><td><strong>硬件MESI</strong></td><td>10×效率</td></tr></tbody></table><h4 id=ub如何处理多npu并发修改同一缓存>UB如何处理多NPU并发修改同一缓存？<a hidden class=anchor aria-hidden=true href=#ub如何处理多npu并发修改同一缓存>#</a></h4><p><strong>硬件加速的MESI协议</strong></p><ul><li><strong>缓存行锁定</strong>：64B为粒度，写操作时独占锁定</li><li><strong>仲裁策略</strong>：<ul><li>读操作：并行执行（无冲突）</li><li>写操作：UB交换机按物理位置优先级仲裁<ul><li>本地节点优先 → 减少网络传输</li></ul></li></ul></li><li><strong>冲突案例</strong>：<ol><li>NPU A和C同时请求写同一KV块</li><li>UB交换机检测冲突，赋予Node_A优先权</li><li>NPU A完成写后通知NPU C失效旧副本</li></ol></li></ul><blockquote><p><strong>注意</strong>：KV缓存以<strong>只读为主</strong>（Decode阶段），写冲突率&lt;0.1%。</p></blockquote><h4 id=为什么cpu-dram池比npu-hbm更适合存储kv缓存>为什么CPU DRAM池比NPU HBM更适合存储KV缓存？<a hidden class=anchor aria-hidden=true href=#为什么cpu-dram池比npu-hbm更适合存储kv缓存>#</a></h4><p><strong>容量与成本的完美平衡</strong></p><table><thead><tr><th><strong>介质</strong></th><th>优势</th><th>劣势</th><th>适用场景</th></tr></thead><tbody><tr><td>NPU HBM</td><td>超高速（3.2TB/s）</td><td>容量小（32GB/NPU）</td><td>热点缓存</td></tr><tr><td>CPU DRAM</td><td>超大容量（TB级）</td><td>较慢（256GB/s）</td><td>主KV存储池</td></tr></tbody></table><ul><li><strong>智能分层</strong>：<ul><li>活跃KV缓存 → 自动迁移至NPU HBM</li><li>历史KV缓存 → 存储在CPU DRAM池</li></ul></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://pillumina.github.io/posts/aiinfra/03-areal/><span class=title>« Prev</span><br><span>[LLM4RL] 异步RL框架: Areal</span>
</a><a class=next href=https://pillumina.github.io/posts/programming/cloud-computing/k8s-operator-dev/><span class=title>Next »</span><br><span>Kubernetes Operator Development History</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 昇腾超节点CloudMatrix384论文拆解 on x" href="https://x.com/intent/tweet/?text=%e6%98%87%e8%85%be%e8%b6%85%e8%8a%82%e7%82%b9CloudMatrix384%e8%ae%ba%e6%96%87%e6%8b%86%e8%a7%a3&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f01-ascend-cloudmatrix%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 昇腾超节点CloudMatrix384论文拆解 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f01-ascend-cloudmatrix%2f&amp;title=%e6%98%87%e8%85%be%e8%b6%85%e8%8a%82%e7%82%b9CloudMatrix384%e8%ae%ba%e6%96%87%e6%8b%86%e8%a7%a3&amp;summary=%e6%98%87%e8%85%be%e8%b6%85%e8%8a%82%e7%82%b9CloudMatrix384%e8%ae%ba%e6%96%87%e6%8b%86%e8%a7%a3&amp;source=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f01-ascend-cloudmatrix%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 昇腾超节点CloudMatrix384论文拆解 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f01-ascend-cloudmatrix%2f&title=%e6%98%87%e8%85%be%e8%b6%85%e8%8a%82%e7%82%b9CloudMatrix384%e8%ae%ba%e6%96%87%e6%8b%86%e8%a7%a3"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 昇腾超节点CloudMatrix384论文拆解 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f01-ascend-cloudmatrix%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 昇腾超节点CloudMatrix384论文拆解 on whatsapp" href="https://api.whatsapp.com/send?text=%e6%98%87%e8%85%be%e8%b6%85%e8%8a%82%e7%82%b9CloudMatrix384%e8%ae%ba%e6%96%87%e6%8b%86%e8%a7%a3%20-%20https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f01-ascend-cloudmatrix%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 昇腾超节点CloudMatrix384论文拆解 on telegram" href="https://telegram.me/share/url?text=%e6%98%87%e8%85%be%e8%b6%85%e8%8a%82%e7%82%b9CloudMatrix384%e8%ae%ba%e6%96%87%e6%8b%86%e8%a7%a3&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f01-ascend-cloudmatrix%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 昇腾超节点CloudMatrix384论文拆解 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e6%98%87%e8%85%be%e8%b6%85%e8%8a%82%e7%82%b9CloudMatrix384%e8%ae%ba%e6%96%87%e6%8b%86%e8%a7%a3&u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f01-ascend-cloudmatrix%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://pillumina.github.io/>CctoctoFX</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div class=reading-progress-bar></div><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelector(".reading-progress-bar");if(!t)return;const n=document.querySelector(".post-single");if(!n)return;function s(){const e=n.getBoundingClientRect(),s=e.height,o=window.innerHeight,i=window.scrollY||window.pageYOffset,a=i/(s-o)*100;t.style.width=`${Math.min(100,Math.max(0,a))}%`}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){s(),e=!1}),e=!0)}),s()}),document.addEventListener("DOMContentLoaded",function(){mediumZoom("article img:not(.nozoom)",{margin:24,background:"var(--theme)",scrollOffset:0})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>