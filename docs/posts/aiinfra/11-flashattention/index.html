<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AIInfra] FlashAttention 深度解析：从数学原理到工程实现 | CctoctoFX</title><meta name=keywords content="flashattention"><meta name=description content="
本文从数学原理出发，深入分析FlashAttention的核心思想、算法设计和各版本演进，通过详实的数学推导、直观的流程图表和具体的数值示例，帮助读者真正掌握这一革命性的Attention优化技术。

1. 问题的本质：传统Attention的根本瓶颈
1.1 传统Attention机制的计算模式
传统的Self-Attention机制遵循如下计算流程：
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$让我们用具体数值来理解这个过程的复杂性：
示例场景：考虑一个典型的语言模型场景

序列长度：$n = 2048$（如GPT-2的上下文长度）
特征维度：$d_k = 64$（每个attention head的维度）
输入张量形状：$Q, K, V \in \mathbb{R}^{2048 \times 64}$

第一步：计算注意力得分矩阵
$$S = \frac{QK^T}{\sqrt{d_k}} \in \mathbb{R}^{2048 \times 2048}$$这一步产生了一个 $2048 \times 2048 = 4,194,304$ 个元素的矩阵，以FP16精度存储需要约8MB内存。
第二步：Softmax归一化
$$P = \text{softmax}(S) \in \mathbb{R}^{2048 \times 2048}$$Softmax计算需要：

计算每行的最大值：$m_i = \max_j S_{i,j}$
计算指数和：$l_i = \sum_j e^{S_{i,j} - m_i}$
归一化：$P_{i,j} = \frac{e^{S_{i,j} - m_i}}{l_i}$

这又需要存储另一个 $2048 \times 2048$ 的矩阵。"><meta name=author content="Me"><link rel=canonical href=https://pillumina.github.io/posts/aiinfra/11-flashattention/><link crossorigin=anonymous href=/assets/css/stylesheet.6bb3496edc6893983898d32cd431faa32603d95051e27f1b35cb71674993e199.css integrity="sha256-a7NJbtxok5g4mNMs1DH6oyYD2VBR4n8bNctxZ0mT4Zk=" rel="preload stylesheet" as=style><link rel=icon href=https://pillumina.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://pillumina.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://pillumina.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://pillumina.github.io/apple-touch-icon.png><link rel=mask-icon href=https://pillumina.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://pillumina.github.io/posts/aiinfra/11-flashattention/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>(function(){function t(){return document.querySelector(".post-content")||document.querySelector(".post-single")||document.body}function n(e){return/\$\$[\s\S]+?\$\$|\\\(|\\\)|\\\[|\\\]/.test(e)}function s(e){if(window.__mathjaxLoaded)return;window.__mathjaxLoaded=!0,window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code","tt"],ignoreHtmlClass:"no-math"}};var t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js",t.defer=!0,t.onload=function(){window.MathJax&&window.MathJax.typesetPromise&&window.MathJax.typesetPromise([e]).catch(function(e){console.warn("MathJax typeset error",e)})},document.head.appendChild(t)}function e(){try{if(typeof renderMathInElement=="function"){const e=t();renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,strict:!1,trust:!0,ignoredTags:["script","noscript","style","textarea","pre","code","tt"],ignoredClasses:["no-math"],macros:{"\\boldsymbol":"\\mathbf{#1}","\\bm":"\\mathbf{#1}"}}),setTimeout(function(){n(e.innerHTML)&&s(e)},200)}}catch(e){console.warn("KaTeX render error:",e)}}document.addEventListener("DOMContentLoaded",function(){e(),setTimeout(e,200)}),window.addEventListener("load",function(){setTimeout(e,0)})})()</script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>const config={startOnLoad:!0,theme:"neutral",themeVariables:{lineColor:"#0f0f0f"},flowchart:{useMaxWidth:!1,htmlLabels:!0}};mermaid.initialize(config),window.onload=()=>{window.mermaid.init(0[0],document.querySelectorAll(".language-mermaid"))}</script><link rel=stylesheet href=/css/custom.min.5546631dfa1f462125e654bf96b4df9741ed659b9e52cd4d0d174d735216e181.css integrity="sha256-VUZjHfofRiEl5lS/lrTfl0HtZZueUs1NDRdNc1IW4YE="><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]"),n=document.querySelectorAll(".toc a");if(t.length===0||n.length===0)return;const s={};t.forEach(e=>{s[e.id]=e.offsetTop});function i(){const t=window.scrollY+100;let e="";for(const[n,o]of Object.entries(s))if(t>=o)e=n;else break;return e}function o(){const e=i();if(n.forEach(e=>{e.classList.remove("active")}),e){const t=document.querySelector(`.toc a[href="#${e}"]`);t&&t.classList.add("active")}}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){o(),e=!1}),e=!0)}),o()})</script><meta property="og:url" content="https://pillumina.github.io/posts/aiinfra/11-flashattention/"><meta property="og:site_name" content="CctoctoFX"><meta property="og:title" content="[AIInfra] FlashAttention 深度解析：从数学原理到工程实现"><meta property="og:description" content=" 本文从数学原理出发，深入分析FlashAttention的核心思想、算法设计和各版本演进，通过详实的数学推导、直观的流程图表和具体的数值示例，帮助读者真正掌握这一革命性的Attention优化技术。
1. 问题的本质：传统Attention的根本瓶颈 1.1 传统Attention机制的计算模式 传统的Self-Attention机制遵循如下计算流程：
$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$让我们用具体数值来理解这个过程的复杂性：
示例场景：考虑一个典型的语言模型场景
序列长度：$n = 2048$（如GPT-2的上下文长度） 特征维度：$d_k = 64$（每个attention head的维度） 输入张量形状：$Q, K, V \in \mathbb{R}^{2048 \times 64}$ 第一步：计算注意力得分矩阵 $$S = \frac{QK^T}{\sqrt{d_k}} \in \mathbb{R}^{2048 \times 2048}$$这一步产生了一个 $2048 \times 2048 = 4,194,304$ 个元素的矩阵，以FP16精度存储需要约8MB内存。
第二步：Softmax归一化 $$P = \text{softmax}(S) \in \mathbb{R}^{2048 \times 2048}$$Softmax计算需要：
计算每行的最大值：$m_i = \max_j S_{i,j}$ 计算指数和：$l_i = \sum_j e^{S_{i,j} - m_i}$ 归一化：$P_{i,j} = \frac{e^{S_{i,j} - m_i}}{l_i}$ 这又需要存储另一个 $2048 \times 2048$ 的矩阵。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-15T11:30:12+08:00"><meta property="article:modified_time" content="2025-09-15T11:30:12+08:00"><meta property="article:tag" content="Flashattention"><meta property="og:image" content="https://pillumina.github.io/imgs/icon_head.png"><meta property="og:see_also" content="https://pillumina.github.io/posts/aiinfra/14-deterministic-rl/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://pillumina.github.io/imgs/icon_head.png"><meta name=twitter:title content="[AIInfra] FlashAttention 深度解析：从数学原理到工程实现"><meta name=twitter:description content="
本文从数学原理出发，深入分析FlashAttention的核心思想、算法设计和各版本演进，通过详实的数学推导、直观的流程图表和具体的数值示例，帮助读者真正掌握这一革命性的Attention优化技术。

1. 问题的本质：传统Attention的根本瓶颈
1.1 传统Attention机制的计算模式
传统的Self-Attention机制遵循如下计算流程：
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$让我们用具体数值来理解这个过程的复杂性：
示例场景：考虑一个典型的语言模型场景

序列长度：$n = 2048$（如GPT-2的上下文长度）
特征维度：$d_k = 64$（每个attention head的维度）
输入张量形状：$Q, K, V \in \mathbb{R}^{2048 \times 64}$

第一步：计算注意力得分矩阵
$$S = \frac{QK^T}{\sqrt{d_k}} \in \mathbb{R}^{2048 \times 2048}$$这一步产生了一个 $2048 \times 2048 = 4,194,304$ 个元素的矩阵，以FP16精度存储需要约8MB内存。
第二步：Softmax归一化
$$P = \text{softmax}(S) \in \mathbb{R}^{2048 \times 2048}$$Softmax计算需要：

计算每行的最大值：$m_i = \max_j S_{i,j}$
计算指数和：$l_i = \sum_j e^{S_{i,j} - m_i}$
归一化：$P_{i,j} = \frac{e^{S_{i,j} - m_i}}{l_i}$

这又需要存储另一个 $2048 \times 2048$ 的矩阵。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://pillumina.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI Infra","item":"https://pillumina.github.io/posts/aiinfra/"},{"@type":"ListItem","position":3,"name":"[AIInfra] FlashAttention 深度解析：从数学原理到工程实现","item":"https://pillumina.github.io/posts/aiinfra/11-flashattention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AIInfra] FlashAttention 深度解析：从数学原理到工程实现","name":"[AIInfra] FlashAttention 深度解析：从数学原理到工程实现","description":" 本文从数学原理出发，深入分析FlashAttention的核心思想、算法设计和各版本演进，通过详实的数学推导、直观的流程图表和具体的数值示例，帮助读者真正掌握这一革命性的Attention优化技术。\n1. 问题的本质：传统Attention的根本瓶颈 1.1 传统Attention机制的计算模式 传统的Self-Attention机制遵循如下计算流程：\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$让我们用具体数值来理解这个过程的复杂性：\n示例场景：考虑一个典型的语言模型场景\n序列长度：$n = 2048$（如GPT-2的上下文长度） 特征维度：$d_k = 64$（每个attention head的维度） 输入张量形状：$Q, K, V \\in \\mathbb{R}^{2048 \\times 64}$ 第一步：计算注意力得分矩阵 $$S = \\frac{QK^T}{\\sqrt{d_k}} \\in \\mathbb{R}^{2048 \\times 2048}$$这一步产生了一个 $2048 \\times 2048 = 4,194,304$ 个元素的矩阵，以FP16精度存储需要约8MB内存。\n第二步：Softmax归一化 $$P = \\text{softmax}(S) \\in \\mathbb{R}^{2048 \\times 2048}$$Softmax计算需要：\n计算每行的最大值：$m_i = \\max_j S_{i,j}$ 计算指数和：$l_i = \\sum_j e^{S_{i,j} - m_i}$ 归一化：$P_{i,j} = \\frac{e^{S_{i,j} - m_i}}{l_i}$ 这又需要存储另一个 $2048 \\times 2048$ 的矩阵。\n","keywords":["flashattention"],"articleBody":" 本文从数学原理出发，深入分析FlashAttention的核心思想、算法设计和各版本演进，通过详实的数学推导、直观的流程图表和具体的数值示例，帮助读者真正掌握这一革命性的Attention优化技术。\n1. 问题的本质：传统Attention的根本瓶颈 1.1 传统Attention机制的计算模式 传统的Self-Attention机制遵循如下计算流程：\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$让我们用具体数值来理解这个过程的复杂性：\n示例场景：考虑一个典型的语言模型场景\n序列长度：$n = 2048$（如GPT-2的上下文长度） 特征维度：$d_k = 64$（每个attention head的维度） 输入张量形状：$Q, K, V \\in \\mathbb{R}^{2048 \\times 64}$ 第一步：计算注意力得分矩阵 $$S = \\frac{QK^T}{\\sqrt{d_k}} \\in \\mathbb{R}^{2048 \\times 2048}$$这一步产生了一个 $2048 \\times 2048 = 4,194,304$ 个元素的矩阵，以FP16精度存储需要约8MB内存。\n第二步：Softmax归一化 $$P = \\text{softmax}(S) \\in \\mathbb{R}^{2048 \\times 2048}$$Softmax计算需要：\n计算每行的最大值：$m_i = \\max_j S_{i,j}$ 计算指数和：$l_i = \\sum_j e^{S_{i,j} - m_i}$ 归一化：$P_{i,j} = \\frac{e^{S_{i,j} - m_i}}{l_i}$ 这又需要存储另一个 $2048 \\times 2048$ 的矩阵。\n第三步：加权求和 $$O = PV \\in \\mathbb{R}^{2048 \\times 64}$$1.2 瓶颈分析：内存墙问题 graph TD A[输入 Q,K,V6MB] --\u003e B[计算 QK^T8MB] B --\u003e C[Softmax计算8MB] C --\u003e D[最终输出 O0.25MB] E[GPU显存] --\u003e F[片上缓存~1-2MB] E --\u003e G[全局显存数十GB] B -.-\u003e H[内存瓶颈需要16MB中间存储] C -.-\u003e H style H fill:#ff9999 style F fill:#99ff99 关键问题：\n内存占用：对于长度为$n$的序列，需要$O(n^2)$的内存存储attention矩阵 内存带宽：GPU的计算能力远超内存带宽，大量时间浪费在数据搬移上 可扩展性：序列长度翻倍，内存需求增长4倍 具体数值对比：\n序列长度 注意力矩阵大小 FP16内存需求 A100显存占比 1024 1M 元素 2MB 0.025% 2048 4M 元素 8MB 0.1% 4096 16M 元素 32MB 0.4% 8192 67M 元素 134MB 1.7% 16384 268M 元素 536MB 6.7% 1.3 GPU架构与访存模式的不匹配 现代GPU的内存层次结构：\n寄存器：~几KB，1个周期访问 共享内存：~100KB，几个周期访问 L2缓存：~几MB，数十个周期访问 全局显存：~80GB，数百个周期访问 传统Attention的访存模式违背了\"数据局部性\"原则，频繁访问全局显存，导致严重的访存瓶颈。\n2. FlashAttention的革命性思想：分块计算与在线算法 2.1 核心洞察：将二维问题转化为一维流式计算 FlashAttention的核心思想是避免物化（materialization）整个注意力矩阵，而是采用分块计算和在线更新的方式。\n关键观察：Softmax函数具有可分解性，可以通过增量更新的方式计算，无需存储完整的中间矩阵。\n2.2 分块Softmax的数学基础 2.2.1 传统Softmax的数值稳定计算 对于向量 $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]$，数值稳定的softmax计算为：\n$$ \\begin{align} m \u0026= \\max_i x_i \\\\ \\text{softmax}(x_i) \u0026= \\frac{e^{x_i - m}}{\\sum_{j=1}^n e^{x_j - m}} \\end{align} $$2.2.2 分块Softmax的数学推导 问题设定：假设我们要计算向量 $\\mathbf{x}$ 的softmax，但 $\\mathbf{x}$ 被分为两块：$\\mathbf{x}^{(1)} = [x_1, \\ldots, x_k]$ 和 $\\mathbf{x}^{(2)} = [x_{k+1}, \\ldots, x_n]$。\n第一块的计算：\n$$ \\begin{align} m^{(1)} \u0026= \\max_{1 \\leq i \\leq k} x_i \\\\ d^{(1)} \u0026= \\sum_{i=1}^k e^{x_i - m^{(1)}} \\\\ \\text{softmax}^{(1)}(x_i) \u0026= \\frac{e^{x_i - m^{(1)}}}{d^{(1)}} \\quad \\text{(临时结果)} \\end{align} $$第二块的计算：\n$$ \\begin{align} m^{(2)} \u0026= \\max_{k+1 \\leq i \\leq n} x_i \\\\ d^{(2)} \u0026= \\sum_{i=k+1}^n e^{x_i - m^{(2)}} \\end{align} $$合并更新：设全局最大值为 $m^{new} = \\max(m^{(1)}, m^{(2)})$，则：\n$$ \\begin{align} d^{new} \u0026= d^{(1)} \\cdot e^{m^{(1)} - m^{new}} + d^{(2)} \\cdot e^{m^{(2)} - m^{new}} \\\\ \\text{softmax}(x_i) \u0026= \\begin{cases} \\frac{e^{x_i - m^{new}}}{d^{new}} = \\frac{e^{x_i - m^{(1)}} \\cdot e^{m^{(1)} - m^{new}}}{d^{new}} \u0026 \\text{if } i \\leq k \\\\ \\frac{e^{x_i - m^{new}}}{d^{new}} = \\frac{e^{x_i - m^{(2)}} \\cdot e^{m^{(2)} - m^{new}}}{d^{new}} \u0026 \\text{if } i \u003e k \\end{cases} \\end{align} $$2.2.3 在线Softmax更新算法 更一般地，我们可以设计一个在线更新算法：\n算法状态：维护当前的 $(m, d, \\mathbf{o})$，其中：\n$m$：当前见过的最大logit值 $d$：当前的归一化因子 $\\mathbf{o}$：当前的输出累积 更新规则：当处理新的块 $(\\mathbf{x}^{new}, \\mathbf{v}^{new})$ 时：\n$$ \\begin{align} m^{new} \u0026= \\max(m^{old}, \\max(\\mathbf{x}^{new})) \\\\ d^{new} \u0026= d^{old} \\cdot e^{m^{old} - m^{new}} + \\sum_j e^{x_j^{new} - m^{new}} \\\\ \\mathbf{o}^{new} \u0026= \\mathbf{o}^{old} \\cdot \\frac{d^{old} \\cdot e^{m^{old} - m^{new}}}{d^{new}} + \\sum_j \\frac{e^{x_j^{new} - m^{new}}}{d^{new}} \\mathbf{v}_j^{new} \\end{align} $$2.3 FlashAttention的分块策略 graph TB subgraph \"传统Attention\" A1[\"Q: 2048x64\"] --\u003e B1[\"QKT: 2048x2048全量计算\"] K1[\"K: 2048x64\"] --\u003e B1 B1 --\u003e C1[\"Softmax: 2048x2048全量存储\"] C1 --\u003e D1[\"PV: 2048x64\"] V1[\"V: 2048x64\"] --\u003e D1 end subgraph \"FlashAttention分块\" A2[\"Q分块: BrX64\"] --\u003e B2[\"局部QKT: BrXBc片上计算\"] K2[\"K分块: BcX64\"] --\u003e B2 B2 --\u003e C2[\"在线Softmax更新无需存储完整矩阵\"] C2 --\u003e D2[\"累积输出BrX64\"] V2[\"V分块: BcX64\"] --\u003e D2 end style B1 fill:#ff9999 style C1 fill:#ff9999 style B2 fill:#99ff99 style C2 fill:#99ff99 关键参数：\n$B_r$：Query块大小（通常为64-128） $B_c$：Key/Value块大小（通常为64-128） 总内存使用：$O(B_r \\cdot B_c)$ 而非 $O(n^2)$ 2.4 数学正确性证明 定理：FlashAttention的分块计算结果与传统Attention完全等价。\n证明思路：\nSoftmax可分解性：根据上述在线更新公式，分块计算的softmax等价于全量计算 线性性保持：注意力加权求和的线性性在分块过程中保持不变 数值稳定性：通过维护全局最大值，避免了数值溢出 具体验证：设 $Q \\in \\mathbb{R}^{n \\times d}$，$K, V \\in \\mathbb{R}^{n \\times d}$，传统方法计算：\n$$O_{traditional} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V$$FlashAttention分块计算：\n$$O_{flash} = \\text{OnlineSoftmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V$$数学上可证明 $O_{traditional} = O_{flash}$（在数值精度范围内）。\n3. FlashAttention各版本的演进：从概念验证到产业标准 3.1 FlashAttention v1 (2022)：开创性的分块算法 3.1.1 设计目标 核心目标：证明分块Attention的可行性，解决 $O(n^2)$ 内存瓶颈\n技术挑战：\n如何在不损失精度的情况下分块计算softmax 如何设计高效的GPU kernel实现 如何处理反向传播的梯度计算 3.1.2 关键创新 1. 分块大小的理论分析\n对于序列长度 $n$，特征维度 $d$，SRAM大小 $M$：\n最优分块大小：$B_c = \\left\\lfloor \\frac{M}{4d} \\right\\rfloor$，$B_r = \\min\\left(B_c, \\frac{M}{4d}\\right)$\n理论依据：\n每个块需要存储：$Q$ 块 ($B_r \\times d$)、$K$ 块 ($B_c \\times d$)、$V$ 块 ($B_c \\times d$)、输出块 ($B_r \\times d$) 总内存需求：$4Bd$，必须小于SRAM容量 $M$ 2. 访存复杂度分析\ngraph LR subgraph \"内存访问模式\" A[\"HBM读取Q,K,V: O(nd)\"] --\u003e B[\"SRAM计算局部QK^T: O(B²)\"] B --\u003e C[\"SRAM更新在线softmax: O(B²)\"] C --\u003e D[\"HBM写回输出O: O(nd)\"] end subgraph \"复杂度对比\" E[\"传统方法HBM: O(n²+nd)计算: O(n²d)\"] F[\"FlashAttention v1HBM: O(nd)计算: O(n²d)\"] end style F fill:#99ff99 style E fill:#ff9999 访存优化效果：\n传统方法：$O(n^2)$ HBM访问 FlashAttention v1：$O(nd)$ HBM访问 加速比：理论上可达 $\\frac{n^2}{nd} = \\frac{n}{d}$ 倍 3.1.3 算法实现细节 前向传播算法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Input: Q, K, V ∈ ℝⁿˣᵈ, 分块大小 Br, Bc Output: O ∈ ℝⁿˣᵈ 1. 将 Q 分为 ⌈n/Br⌉ 块，K,V 分为 ⌈n/Bc⌉ 块 2. 初始化 O = 0 ∈ ℝⁿˣᵈ 3. for i = 1 to ⌈n/Br⌉ do: a. 加载 Qi ∈ ℝᴮʳˣᵈ 到SRAM b. 初始化 ℓi = 0 ∈ ℝᴮʳ, mi = -∞ ∈ ℝᴮʳ, Oi = 0 ∈ ℝᴮʳˣᵈ c. for j = 1 to ⌈n/Bc⌉ do: i. 加载 Kj, Vj ∈ ℝᴮᶜˣᵈ 到SRAM ii. 计算 Sij = QiKjᵀ ∈ ℝᴮʳˣᴮᶜ iii. 计算 m̃ij = rowmax(Sij) ∈ ℝᴮʳ iv. 计算 P̃ij = exp(Sij - m̃ij) ∈ ℝᴮʳˣᴮᶜ v. 计算 ℓ̃ij = rowsum(P̃ij) ∈ ℝᴮʳ vi. 计算 mi^new = max(mi, m̃ij), ℓi^new = ℓi·exp(mi - mi^new) + ℓ̃ij·exp(m̃ij - mi^new) vii. 更新 Oi = diag(ℓi/ℓi^new)·exp(mi - mi^new)·Oi + diag(exp(m̃ij - mi^new)/ℓi^new)·P̃ij·Vj viii.更新 ℓi = ℓi^new, mi = mi^new d. 将 Oi 写回HBM 反向传播算法：需要重新计算前向过程中的中间值，因为它们没有被存储。\n3.2 FlashAttention v2 (2023)：工程优化与扩展性 3.2.1 设计目标 核心目标：在保持v1算法正确性的基础上，大幅提升实际性能\n优化方向：\n并行化优化：更好利用GPU的并行计算能力 内存访问优化：减少不必要的内存传输 支持更多场景：causal mask、不同head维度等 3.2.2 关键优化 1. 分块策略重新设计\nv1的问题：外层循环遍历Query块，内层循环遍历Key/Value块，导致：\nQuery块需要重复加载 并行度不够高 v2的改进：\n按Key/Value维度分块：外层循环遍历Key/Value，内层遍历Query 更好的并行性：不同的Query块可以并行处理 2. 工作分配优化\ngraph TD subgraph \"FlashAttention v1\" A1[\"Thread Block 1处理Q块1\"] --\u003e B1[\"串行处理所有K,V块\"] A2[\"Thread Block 2处理Q块2\"] --\u003e B2[\"串行处理所有K,V块\"] A3[\"Thread Block 3处理Q块3\"] --\u003e B3[\"串行处理所有K,V块\"] end subgraph \"FlashAttention v2\" C1[\"Thread Block 1处理K块1,V块1\"] --\u003e D1[\"并行处理所有Q块\"] C2[\"Thread Block 2处理K块2,V块2\"] --\u003e D2[\"并行处理所有Q块\"] C3[\"Thread Block 3处理K块3,V块3\"] --\u003e D3[\"并行处理所有Q块\"] end style A1 fill:#ff9999 style A2 fill:#ff9999 style A3 fill:#ff9999 style C1 fill:#99ff99 style C2 fill:#99ff99 style C3 fill:#99ff99 3. 内存访问模式优化\n减少冗余加载：Key和Value块在多个Query块间共享 更好的缓存利用：改进内存访问的空间局部性 向量化操作：更充分利用GPU的向量化指令 3.2.3 性能提升分析 理论分析：\n并行度提升：从 $O(\\lceil n/B_r \\rceil)$ 提升到 $O(\\lceil n/B_c \\rceil \\times \\lceil n/B_r \\rceil)$ 内存访问优化：减少约20-30%的冗余访问 计算效率：更好的指令级并行和向量化 实际性能：\n序列长度 Head维度 v1性能 v2性能 提升比例 2048 64 1.2x 1.8x +50% 4096 64 1.5x 2.3x +53% 8192 64 1.8x 3.1x +72% 3.3 FlashAttention v3 (2024)：硬件协同设计 3.3.1 设计目标 核心目标：充分利用新一代GPU硬件特性，支持更复杂的应用场景\n技术趋势：\n新硬件特性：H100的FP8支持、更大的共享内存 应用需求：更长序列、混合精度、稀疏attention 系统集成：更好的编译器支持、自动调优 3.3.2 核心创新 1. 异构精度计算\n支持FP8/FP16/FP32混合精度：\n输入：FP8存储，减少内存带宽 计算：FP16/FP32，保证数值精度 输出：根据需求选择精度 2. 自适应分块策略\ngraph TD A[输入分析] --\u003e B{序列长度} B --\u003e|短序列\u003c1024| C[小块策略Br=64, Bc=64] B --\u003e|中等序列1024-4096| D[中等块策略Br=128, Bc=128] B --\u003e|长序列\u003e4096| E[大块策略Br=256, Bc=256] F[硬件检测] --\u003e G{GPU类型} G --\u003e|A100| H[优化A100参数] G --\u003e|H100| I[优化H100参数] G --\u003e|其他| J[通用参数] C --\u003e K[执行kernel] D --\u003e K E --\u003e K H --\u003e K I --\u003e K J --\u003e K 3. 编译时优化\n模板特化：针对常见的head维度生成专门的kernel 循环展开：减少分支预测开销 指令调度：更好的指令级并行 3.3.3 应用场景扩展 1. 长上下文支持\n支持1M+token的超长序列 分层attention策略 渐进式精度降低 2. 稀疏attention模式\nBlock-sparse attention 滑动窗口attention 局部-全局混合attention 3. 多模态支持\n文本-图像联合attention 不同模态的attention权重 跨模态的梯度优化 4. FlashAttention算法流程深度解析 4.1 完整算法流程可视化 4.1.1 整体计算流程 flowchart TD A[\"输入矩阵Q: nxd, K: nxd, V: nxd\"] --\u003e B[\"矩阵分块\"] B --\u003e C[\"Q分块: ⌈n/Br⌉个块每块大小BrXd\"] B --\u003e D[\"K,V分块: ⌈n/Bc⌉个块每块大小BcXd\"] C --\u003e E[外层循环: 遍历Q块] D --\u003e F[内层循环: 遍历K,V块] E --\u003e G[加载Qi到SRAM] F --\u003e H[加载Kj,Vj到SRAM] G --\u003e I[\"计算局部注意力得分Sij = Qi x KjT\"] H --\u003e I I --\u003e J[\"在线Softmax更新\"] J --\u003e K[\"计算加权输出Oi += aij x Vj\"] K --\u003e L{所有K,V块处理完毕?} L --\u003e|否| F L --\u003e|是| M[写回Oi到HBM] M --\u003e N{所有Q块处理完毕?} N --\u003e|否| E N --\u003e|是| O[输出完整结果O] style A fill:#e1f5fe style G fill:#c8e6c9 style H fill:#c8e6c9 style I fill:#fff3e0 style J fill:#fce4ec style K fill:#f3e5f5 style O fill:#e8f5e8 4.1.2 SRAM内存管理流程 sequenceDiagram participant HBM as HBM显存 participant SRAM as SRAM缓存 participant Compute as 计算单元 Note over HBM,Compute: 处理第i个Q块，第j个K,V块 HBM-\u003e\u003eSRAM: 1. 加载Qi (BrXd) HBM-\u003e\u003eSRAM: 2. 加载Kj (BcXd) HBM-\u003e\u003eSRAM: 3. 加载Vj (BcXd) SRAM-\u003e\u003eCompute: 4. 计算Sij = QiXKjT Compute-\u003e\u003eSRAM: 5. 存储局部得分Sij SRAM-\u003e\u003eCompute: 6. 在线Softmax更新 Note over Compute: 更新mi, li, Oi SRAM-\u003e\u003eCompute: 7. 计算PijXVj Compute-\u003e\u003eSRAM: 8. 累积到输出Oi Note over SRAM: 释放Kj, Vj空间 alt 所有K,V块处理完毕 SRAM-\u003e\u003eHBM: 9. 写回最终Oi Note over SRAM: 释放Qi空间 end 4.2 核心算法：在线Softmax更新详解 4.2.1 状态维护与更新 算法状态：对每个Query块 $Q_i$，维护三元组 $(m_i, \\ell_i, O_i)$：\n$$ \\begin{align} m_i \u0026\\in \\mathbb{R}^{B_r} \\quad \\text{(当前最大logit值)} \\\\ \\ell_i \u0026\\in \\mathbb{R}^{B_r} \\quad \\text{(当前归一化因子)} \\\\ O_i \u0026\\in \\mathbb{R}^{B_r \\times d} \\quad \\text{(当前输出累积)} \\end{align} $$4.2.2 逐步更新过程 graph TD subgraph \"第j步更新前状态\" A1[\"mi⁽ʲ⁻¹⁾: 前j-1块的最大值\"] A2[\"ℓi⁽ʲ⁻¹⁾: 前j-1块的归一化因子\"] A3[\"Oi⁽ʲ⁻¹⁾: 前j-1块的累积输出\"] end subgraph \"当前块计算\" B1[\"计算Sij = QiXKjT\"] B2[\"计算局部最大值 rowmax(Sij)\"] B3[\"计算局部softmax权重\"] B4[\"计算局部行和 rowsum\"] end subgraph \"状态更新\" C1[\"更新全局最大值\"] C2[\"更新归一化因子\"] C3[\"更新累积输出\"] end A1 --\u003e C1 A2 --\u003e C2 A3 --\u003e C3 B1 --\u003e B2 --\u003e B3 --\u003e B4 B2 --\u003e C1 B3 --\u003e C3 B4 --\u003e C2 style B1 fill:#fff3e0 style C1 fill:#e8f5e8 style C2 fill:#e8f5e8 style C3 fill:#e8f5e8 更新公式详解：\n设当前处理第 $j$ 个Key/Value块，更新规则为：\n$$ \\begin{align} m_i^{(j)} \u0026= \\max(m_i^{(j-1)}, \\tilde{m}_{ij}) \\\\ \\alpha \u0026= \\exp(m_i^{(j-1)} - m_i^{(j)}) \\\\ \\beta \u0026= \\exp(\\tilde{m}_{ij} - m_i^{(j)}) \\\\ \\ell_i^{(j)} \u0026= \\ell_i^{(j-1)} \\cdot \\alpha + \\tilde{\\ell}_{ij} \\cdot \\beta \\\\ O_i^{(j)} \u0026= \\frac{\\ell_i^{(j-1)}}{\\ell_i^{(j)}} \\cdot \\alpha \\cdot O_i^{(j-1)} + \\frac{\\beta}{\\ell_i^{(j)}} \\cdot \\tilde{P}_{ij} V_j \\end{align} $$4.3 具体数值示例：逐步计算过程 4.3.1 示例设置 输入参数：\n序列长度：$n = 4$ 特征维度：$d = 2$ 分块大小：$B_r = B_c = 2$ 输入矩阵：\n$$ Q = \\begin{bmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\\\ 2 \u0026 1 \\\\ 1 \u0026 2 \\end{bmatrix}, \\quad K = \\begin{bmatrix} 1 \u0026 1 \\\\ 0 \u0026 2 \\\\ 1 \u0026 0 \\\\ 2 \u0026 1 \\end{bmatrix}, \\quad V = \\begin{bmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\\\ 2 \u0026 1 \\\\ 1 \u0026 2 \\end{bmatrix} $$4.3.2 分块结果 $$ Q_1 = \\begin{bmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\end{bmatrix}, \\quad Q_2 = \\begin{bmatrix} 2 \u0026 1 \\\\ 1 \u0026 2 \\end{bmatrix} $$$$ K_1 = \\begin{bmatrix} 1 \u0026 1 \\\\ 0 \u0026 2 \\end{bmatrix}, \\quad K_2 = \\begin{bmatrix} 1 \u0026 0 \\\\ 2 \u0026 1 \\end{bmatrix} $$$$ V_1 = \\begin{bmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\end{bmatrix}, \\quad V_2 = \\begin{bmatrix} 2 \u0026 1 \\\\ 1 \u0026 2 \\end{bmatrix} $$4.3.3 处理Q1块的详细步骤 步骤1：处理K1,V1块\n计算注意力得分：\n$$S_{11} = Q_1 K_1^T = \\begin{bmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 1 \u0026 0 \\\\ 1 \u0026 2 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \\\\ 1 \u0026 2 \\end{bmatrix}$$ 计算行最大值：\n$$\\tilde{m}_{11} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$$ 计算softmax权重：\n$$\\tilde{P}_{11} = \\begin{bmatrix} e^{1-1} \u0026 e^{0-1} \\\\ e^{1-2} \u0026 e^{2-2} \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 e^{-1} \\\\ e^{-1} \u0026 1 \\end{bmatrix}$$ 计算行和：\n$$\\tilde{\\ell}_{11} = \\begin{bmatrix} 1 + e^{-1} \\\\ e^{-1} + 1 \\end{bmatrix} \\approx \\begin{bmatrix} 1.368 \\\\ 1.368 \\end{bmatrix}$$ 初始化状态：\n$$m_1^{(1)} = \\tilde{m}_{11} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\ell_1^{(1)} = \\tilde{\\ell}_{11} \\approx \\begin{bmatrix} 1.368 \\\\ 1.368 \\end{bmatrix}$$ 计算输出：\n$$O_1^{(1)} = \\frac{\\tilde{P}_{11}}{\\ell_1^{(1)}} V_1 \\approx \\begin{bmatrix} 0.731 \u0026 0.269 \\\\ 0.269 \u0026 0.731 \\end{bmatrix} \\begin{bmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} 0.731 \u0026 0.269 \\\\ 0.269 \u0026 0.731 \\end{bmatrix}$$ 步骤2：处理K2,V2块\n计算注意力得分：\n$$S_{12} = Q_1 K_2^T = \\begin{bmatrix} 1 \u0026 0 \\\\ 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 1 \u0026 2 \\\\ 0 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 2 \\\\ 0 \u0026 1 \\end{bmatrix}$$ 计算行最大值：\n$$\\tilde{m}_{12} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$$ 更新全局最大值：\n$$m_1^{(2)} = \\max(m_1^{(1)}, \\tilde{m}_{12}) = \\max\\left(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\right) = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}$$ 计算修正因子：\n$$\\alpha = \\exp(m_1^{(1)} - m_1^{(2)}) = \\exp\\left(\\begin{bmatrix} 1-2 \\\\ 2-2 \\end{bmatrix}\\right) = \\begin{bmatrix} e^{-1} \\\\ 1 \\end{bmatrix}$$ $$\\beta = \\exp(\\tilde{m}_{12} - m_1^{(2)}) = \\exp\\left(\\begin{bmatrix} 2-2 \\\\ 1-2 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 \\\\ e^{-1} \\end{bmatrix}$$ 更新归一化因子和输出… 4.4 算法复杂度分析 4.4.1 时间复杂度 graph LR subgraph \"计算复杂度分析\" A[\"外层循环⌈n/Br⌉次\"] --\u003e B[\"内层循环⌈n/Bc⌉次\"] B --\u003e C[\"矩阵乘法O(BrXBcXd)\"] C --\u003e D[\"Softmax计算O(BrXBc)\"] D --\u003e E[\"输出更新O(BrXBcXd)\"] end subgraph \"总复杂度\" F[\"总时间复杂度O(n²d)\"] G[\"与传统方法相同但常数项更小\"] end A --\u003e F style F fill:#e8f5e8 style G fill:#fff3e0 详细分析：\n外层循环：$\\lceil n/B_r \\rceil$ 次 内层循环：$\\lceil n/B_c \\rceil$ 次 每次迭代：$O(B_r B_c d + B_r B_c + B_r B_c d) = O(B_r B_c d)$ 总时间复杂度：$O(\\frac{n}{B_r} \\cdot \\frac{n}{B_c} \\cdot B_r B_c d) = O(n^2 d)$ 4.4.2 空间复杂度 SRAM使用分析：\nQuery块：$B_r \\times d$ Key块：$B_c \\times d$ Value块：$B_c \\times d$ 中间结果：$B_r \\times B_c$（注意力得分） 状态向量：$B_r$（最大值）+ $B_r$（归一化因子） 输出块：$B_r \\times d$ 总SRAM需求：$O(B_r d + B_c d + B_r B_c) = O((B_r + B_c)d + B_r B_c)$\nHBM使用：仅需存储输入输出，$O(nd)$，相比传统方法的 $O(n^2 + nd)$ 大幅减少。\n5. 行业实践与生态发展 5.1 主流框架集成现状 5.1.1 深度学习框架支持 graph TB subgraph \"主流框架集成\" A[PyTorch] --\u003e A1[torch.nn.functional.scaled_dot_product_attention原生支持FlashAttention v2] A --\u003e A2[xFormers库完整FlashAttention实现] B[TensorFlow] --\u003e B1[TensorFlow Addons社区FlashAttention实现] B --\u003e B2[JAX/Flax研究社区广泛使用] C[HuggingFace] --\u003e C1[Transformers库默认启用FlashAttention] C --\u003e C2[Accelerate库自动优化选择] end subgraph \"推理框架\" D[vLLM] --\u003e D1[生产级推理FlashAttention v2集成] E[TensorRT-LLM] --\u003e E1[NVIDIA优化硬件特定优化] F[Text Generation Inference] --\u003e F1[HuggingFace推理自动FlashAttention] end style A1 fill:#c8e6c9 style C1 fill:#c8e6c9 style D1 fill:#c8e6c9 5.1.2 性能基准测试 训练性能提升（相对于标准Attention）：\n模型规模 序列长度 传统Attention FlashAttention v2 内存节省 速度提升 125M 2048 基线 1.8x 60% 80% 1.3B 2048 基线 2.1x 65% 110% 6.7B 4096 OOM 可运行 70% N/A 13B 8192 OOM 可运行 75% N/A 推理性能提升：\ngraph LR subgraph \"推理延迟对比 (ms)\" A[序列长度: 1024标准: 45msFlash: 28ms提升: 38%] B[序列长度: 2048标准: 180msFlash: 89ms提升: 51%] C[序列长度: 4096标准: 720msFlash: 298ms提升: 59%] D[序列长度: 8192标准: OOMFlash: 1100ms提升: 可运行] end style A fill:#e8f5e8 style B fill:#c8e6c9 style C fill:#a5d6a7 style D fill:#81c784 5.2 产业应用案例 5.2.1 大型语言模型训练 OpenAI GPT系列：\nGPT-3.5/4训练中广泛使用FlashAttention优化 支持更长的上下文窗口（32k tokens） 训练成本降低约30-40% Meta Llama系列：\nLlama 2/3全面采用FlashAttention v2 支持70B参数模型的高效训练 Code Llama支持16k代码上下文 Google PaLM/Gemini：\n内部优化版本的FlashAttention 针对TPU硬件的特定优化 支持多模态长序列处理 5.3 技术生态与工具链 5.3.1 开发工具支持 性能分析工具：\nNVIDIA Nsight：支持FlashAttention kernel分析 PyTorch Profiler：可视化FlashAttention性能特征 自动优化工具：\nDeepSpeed：自动选择最优的Attention实现 FairScale：大规模训练中的FlashAttention集成 Composer：训练配方中的自动FlashAttention启用 5.3.2 社区贡献与扩展 开源实现：\nflash-attn：官方Python包，支持多种GPU xFormers：Meta开源的高效Transformer组件 FlashInfer：专门用于推理的FlashAttention实现 研究扩展：\nFlashAttention-2：进一步的工程优化 PagedAttention：结合分页机制的变体 StreamingLLM：流式处理的FlashAttention变体 5.4 未来发展趋势 5.4.1 硬件协同演进 下一代GPU优化：\ntimeline title FlashAttention硬件协同发展路线图 2022 : FlashAttention v1 : A100/V100支持 : FP16/BF16精度 2023 : FlashAttention v2/v3 : H100优化 : FP8支持 : 更大shared memory 2024 : 硬件特定优化 : Grace Hopper架构 : 统一内存访问 : AI专用指令集 2025+ : 未来趋势 : 内存计算融合 核心要点：FlashAttention的成功不是偶然的，它是数学理论、工程实践和系统思维完美结合的结果。理解其本质，不仅能帮助我们更好地使用这一技术，更能启发我们在面对其他复杂系统问题时的思维方式。\n致谢：本文的完成得益于FlashAttention原作者Tri Dao等人的开创性工作，以及整个AI社区在理论研究和工程实践方面的持续贡献。\n如需获取更多技术细节、实现代码或性能基准测试数据，欢迎进一步交流讨论。\n","wordCount":"1965","inLanguage":"en","image":"https://pillumina.github.io/imgs/icon_head.png","datePublished":"2025-09-15T11:30:12+08:00","dateModified":"2025-09-15T11:30:12+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://pillumina.github.io/posts/aiinfra/11-flashattention/"},"publisher":{"@type":"Organization","name":"CctoctoFX","logo":{"@type":"ImageObject","url":"https://pillumina.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://pillumina.github.io/ accesskey=h title="CctoctoFX (Alt + H)"><img src=https://pillumina.github.io/apple-touch-icon.png alt aria-label=logo height=30>CctoctoFX</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://pillumina.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://pillumina.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://pillumina.github.io/posts/aiinfra/ title="AI Infra"><span>AI Infra</span></a></li><li><a href=https://pillumina.github.io/posts/llmtheory/ title=Thoery><span>Thoery</span></a></li><li><a href=https://pillumina.github.io/posts/programming/ title=Programming><span>Programming</span></a></li><li><a href=https://pillumina.github.io/social/ title=Social><span>Social</span></a></li><li><a href=https://pillumina.github.io/open_courses/ title=Study><span>Study</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://pillumina.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/aiinfra/>AI Infra</a></div><h1 class="post-title entry-hint-parent">[AIInfra] FlashAttention 深度解析：从数学原理到工程实现</h1><div class=post-meta><span title='2025-09-15 11:30:12 +0800 CST'>September 15, 2025</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;1965 words&nbsp;·&nbsp;Me</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#1-问题的本质传统attention的根本瓶颈>1. 问题的本质：传统Attention的根本瓶颈</a><ul><li><a href=#11-传统attention机制的计算模式>1.1 传统Attention机制的计算模式</a><ul><li><a href=#第一步计算注意力得分矩阵>第一步：计算注意力得分矩阵</a></li><li><a href=#第二步softmax归一化>第二步：Softmax归一化</a></li><li><a href=#第三步加权求和>第三步：加权求和</a></li></ul></li><li><a href=#12-瓶颈分析内存墙问题>1.2 瓶颈分析：内存墙问题</a></li><li><a href=#13-gpu架构与访存模式的不匹配>1.3 GPU架构与访存模式的不匹配</a></li></ul></li><li><a href=#2-flashattention的革命性思想分块计算与在线算法>2. FlashAttention的革命性思想：分块计算与在线算法</a><ul><li><a href=#21-核心洞察将二维问题转化为一维流式计算>2.1 核心洞察：将二维问题转化为一维流式计算</a></li><li><a href=#22-分块softmax的数学基础>2.2 分块Softmax的数学基础</a><ul><li><a href=#221-传统softmax的数值稳定计算>2.2.1 传统Softmax的数值稳定计算</a></li><li><a href=#222-分块softmax的数学推导>2.2.2 分块Softmax的数学推导</a></li><li><a href=#223-在线softmax更新算法>2.2.3 在线Softmax更新算法</a></li></ul></li><li><a href=#23-flashattention的分块策略>2.3 FlashAttention的分块策略</a></li><li><a href=#24-数学正确性证明>2.4 数学正确性证明</a></li></ul></li><li><a href=#3-flashattention各版本的演进从概念验证到产业标准>3. FlashAttention各版本的演进：从概念验证到产业标准</a><ul><li><a href=#31-flashattention-v1-2022开创性的分块算法>3.1 FlashAttention v1 (2022)：开创性的分块算法</a><ul><li><a href=#311-设计目标>3.1.1 设计目标</a></li><li><a href=#312-关键创新>3.1.2 关键创新</a></li><li><a href=#313-算法实现细节>3.1.3 算法实现细节</a></li></ul></li><li><a href=#32-flashattention-v2-2023工程优化与扩展性>3.2 FlashAttention v2 (2023)：工程优化与扩展性</a><ul><li><a href=#321-设计目标>3.2.1 设计目标</a></li><li><a href=#322-关键优化>3.2.2 关键优化</a></li><li><a href=#323-性能提升分析>3.2.3 性能提升分析</a></li></ul></li><li><a href=#33-flashattention-v3-2024硬件协同设计>3.3 FlashAttention v3 (2024)：硬件协同设计</a><ul><li><a href=#331-设计目标>3.3.1 设计目标</a></li><li><a href=#332-核心创新>3.3.2 核心创新</a></li><li><a href=#333-应用场景扩展>3.3.3 应用场景扩展</a></li></ul></li></ul></li><li><a href=#4-flashattention算法流程深度解析>4. FlashAttention算法流程深度解析</a><ul><li><a href=#41-完整算法流程可视化>4.1 完整算法流程可视化</a><ul><li><a href=#411-整体计算流程>4.1.1 整体计算流程</a></li><li><a href=#412-sram内存管理流程>4.1.2 SRAM内存管理流程</a></li></ul></li><li><a href=#42-核心算法在线softmax更新详解>4.2 核心算法：在线Softmax更新详解</a><ul><li><a href=#421-状态维护与更新>4.2.1 状态维护与更新</a></li><li><a href=#422-逐步更新过程>4.2.2 逐步更新过程</a></li></ul></li><li><a href=#43-具体数值示例逐步计算过程>4.3 具体数值示例：逐步计算过程</a><ul><li><a href=#431-示例设置>4.3.1 示例设置</a></li><li><a href=#432-分块结果>4.3.2 分块结果</a></li><li><a href=#433-处理q1块的详细步骤>4.3.3 处理Q1块的详细步骤</a></li></ul></li><li><a href=#44-算法复杂度分析>4.4 算法复杂度分析</a><ul><li><a href=#441-时间复杂度>4.4.1 时间复杂度</a></li><li><a href=#442-空间复杂度>4.4.2 空间复杂度</a></li></ul></li></ul></li><li><a href=#5-行业实践与生态发展>5. 行业实践与生态发展</a><ul><li><a href=#51-主流框架集成现状>5.1 主流框架集成现状</a><ul><li><a href=#511-深度学习框架支持>5.1.1 深度学习框架支持</a></li><li><a href=#512-性能基准测试>5.1.2 性能基准测试</a></li></ul></li><li><a href=#52-产业应用案例>5.2 产业应用案例</a><ul><li><a href=#521-大型语言模型训练>5.2.1 大型语言模型训练</a></li></ul></li><li><a href=#53-技术生态与工具链>5.3 技术生态与工具链</a><ul><li><a href=#531-开发工具支持>5.3.1 开发工具支持</a></li><li><a href=#532-社区贡献与扩展>5.3.2 社区贡献与扩展</a></li></ul></li><li><a href=#54-未来发展趋势>5.4 未来发展趋势</a><ul><li><a href=#541-硬件协同演进>5.4.1 硬件协同演进</a></li></ul></li></ul></li></ul></li></ul></nav></div></details></div><div class=post-content><blockquote><p>本文从数学原理出发，深入分析FlashAttention的核心思想、算法设计和各版本演进，通过详实的数学推导、直观的流程图表和具体的数值示例，帮助读者真正掌握这一革命性的Attention优化技术。</p></blockquote><hr><h2 id=1-问题的本质传统attention的根本瓶颈>1. 问题的本质：传统Attention的根本瓶颈<a hidden class=anchor aria-hidden=true href=#1-问题的本质传统attention的根本瓶颈>#</a></h2><h3 id=11-传统attention机制的计算模式>1.1 传统Attention机制的计算模式<a hidden class=anchor aria-hidden=true href=#11-传统attention机制的计算模式>#</a></h3><p>传统的Self-Attention机制遵循如下计算流程：</p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$<p>让我们用具体数值来理解这个过程的复杂性：</p><p><strong>示例场景</strong>：考虑一个典型的语言模型场景</p><ul><li>序列长度：$n = 2048$（如GPT-2的上下文长度）</li><li>特征维度：$d_k = 64$（每个attention head的维度）</li><li>输入张量形状：$Q, K, V \in \mathbb{R}^{2048 \times 64}$</li></ul><h4 id=第一步计算注意力得分矩阵>第一步：计算注意力得分矩阵<a hidden class=anchor aria-hidden=true href=#第一步计算注意力得分矩阵>#</a></h4>$$S = \frac{QK^T}{\sqrt{d_k}} \in \mathbb{R}^{2048 \times 2048}$$<p>这一步产生了一个 $2048 \times 2048 = 4,194,304$ 个元素的矩阵，以FP16精度存储需要约8MB内存。</p><h4 id=第二步softmax归一化>第二步：Softmax归一化<a hidden class=anchor aria-hidden=true href=#第二步softmax归一化>#</a></h4>$$P = \text{softmax}(S) \in \mathbb{R}^{2048 \times 2048}$$<p>Softmax计算需要：</p><ol><li>计算每行的最大值：$m_i = \max_j S_{i,j}$</li><li>计算指数和：$l_i = \sum_j e^{S_{i,j} - m_i}$</li><li>归一化：$P_{i,j} = \frac{e^{S_{i,j} - m_i}}{l_i}$</li></ol><p>这又需要存储另一个 $2048 \times 2048$ 的矩阵。</p><h4 id=第三步加权求和>第三步：加权求和<a hidden class=anchor aria-hidden=true href=#第三步加权求和>#</a></h4>$$O = PV \in \mathbb{R}^{2048 \times 64}$$<h3 id=12-瓶颈分析内存墙问题>1.2 瓶颈分析：内存墙问题<a hidden class=anchor aria-hidden=true href=#12-瓶颈分析内存墙问题>#</a></h3><pre class=mermaid>
  graph TD
    A[输入 Q,K,V&lt;br/&gt;6MB] --&gt; B[计算 QK^T&lt;br/&gt;8MB]
    B --&gt; C[Softmax计算&lt;br/&gt;8MB]
    C --&gt; D[最终输出 O&lt;br/&gt;0.25MB]
    
    E[GPU显存] --&gt; F[片上缓存&lt;br/&gt;~1-2MB]
    E --&gt; G[全局显存&lt;br/&gt;数十GB]
    
    B -.-&gt; H[内存瓶颈&lt;br/&gt;需要16MB中间存储]
    C -.-&gt; H
    
    style H fill:#ff9999
    style F fill:#99ff99
</pre><p><strong>关键问题</strong>：</p><ol><li><strong>内存占用</strong>：对于长度为$n$的序列，需要$O(n^2)$的内存存储attention矩阵</li><li><strong>内存带宽</strong>：GPU的计算能力远超内存带宽，大量时间浪费在数据搬移上</li><li><strong>可扩展性</strong>：序列长度翻倍，内存需求增长4倍</li></ol><p><strong>具体数值对比</strong>：</p><table><thead><tr><th>序列长度</th><th>注意力矩阵大小</th><th>FP16内存需求</th><th>A100显存占比</th></tr></thead><tbody><tr><td>1024</td><td>1M 元素</td><td>2MB</td><td>0.025%</td></tr><tr><td>2048</td><td>4M 元素</td><td>8MB</td><td>0.1%</td></tr><tr><td>4096</td><td>16M 元素</td><td>32MB</td><td>0.4%</td></tr><tr><td>8192</td><td>67M 元素</td><td>134MB</td><td>1.7%</td></tr><tr><td>16384</td><td>268M 元素</td><td>536MB</td><td>6.7%</td></tr></tbody></table><h3 id=13-gpu架构与访存模式的不匹配>1.3 GPU架构与访存模式的不匹配<a hidden class=anchor aria-hidden=true href=#13-gpu架构与访存模式的不匹配>#</a></h3><p>现代GPU的内存层次结构：</p><ul><li><strong>寄存器</strong>：~几KB，1个周期访问</li><li><strong>共享内存</strong>：~100KB，几个周期访问</li><li><strong>L2缓存</strong>：~几MB，数十个周期访问</li><li><strong>全局显存</strong>：~80GB，数百个周期访问</li></ul><p>传统Attention的访存模式违背了"数据局部性"原则，频繁访问全局显存，导致严重的访存瓶颈。</p><hr><h2 id=2-flashattention的革命性思想分块计算与在线算法>2. FlashAttention的革命性思想：分块计算与在线算法<a hidden class=anchor aria-hidden=true href=#2-flashattention的革命性思想分块计算与在线算法>#</a></h2><h3 id=21-核心洞察将二维问题转化为一维流式计算>2.1 核心洞察：将二维问题转化为一维流式计算<a hidden class=anchor aria-hidden=true href=#21-核心洞察将二维问题转化为一维流式计算>#</a></h3><p>FlashAttention的核心思想是<strong>避免物化（materialization）整个注意力矩阵</strong>，而是采用分块计算和在线更新的方式。</p><p><strong>关键观察</strong>：Softmax函数具有可分解性，可以通过增量更新的方式计算，无需存储完整的中间矩阵。</p><h3 id=22-分块softmax的数学基础>2.2 分块Softmax的数学基础<a hidden class=anchor aria-hidden=true href=#22-分块softmax的数学基础>#</a></h3><h4 id=221-传统softmax的数值稳定计算>2.2.1 传统Softmax的数值稳定计算<a hidden class=anchor aria-hidden=true href=#221-传统softmax的数值稳定计算>#</a></h4><p>对于向量 $\mathbf{x} = [x_1, x_2, \ldots, x_n]$，数值稳定的softmax计算为：</p>$$
\begin{align}
m &= \max_i x_i \\
\text{softmax}(x_i) &= \frac{e^{x_i - m}}{\sum_{j=1}^n e^{x_j - m}}
\end{align}
$$<h4 id=222-分块softmax的数学推导>2.2.2 分块Softmax的数学推导<a hidden class=anchor aria-hidden=true href=#222-分块softmax的数学推导>#</a></h4><p><strong>问题设定</strong>：假设我们要计算向量 $\mathbf{x}$ 的softmax，但 $\mathbf{x}$ 被分为两块：$\mathbf{x}^{(1)} = [x_1, \ldots, x_k]$ 和 $\mathbf{x}^{(2)} = [x_{k+1}, \ldots, x_n]$。</p><p><strong>第一块的计算</strong>：<br></p>$$
\begin{align}
m^{(1)} &= \max_{1 \leq i \leq k} x_i \\
d^{(1)} &= \sum_{i=1}^k e^{x_i - m^{(1)}} \\
\text{softmax}^{(1)}(x_i) &= \frac{e^{x_i - m^{(1)}}}{d^{(1)}} \quad \text{(临时结果)}
\end{align}
$$<p><strong>第二块的计算</strong>：<br></p>$$
\begin{align}
m^{(2)} &= \max_{k+1 \leq i \leq n} x_i \\
d^{(2)} &= \sum_{i=k+1}^n e^{x_i - m^{(2)}}
\end{align}
$$<p><strong>合并更新</strong>：设全局最大值为 $m^{new} = \max(m^{(1)}, m^{(2)})$，则：</p>$$
\begin{align}
d^{new} &= d^{(1)} \cdot e^{m^{(1)} - m^{new}} + d^{(2)} \cdot e^{m^{(2)} - m^{new}} \\
\text{softmax}(x_i) &= \begin{cases}
\frac{e^{x_i - m^{new}}}{d^{new}} = \frac{e^{x_i - m^{(1)}} \cdot e^{m^{(1)} - m^{new}}}{d^{new}} & \text{if } i \leq k \\
\frac{e^{x_i - m^{new}}}{d^{new}} = \frac{e^{x_i - m^{(2)}} \cdot e^{m^{(2)} - m^{new}}}{d^{new}} & \text{if } i > k
\end{cases}
\end{align}
$$<h4 id=223-在线softmax更新算法>2.2.3 在线Softmax更新算法<a hidden class=anchor aria-hidden=true href=#223-在线softmax更新算法>#</a></h4><p>更一般地，我们可以设计一个在线更新算法：</p><p><strong>算法状态</strong>：维护当前的 $(m, d, \mathbf{o})$，其中：</p><ul><li>$m$：当前见过的最大logit值</li><li>$d$：当前的归一化因子</li><li>$\mathbf{o}$：当前的输出累积</li></ul><p><strong>更新规则</strong>：当处理新的块 $(\mathbf{x}^{new}, \mathbf{v}^{new})$ 时：</p>$$
\begin{align}
m^{new} &= \max(m^{old}, \max(\mathbf{x}^{new})) \\
d^{new} &= d^{old} \cdot e^{m^{old} - m^{new}} + \sum_j e^{x_j^{new} - m^{new}} \\
\mathbf{o}^{new} &= \mathbf{o}^{old} \cdot \frac{d^{old} \cdot e^{m^{old} - m^{new}}}{d^{new}} + \sum_j \frac{e^{x_j^{new} - m^{new}}}{d^{new}} \mathbf{v}_j^{new}
\end{align}
$$<h3 id=23-flashattention的分块策略>2.3 FlashAttention的分块策略<a hidden class=anchor aria-hidden=true href=#23-flashattention的分块策略>#</a></h3><pre class=mermaid>
  graph TB
    subgraph &#34;传统Attention&#34;
        A1[&#34;Q: 2048x64&#34;] --&gt; B1[&#34;QKT: 2048x2048&lt;br/&gt;全量计算&#34;]
        K1[&#34;K: 2048x64&#34;] --&gt; B1
        B1 --&gt; C1[&#34;Softmax: 2048x2048&lt;br/&gt;全量存储&#34;]
        C1 --&gt; D1[&#34;PV: 2048x64&#34;]
        V1[&#34;V: 2048x64&#34;] --&gt; D1
    end
    
    subgraph &#34;FlashAttention分块&#34;
        A2[&#34;Q分块: BrX64&#34;] --&gt; B2[&#34;局部QKT: BrXBc&lt;br/&gt;片上计算&#34;]
        K2[&#34;K分块: BcX64&#34;] --&gt; B2
        B2 --&gt; C2[&#34;在线Softmax更新&lt;br/&gt;无需存储完整矩阵&#34;]
        C2 --&gt; D2[&#34;累积输出&lt;br/&gt;BrX64&#34;]
        V2[&#34;V分块: BcX64&#34;] --&gt; D2
    end
    
    style B1 fill:#ff9999
    style C1 fill:#ff9999
    style B2 fill:#99ff99
    style C2 fill:#99ff99
</pre><p><strong>关键参数</strong>：</p><ul><li>$B_r$：Query块大小（通常为64-128）</li><li>$B_c$：Key/Value块大小（通常为64-128）</li><li>总内存使用：$O(B_r \cdot B_c)$ 而非 $O(n^2)$</li></ul><h3 id=24-数学正确性证明>2.4 数学正确性证明<a hidden class=anchor aria-hidden=true href=#24-数学正确性证明>#</a></h3><p><strong>定理</strong>：FlashAttention的分块计算结果与传统Attention完全等价。</p><p><strong>证明思路</strong>：</p><ol><li><strong>Softmax可分解性</strong>：根据上述在线更新公式，分块计算的softmax等价于全量计算</li><li><strong>线性性保持</strong>：注意力加权求和的线性性在分块过程中保持不变</li><li><strong>数值稳定性</strong>：通过维护全局最大值，避免了数值溢出</li></ol><p><strong>具体验证</strong>：设 $Q \in \mathbb{R}^{n \times d}$，$K, V \in \mathbb{R}^{n \times d}$，传统方法计算：<br></p>$$O_{traditional} = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V$$<p>FlashAttention分块计算：<br></p>$$O_{flash} = \text{OnlineSoftmax}\left(\frac{QK^T}{\sqrt{d}}\right)V$$<p>数学上可证明 $O_{traditional} = O_{flash}$（在数值精度范围内）。</p><hr><h2 id=3-flashattention各版本的演进从概念验证到产业标准>3. FlashAttention各版本的演进：从概念验证到产业标准<a hidden class=anchor aria-hidden=true href=#3-flashattention各版本的演进从概念验证到产业标准>#</a></h2><h3 id=31-flashattention-v1-2022开创性的分块算法>3.1 FlashAttention v1 (2022)：开创性的分块算法<a hidden class=anchor aria-hidden=true href=#31-flashattention-v1-2022开创性的分块算法>#</a></h3><h4 id=311-设计目标>3.1.1 设计目标<a hidden class=anchor aria-hidden=true href=#311-设计目标>#</a></h4><p><strong>核心目标</strong>：证明分块Attention的可行性，解决 $O(n^2)$ 内存瓶颈</p><p><strong>技术挑战</strong>：</p><ol><li>如何在不损失精度的情况下分块计算softmax</li><li>如何设计高效的GPU kernel实现</li><li>如何处理反向传播的梯度计算</li></ol><h4 id=312-关键创新>3.1.2 关键创新<a hidden class=anchor aria-hidden=true href=#312-关键创新>#</a></h4><p><strong>1. 分块大小的理论分析</strong></p><p>对于序列长度 $n$，特征维度 $d$，SRAM大小 $M$：</p><p>最优分块大小：$B_c = \left\lfloor \frac{M}{4d} \right\rfloor$，$B_r = \min\left(B_c, \frac{M}{4d}\right)$</p><p><strong>理论依据</strong>：</p><ul><li>每个块需要存储：$Q$ 块 ($B_r \times d$)、$K$ 块 ($B_c \times d$)、$V$ 块 ($B_c \times d$)、输出块 ($B_r \times d$)</li><li>总内存需求：$4Bd$，必须小于SRAM容量 $M$</li></ul><p><strong>2. 访存复杂度分析</strong></p><pre class=mermaid>
  graph LR
    subgraph &#34;内存访问模式&#34;
        A[&#34;HBM读取&lt;br/&gt;Q,K,V: O(nd)&#34;] --&gt; B[&#34;SRAM计算&lt;br/&gt;局部QK^T: O(B²)&#34;]
        B --&gt; C[&#34;SRAM更新&lt;br/&gt;在线softmax: O(B²)&#34;]
        C --&gt; D[&#34;HBM写回&lt;br/&gt;输出O: O(nd)&#34;]
    end
    
    subgraph &#34;复杂度对比&#34;
        E[&#34;传统方法&lt;br/&gt;HBM: O(n²+nd)&lt;br/&gt;计算: O(n²d)&#34;]
        F[&#34;FlashAttention v1&lt;br/&gt;HBM: O(nd)&lt;br/&gt;计算: O(n²d)&#34;]
    end
    
    style F fill:#99ff99
    style E fill:#ff9999
</pre><p><strong>访存优化效果</strong>：</p><ul><li><strong>传统方法</strong>：$O(n^2)$ HBM访问</li><li><strong>FlashAttention v1</strong>：$O(nd)$ HBM访问</li><li><strong>加速比</strong>：理论上可达 $\frac{n^2}{nd} = \frac{n}{d}$ 倍</li></ul><h4 id=313-算法实现细节>3.1.3 算法实现细节<a hidden class=anchor aria-hidden=true href=#313-算法实现细节>#</a></h4><p><strong>前向传播算法</strong>：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>Input: Q, K, V ∈ ℝⁿˣᵈ, 分块大小 Br, Bc
</span></span><span class=line><span class=cl>Output: O ∈ ℝⁿˣᵈ
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>1. 将 Q 分为 ⌈n/Br⌉ 块，K,V 分为 ⌈n/Bc⌉ 块
</span></span><span class=line><span class=cl>2. 初始化 O = 0 ∈ ℝⁿˣᵈ
</span></span><span class=line><span class=cl>3. for i = 1 to ⌈n/Br⌉ do:
</span></span><span class=line><span class=cl>   a. 加载 Qi ∈ ℝᴮʳˣᵈ 到SRAM
</span></span><span class=line><span class=cl>   b. 初始化 ℓi = 0 ∈ ℝᴮʳ, mi = -∞ ∈ ℝᴮʳ, Oi = 0 ∈ ℝᴮʳˣᵈ
</span></span><span class=line><span class=cl>   c. for j = 1 to ⌈n/Bc⌉ do:
</span></span><span class=line><span class=cl>      i.   加载 Kj, Vj ∈ ℝᴮᶜˣᵈ 到SRAM
</span></span><span class=line><span class=cl>      ii.  计算 Sij = QiKjᵀ ∈ ℝᴮʳˣᴮᶜ
</span></span><span class=line><span class=cl>      iii. 计算 m̃ij = rowmax(Sij) ∈ ℝᴮʳ
</span></span><span class=line><span class=cl>      iv.  计算 P̃ij = exp(Sij - m̃ij) ∈ ℝᴮʳˣᴮᶜ
</span></span><span class=line><span class=cl>      v.   计算 ℓ̃ij = rowsum(P̃ij) ∈ ℝᴮʳ
</span></span><span class=line><span class=cl>      vi.  计算 mi^new = max(mi, m̃ij), ℓi^new = ℓi·exp(mi - mi^new) + ℓ̃ij·exp(m̃ij - mi^new)
</span></span><span class=line><span class=cl>      vii. 更新 Oi = diag(ℓi/ℓi^new)·exp(mi - mi^new)·Oi + diag(exp(m̃ij - mi^new)/ℓi^new)·P̃ij·Vj
</span></span><span class=line><span class=cl>      viii.更新 ℓi = ℓi^new, mi = mi^new
</span></span><span class=line><span class=cl>   d. 将 Oi 写回HBM
</span></span></code></pre></td></tr></table></div></div><p><strong>反向传播算法</strong>：需要重新计算前向过程中的中间值，因为它们没有被存储。</p><h3 id=32-flashattention-v2-2023工程优化与扩展性>3.2 FlashAttention v2 (2023)：工程优化与扩展性<a hidden class=anchor aria-hidden=true href=#32-flashattention-v2-2023工程优化与扩展性>#</a></h3><h4 id=321-设计目标>3.2.1 设计目标<a hidden class=anchor aria-hidden=true href=#321-设计目标>#</a></h4><p><strong>核心目标</strong>：在保持v1算法正确性的基础上，大幅提升实际性能</p><p><strong>优化方向</strong>：</p><ol><li><strong>并行化优化</strong>：更好利用GPU的并行计算能力</li><li><strong>内存访问优化</strong>：减少不必要的内存传输</li><li><strong>支持更多场景</strong>：causal mask、不同head维度等</li></ol><h4 id=322-关键优化>3.2.2 关键优化<a hidden class=anchor aria-hidden=true href=#322-关键优化>#</a></h4><p><strong>1. 分块策略重新设计</strong></p><p>v1的问题：外层循环遍历Query块，内层循环遍历Key/Value块，导致：</p><ul><li>Query块需要重复加载</li><li>并行度不够高</li></ul><p>v2的改进：</p><ul><li><strong>按Key/Value维度分块</strong>：外层循环遍历Key/Value，内层遍历Query</li><li><strong>更好的并行性</strong>：不同的Query块可以并行处理</li></ul><p><strong>2. 工作分配优化</strong></p><pre class=mermaid>
  graph TD
    subgraph &#34;FlashAttention v1&#34;
        A1[&#34;Thread Block 1&lt;br/&gt;处理Q块1&#34;] --&gt; B1[&#34;串行处理所有K,V块&#34;]
        A2[&#34;Thread Block 2&lt;br/&gt;处理Q块2&#34;] --&gt; B2[&#34;串行处理所有K,V块&#34;]
        A3[&#34;Thread Block 3&lt;br/&gt;处理Q块3&#34;] --&gt; B3[&#34;串行处理所有K,V块&#34;]
    end
    
    subgraph &#34;FlashAttention v2&#34;
        C1[&#34;Thread Block 1&lt;br/&gt;处理K块1,V块1&#34;] --&gt; D1[&#34;并行处理所有Q块&#34;]
        C2[&#34;Thread Block 2&lt;br/&gt;处理K块2,V块2&#34;] --&gt; D2[&#34;并行处理所有Q块&#34;]
        C3[&#34;Thread Block 3&lt;br/&gt;处理K块3,V块3&#34;] --&gt; D3[&#34;并行处理所有Q块&#34;]
    end
    
    style A1 fill:#ff9999
    style A2 fill:#ff9999
    style A3 fill:#ff9999
    style C1 fill:#99ff99
    style C2 fill:#99ff99
    style C3 fill:#99ff99
</pre><p><strong>3. 内存访问模式优化</strong></p><ul><li><strong>减少冗余加载</strong>：Key和Value块在多个Query块间共享</li><li><strong>更好的缓存利用</strong>：改进内存访问的空间局部性</li><li><strong>向量化操作</strong>：更充分利用GPU的向量化指令</li></ul><h4 id=323-性能提升分析>3.2.3 性能提升分析<a hidden class=anchor aria-hidden=true href=#323-性能提升分析>#</a></h4><p><strong>理论分析</strong>：</p><ul><li><strong>并行度提升</strong>：从 $O(\lceil n/B_r \rceil)$ 提升到 $O(\lceil n/B_c \rceil \times \lceil n/B_r \rceil)$</li><li><strong>内存访问优化</strong>：减少约20-30%的冗余访问</li><li><strong>计算效率</strong>：更好的指令级并行和向量化</li></ul><p><strong>实际性能</strong>：</p><table><thead><tr><th>序列长度</th><th>Head维度</th><th>v1性能</th><th>v2性能</th><th>提升比例</th></tr></thead><tbody><tr><td>2048</td><td>64</td><td>1.2x</td><td>1.8x</td><td>+50%</td></tr><tr><td>4096</td><td>64</td><td>1.5x</td><td>2.3x</td><td>+53%</td></tr><tr><td>8192</td><td>64</td><td>1.8x</td><td>3.1x</td><td>+72%</td></tr></tbody></table><h3 id=33-flashattention-v3-2024硬件协同设计>3.3 FlashAttention v3 (2024)：硬件协同设计<a hidden class=anchor aria-hidden=true href=#33-flashattention-v3-2024硬件协同设计>#</a></h3><h4 id=331-设计目标>3.3.1 设计目标<a hidden class=anchor aria-hidden=true href=#331-设计目标>#</a></h4><p><strong>核心目标</strong>：充分利用新一代GPU硬件特性，支持更复杂的应用场景</p><p><strong>技术趋势</strong>：</p><ol><li><strong>新硬件特性</strong>：H100的FP8支持、更大的共享内存</li><li><strong>应用需求</strong>：更长序列、混合精度、稀疏attention</li><li><strong>系统集成</strong>：更好的编译器支持、自动调优</li></ol><h4 id=332-核心创新>3.3.2 核心创新<a hidden class=anchor aria-hidden=true href=#332-核心创新>#</a></h4><p><strong>1. 异构精度计算</strong></p><p>支持FP8/FP16/FP32混合精度：</p><ul><li><strong>输入</strong>：FP8存储，减少内存带宽</li><li><strong>计算</strong>：FP16/FP32，保证数值精度</li><li><strong>输出</strong>：根据需求选择精度</li></ul><p><strong>2. 自适应分块策略</strong></p><pre class=mermaid>
  graph TD
    A[输入分析] --&gt; B{序列长度}
    B --&gt;|短序列&lt;1024| C[小块策略&lt;br/&gt;Br=64, Bc=64]
    B --&gt;|中等序列1024-4096| D[中等块策略&lt;br/&gt;Br=128, Bc=128]
    B --&gt;|长序列&gt;4096| E[大块策略&lt;br/&gt;Br=256, Bc=256]
    
    F[硬件检测] --&gt; G{GPU类型}
    G --&gt;|A100| H[优化A100参数]
    G --&gt;|H100| I[优化H100参数]
    G --&gt;|其他| J[通用参数]
    
    C --&gt; K[执行kernel]
    D --&gt; K
    E --&gt; K
    H --&gt; K
    I --&gt; K
    J --&gt; K
</pre><p><strong>3. 编译时优化</strong></p><ul><li><strong>模板特化</strong>：针对常见的head维度生成专门的kernel</li><li><strong>循环展开</strong>：减少分支预测开销</li><li><strong>指令调度</strong>：更好的指令级并行</li></ul><h4 id=333-应用场景扩展>3.3.3 应用场景扩展<a hidden class=anchor aria-hidden=true href=#333-应用场景扩展>#</a></h4><p><strong>1. 长上下文支持</strong></p><ul><li>支持1M+token的超长序列</li><li>分层attention策略</li><li>渐进式精度降低</li></ul><p><strong>2. 稀疏attention模式</strong></p><ul><li>Block-sparse attention</li><li>滑动窗口attention</li><li>局部-全局混合attention</li></ul><p><strong>3. 多模态支持</strong></p><ul><li>文本-图像联合attention</li><li>不同模态的attention权重</li><li>跨模态的梯度优化</li></ul><hr><h2 id=4-flashattention算法流程深度解析>4. FlashAttention算法流程深度解析<a hidden class=anchor aria-hidden=true href=#4-flashattention算法流程深度解析>#</a></h2><h3 id=41-完整算法流程可视化>4.1 完整算法流程可视化<a hidden class=anchor aria-hidden=true href=#41-完整算法流程可视化>#</a></h3><h4 id=411-整体计算流程>4.1.1 整体计算流程<a hidden class=anchor aria-hidden=true href=#411-整体计算流程>#</a></h4><pre class=mermaid>
  flowchart TD
    A[&#34;输入矩阵&lt;br/&gt;Q: nxd, K: nxd, V: nxd&#34;] --&gt; B[&#34;矩阵分块&#34;]
    
    B --&gt; C[&#34;Q分块: ⌈n/Br⌉个块&lt;br/&gt;每块大小BrXd&#34;]
    B --&gt; D[&#34;K,V分块: ⌈n/Bc⌉个块&lt;br/&gt;每块大小BcXd&#34;]
    
    C --&gt; E[外层循环: 遍历Q块]
    D --&gt; F[内层循环: 遍历K,V块]
    
    E --&gt; G[加载Qi到SRAM]
    F --&gt; H[加载Kj,Vj到SRAM]
    
    G --&gt; I[&#34;计算局部注意力得分&lt;br/&gt;Sij = Qi x KjT&#34;]
    H --&gt; I
    
    I --&gt; J[&#34;在线Softmax更新&#34;]
    J --&gt; K[&#34;计算加权输出&lt;br/&gt;Oi += aij x Vj&#34;]
    
    K --&gt; L{所有K,V块&lt;br/&gt;处理完毕?}
    L --&gt;|否| F
    L --&gt;|是| M[写回Oi到HBM]
    
    M --&gt; N{所有Q块&lt;br/&gt;处理完毕?}
    N --&gt;|否| E
    N --&gt;|是| O[输出完整结果O]
    
    style A fill:#e1f5fe
    style G fill:#c8e6c9
    style H fill:#c8e6c9
    style I fill:#fff3e0
    style J fill:#fce4ec
    style K fill:#f3e5f5
    style O fill:#e8f5e8
</pre><h4 id=412-sram内存管理流程>4.1.2 SRAM内存管理流程<a hidden class=anchor aria-hidden=true href=#412-sram内存管理流程>#</a></h4><pre class=mermaid>
  sequenceDiagram
    participant HBM as HBM显存
    participant SRAM as SRAM缓存
    participant Compute as 计算单元
    
    Note over HBM,Compute: 处理第i个Q块，第j个K,V块
    
    HBM-&gt;&gt;SRAM: 1. 加载Qi (BrXd)
    HBM-&gt;&gt;SRAM: 2. 加载Kj (BcXd)
    HBM-&gt;&gt;SRAM: 3. 加载Vj (BcXd)
    
    SRAM-&gt;&gt;Compute: 4. 计算Sij = QiXKjT
    Compute-&gt;&gt;SRAM: 5. 存储局部得分Sij
    
    SRAM-&gt;&gt;Compute: 6. 在线Softmax更新
    Note over Compute: 更新mi, li, Oi
    
    SRAM-&gt;&gt;Compute: 7. 计算PijXVj
    Compute-&gt;&gt;SRAM: 8. 累积到输出Oi
    
    Note over SRAM: 释放Kj, Vj空间
    
    alt 所有K,V块处理完毕
        SRAM-&gt;&gt;HBM: 9. 写回最终Oi
        Note over SRAM: 释放Qi空间
    end
</pre><h3 id=42-核心算法在线softmax更新详解>4.2 核心算法：在线Softmax更新详解<a hidden class=anchor aria-hidden=true href=#42-核心算法在线softmax更新详解>#</a></h3><h4 id=421-状态维护与更新>4.2.1 状态维护与更新<a hidden class=anchor aria-hidden=true href=#421-状态维护与更新>#</a></h4><p><strong>算法状态</strong>：对每个Query块 $Q_i$，维护三元组 $(m_i, \ell_i, O_i)$：</p>$$
\begin{align}
m_i &\in \mathbb{R}^{B_r} \quad \text{(当前最大logit值)} \\
\ell_i &\in \mathbb{R}^{B_r} \quad \text{(当前归一化因子)} \\
O_i &\in \mathbb{R}^{B_r \times d} \quad \text{(当前输出累积)}
\end{align}
$$<h4 id=422-逐步更新过程>4.2.2 逐步更新过程<a hidden class=anchor aria-hidden=true href=#422-逐步更新过程>#</a></h4><pre class=mermaid>
  graph TD
    subgraph &#34;第j步更新前状态&#34;
        A1[&#34;mi⁽ʲ⁻¹⁾: 前j-1块的最大值&#34;]
        A2[&#34;ℓi⁽ʲ⁻¹⁾: 前j-1块的归一化因子&#34;]
        A3[&#34;Oi⁽ʲ⁻¹⁾: 前j-1块的累积输出&#34;]
    end
    
    subgraph &#34;当前块计算&#34;
        B1[&#34;计算Sij = QiXKjT&#34;]
        B2[&#34;计算局部最大值 rowmax(Sij)&#34;]
        B3[&#34;计算局部softmax权重&#34;]
        B4[&#34;计算局部行和 rowsum&#34;]
    end
    
    subgraph &#34;状态更新&#34;
        C1[&#34;更新全局最大值&#34;]
        C2[&#34;更新归一化因子&#34;]
        C3[&#34;更新累积输出&#34;]
    end
    
    A1 --&gt; C1
    A2 --&gt; C2
    A3 --&gt; C3
    B1 --&gt; B2 --&gt; B3 --&gt; B4
    B2 --&gt; C1
    B3 --&gt; C3
    B4 --&gt; C2
    
    style B1 fill:#fff3e0
    style C1 fill:#e8f5e8
    style C2 fill:#e8f5e8
    style C3 fill:#e8f5e8
</pre><p><strong>更新公式详解</strong>：</p><p>设当前处理第 $j$ 个Key/Value块，更新规则为：</p>$$
\begin{align}
m_i^{(j)} &= \max(m_i^{(j-1)}, \tilde{m}_{ij}) \\
\alpha &= \exp(m_i^{(j-1)} - m_i^{(j)}) \\
\beta &= \exp(\tilde{m}_{ij} - m_i^{(j)}) \\
\ell_i^{(j)} &= \ell_i^{(j-1)} \cdot \alpha + \tilde{\ell}_{ij} \cdot \beta \\
O_i^{(j)} &= \frac{\ell_i^{(j-1)}}{\ell_i^{(j)}} \cdot \alpha \cdot O_i^{(j-1)} + \frac{\beta}{\ell_i^{(j)}} \cdot \tilde{P}_{ij} V_j
\end{align}
$$<h3 id=43-具体数值示例逐步计算过程>4.3 具体数值示例：逐步计算过程<a hidden class=anchor aria-hidden=true href=#43-具体数值示例逐步计算过程>#</a></h3><h4 id=431-示例设置>4.3.1 示例设置<a hidden class=anchor aria-hidden=true href=#431-示例设置>#</a></h4><p><strong>输入参数</strong>：</p><ul><li>序列长度：$n = 4$</li><li>特征维度：$d = 2$</li><li>分块大小：$B_r = B_c = 2$</li></ul><p><strong>输入矩阵</strong>：<br></p>$$
Q = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 2 & 1 \\ 1 & 2 \end{bmatrix}, \quad
K = \begin{bmatrix} 1 & 1 \\ 0 & 2 \\ 1 & 0 \\ 2 & 1 \end{bmatrix}, \quad
V = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 2 & 1 \\ 1 & 2 \end{bmatrix}
$$<h4 id=432-分块结果>4.3.2 分块结果<a hidden class=anchor aria-hidden=true href=#432-分块结果>#</a></h4>$$
Q_1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \quad
Q_2 = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}
$$$$
K_1 = \begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix}, \quad
K_2 = \begin{bmatrix} 1 & 0 \\ 2 & 1 \end{bmatrix}
$$$$
V_1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \quad
V_2 = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}
$$<h4 id=433-处理q1块的详细步骤>4.3.3 处理Q1块的详细步骤<a hidden class=anchor aria-hidden=true href=#433-处理q1块的详细步骤>#</a></h4><p><strong>步骤1：处理K1,V1块</strong></p><ol><li><p>计算注意力得分：<br></p>$$S_{11} = Q_1 K_1^T = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 1 & 2 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 1 & 2 \end{bmatrix}$$</li><li><p>计算行最大值：<br></p>$$\tilde{m}_{11} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$$</li><li><p>计算softmax权重：<br></p>$$\tilde{P}_{11} = \begin{bmatrix} e^{1-1} & e^{0-1} \\ e^{1-2} & e^{2-2} \end{bmatrix} = \begin{bmatrix} 1 & e^{-1} \\ e^{-1} & 1 \end{bmatrix}$$</li><li><p>计算行和：<br></p>$$\tilde{\ell}_{11} = \begin{bmatrix} 1 + e^{-1} \\ e^{-1} + 1 \end{bmatrix} \approx \begin{bmatrix} 1.368 \\ 1.368 \end{bmatrix}$$</li><li><p>初始化状态：<br></p>$$m_1^{(1)} = \tilde{m}_{11} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad
\ell_1^{(1)} = \tilde{\ell}_{11} \approx \begin{bmatrix} 1.368 \\ 1.368 \end{bmatrix}$$</li><li><p>计算输出：<br></p>$$O_1^{(1)} = \frac{\tilde{P}_{11}}{\ell_1^{(1)}} V_1 \approx \begin{bmatrix} 0.731 & 0.269 \\ 0.269 & 0.731 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.731 & 0.269 \\ 0.269 & 0.731 \end{bmatrix}$$</li></ol><p><strong>步骤2：处理K2,V2块</strong></p><ol><li><p>计算注意力得分：<br></p>$$S_{12} = Q_1 K_2^T = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}$$</li><li><p>计算行最大值：<br></p>$$\tilde{m}_{12} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}$$</li><li><p>更新全局最大值：<br></p>$$m_1^{(2)} = \max(m_1^{(1)}, \tilde{m}_{12}) = \max\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 2 \\ 1 \end{bmatrix}\right) = \begin{bmatrix} 2 \\ 2 \end{bmatrix}$$</li><li><p>计算修正因子：<br></p>$$\alpha = \exp(m_1^{(1)} - m_1^{(2)}) = \exp\left(\begin{bmatrix} 1-2 \\ 2-2 \end{bmatrix}\right) = \begin{bmatrix} e^{-1} \\ 1 \end{bmatrix}$$</li></ol>$$\beta = \exp(\tilde{m}_{12} - m_1^{(2)}) = \exp\left(\begin{bmatrix} 2-2 \\ 1-2 \end{bmatrix}\right) = \begin{bmatrix} 1 \\ e^{-1} \end{bmatrix}$$<ol start=5><li>更新归一化因子和输出&mldr;</li></ol><h3 id=44-算法复杂度分析>4.4 算法复杂度分析<a hidden class=anchor aria-hidden=true href=#44-算法复杂度分析>#</a></h3><h4 id=441-时间复杂度>4.4.1 时间复杂度<a hidden class=anchor aria-hidden=true href=#441-时间复杂度>#</a></h4><pre class=mermaid>
  graph LR
    subgraph &#34;计算复杂度分析&#34;
        A[&#34;外层循环&lt;br/&gt;⌈n/Br⌉次&#34;] --&gt; B[&#34;内层循环&lt;br/&gt;⌈n/Bc⌉次&#34;]
        B --&gt; C[&#34;矩阵乘法&lt;br/&gt;O(BrXBcXd)&#34;]
        C --&gt; D[&#34;Softmax计算&lt;br/&gt;O(BrXBc)&#34;]
        D --&gt; E[&#34;输出更新&lt;br/&gt;O(BrXBcXd)&#34;]
    end
    
    subgraph &#34;总复杂度&#34;
        F[&#34;总时间复杂度&lt;br/&gt;O(n²d)&#34;]
        G[&#34;与传统方法相同&lt;br/&gt;但常数项更小&#34;]
    end
    
    A --&gt; F
    style F fill:#e8f5e8
    style G fill:#fff3e0
</pre><p><strong>详细分析</strong>：</p><ul><li><strong>外层循环</strong>：$\lceil n/B_r \rceil$ 次</li><li><strong>内层循环</strong>：$\lceil n/B_c \rceil$ 次</li><li><strong>每次迭代</strong>：$O(B_r B_c d + B_r B_c + B_r B_c d) = O(B_r B_c d)$</li><li><strong>总时间复杂度</strong>：$O(\frac{n}{B_r} \cdot \frac{n}{B_c} \cdot B_r B_c d) = O(n^2 d)$</li></ul><h4 id=442-空间复杂度>4.4.2 空间复杂度<a hidden class=anchor aria-hidden=true href=#442-空间复杂度>#</a></h4><p><strong>SRAM使用分析</strong>：</p><ul><li><strong>Query块</strong>：$B_r \times d$</li><li><strong>Key块</strong>：$B_c \times d$</li><li><strong>Value块</strong>：$B_c \times d$</li><li><strong>中间结果</strong>：$B_r \times B_c$（注意力得分）</li><li><strong>状态向量</strong>：$B_r$（最大值）+ $B_r$（归一化因子）</li><li><strong>输出块</strong>：$B_r \times d$</li></ul><p><strong>总SRAM需求</strong>：$O(B_r d + B_c d + B_r B_c) = O((B_r + B_c)d + B_r B_c)$</p><p><strong>HBM使用</strong>：仅需存储输入输出，$O(nd)$，相比传统方法的 $O(n^2 + nd)$ 大幅减少。</p><hr><h2 id=5-行业实践与生态发展>5. 行业实践与生态发展<a hidden class=anchor aria-hidden=true href=#5-行业实践与生态发展>#</a></h2><h3 id=51-主流框架集成现状>5.1 主流框架集成现状<a hidden class=anchor aria-hidden=true href=#51-主流框架集成现状>#</a></h3><h4 id=511-深度学习框架支持>5.1.1 深度学习框架支持<a hidden class=anchor aria-hidden=true href=#511-深度学习框架支持>#</a></h4><pre class=mermaid>
  graph TB
    subgraph &#34;主流框架集成&#34;
        A[PyTorch] --&gt; A1[torch.nn.functional.scaled_dot_product_attention&lt;br/&gt;原生支持FlashAttention v2]
        A --&gt; A2[xFormers库&lt;br/&gt;完整FlashAttention实现]
        
        B[TensorFlow] --&gt; B1[TensorFlow Addons&lt;br/&gt;社区FlashAttention实现]
        B --&gt; B2[JAX/Flax&lt;br/&gt;研究社区广泛使用]
        
        C[HuggingFace] --&gt; C1[Transformers库&lt;br/&gt;默认启用FlashAttention]
        C --&gt; C2[Accelerate库&lt;br/&gt;自动优化选择]
    end
    
    subgraph &#34;推理框架&#34;
        D[vLLM] --&gt; D1[生产级推理&lt;br/&gt;FlashAttention v2集成]
        E[TensorRT-LLM] --&gt; E1[NVIDIA优化&lt;br/&gt;硬件特定优化]
        F[Text Generation Inference] --&gt; F1[HuggingFace推理&lt;br/&gt;自动FlashAttention]
    end
    
    style A1 fill:#c8e6c9
    style C1 fill:#c8e6c9
    style D1 fill:#c8e6c9
</pre><h4 id=512-性能基准测试>5.1.2 性能基准测试<a hidden class=anchor aria-hidden=true href=#512-性能基准测试>#</a></h4><p><strong>训练性能提升</strong>（相对于标准Attention）：</p><table><thead><tr><th>模型规模</th><th>序列长度</th><th>传统Attention</th><th>FlashAttention v2</th><th>内存节省</th><th>速度提升</th></tr></thead><tbody><tr><td>125M</td><td>2048</td><td>基线</td><td>1.8x</td><td>60%</td><td>80%</td></tr><tr><td>1.3B</td><td>2048</td><td>基线</td><td>2.1x</td><td>65%</td><td>110%</td></tr><tr><td>6.7B</td><td>4096</td><td>OOM</td><td>可运行</td><td>70%</td><td>N/A</td></tr><tr><td>13B</td><td>8192</td><td>OOM</td><td>可运行</td><td>75%</td><td>N/A</td></tr></tbody></table><p><strong>推理性能提升</strong>：</p><pre class=mermaid>
  graph LR
    subgraph &#34;推理延迟对比 (ms)&#34;
        A[序列长度: 1024&lt;br/&gt;标准: 45ms&lt;br/&gt;Flash: 28ms&lt;br/&gt;提升: 38%]
        B[序列长度: 2048&lt;br/&gt;标准: 180ms&lt;br/&gt;Flash: 89ms&lt;br/&gt;提升: 51%]
        C[序列长度: 4096&lt;br/&gt;标准: 720ms&lt;br/&gt;Flash: 298ms&lt;br/&gt;提升: 59%]
        D[序列长度: 8192&lt;br/&gt;标准: OOM&lt;br/&gt;Flash: 1100ms&lt;br/&gt;提升: 可运行]
    end
    
    style A fill:#e8f5e8
    style B fill:#c8e6c9
    style C fill:#a5d6a7
    style D fill:#81c784
</pre><h3 id=52-产业应用案例>5.2 产业应用案例<a hidden class=anchor aria-hidden=true href=#52-产业应用案例>#</a></h3><h4 id=521-大型语言模型训练>5.2.1 大型语言模型训练<a hidden class=anchor aria-hidden=true href=#521-大型语言模型训练>#</a></h4><p><strong>OpenAI GPT系列</strong>：</p><ul><li>GPT-3.5/4训练中广泛使用FlashAttention优化</li><li>支持更长的上下文窗口（32k tokens）</li><li>训练成本降低约30-40%</li></ul><p><strong>Meta Llama系列</strong>：</p><ul><li>Llama 2/3全面采用FlashAttention v2</li><li>支持70B参数模型的高效训练</li><li>Code Llama支持16k代码上下文</li></ul><p><strong>Google PaLM/Gemini</strong>：</p><ul><li>内部优化版本的FlashAttention</li><li>针对TPU硬件的特定优化</li><li>支持多模态长序列处理</li></ul><h3 id=53-技术生态与工具链>5.3 技术生态与工具链<a hidden class=anchor aria-hidden=true href=#53-技术生态与工具链>#</a></h3><h4 id=531-开发工具支持>5.3.1 开发工具支持<a hidden class=anchor aria-hidden=true href=#531-开发工具支持>#</a></h4><p><strong>性能分析工具</strong>：</p><ul><li><strong>NVIDIA Nsight</strong>：支持FlashAttention kernel分析</li><li><strong>PyTorch Profiler</strong>：可视化FlashAttention性能特征</li></ul><p><strong>自动优化工具</strong>：</p><ul><li><strong>DeepSpeed</strong>：自动选择最优的Attention实现</li><li><strong>FairScale</strong>：大规模训练中的FlashAttention集成</li><li><strong>Composer</strong>：训练配方中的自动FlashAttention启用</li></ul><h4 id=532-社区贡献与扩展>5.3.2 社区贡献与扩展<a hidden class=anchor aria-hidden=true href=#532-社区贡献与扩展>#</a></h4><p><strong>开源实现</strong>：</p><ul><li><strong>flash-attn</strong>：官方Python包，支持多种GPU</li><li><strong>xFormers</strong>：Meta开源的高效Transformer组件</li><li><strong>FlashInfer</strong>：专门用于推理的FlashAttention实现</li></ul><p><strong>研究扩展</strong>：</p><ul><li><strong>FlashAttention-2</strong>：进一步的工程优化</li><li><strong>PagedAttention</strong>：结合分页机制的变体</li><li><strong>StreamingLLM</strong>：流式处理的FlashAttention变体</li></ul><h3 id=54-未来发展趋势>5.4 未来发展趋势<a hidden class=anchor aria-hidden=true href=#54-未来发展趋势>#</a></h3><h4 id=541-硬件协同演进>5.4.1 硬件协同演进<a hidden class=anchor aria-hidden=true href=#541-硬件协同演进>#</a></h4><p><strong>下一代GPU优化</strong>：</p><pre class=mermaid>
  timeline
    title FlashAttention硬件协同发展路线图
    
    2022 : FlashAttention v1
         : A100/V100支持
         : FP16/BF16精度
    
    2023 : FlashAttention v2/v3
         : H100优化
         : FP8支持
         : 更大shared memory
    
    2024 : 硬件特定优化
         : Grace Hopper架构
         : 统一内存访问
         : AI专用指令集
    
    2025+ : 未来趋势
          : 内存计算融合
</pre><blockquote><p><strong>核心要点</strong>：FlashAttention的成功不是偶然的，它是数学理论、工程实践和系统思维完美结合的结果。理解其本质，不仅能帮助我们更好地使用这一技术，更能启发我们在面对其他复杂系统问题时的思维方式。</p></blockquote><hr><p><strong>致谢</strong>：本文的完成得益于FlashAttention原作者Tri Dao等人的开创性工作，以及整个AI社区在理论研究和工程实践方面的持续贡献。</p><hr><p><em>如需获取更多技术细节、实现代码或性能基准测试数据，欢迎进一步交流讨论。</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://pillumina.github.io/tags/flashattention/>Flashattention</a></li></ul><nav class=paginav><a class=prev href=https://pillumina.github.io/posts/aiinfra/12-verl-sglang-memory/><span class=title>« Prev</span><br><span>[VeRL,SGLang] RL训推显存管理优化</span>
</a><a class=next href=https://pillumina.github.io/posts/aiinfra/10-verl-dataproto/><span class=title>Next »</span><br><span>[VeRL] DataProto介绍</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [AIInfra] FlashAttention 深度解析：从数学原理到工程实现 on x" href="https://x.com/intent/tweet/?text=%5bAIInfra%5d%20FlashAttention%20%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%ef%bc%9a%e4%bb%8e%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86%e5%88%b0%e5%b7%a5%e7%a8%8b%e5%ae%9e%e7%8e%b0&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f11-flashattention%2f&amp;hashtags=flashattention"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [AIInfra] FlashAttention 深度解析：从数学原理到工程实现 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f11-flashattention%2f&amp;title=%5bAIInfra%5d%20FlashAttention%20%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%ef%bc%9a%e4%bb%8e%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86%e5%88%b0%e5%b7%a5%e7%a8%8b%e5%ae%9e%e7%8e%b0&amp;summary=%5bAIInfra%5d%20FlashAttention%20%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%ef%bc%9a%e4%bb%8e%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86%e5%88%b0%e5%b7%a5%e7%a8%8b%e5%ae%9e%e7%8e%b0&amp;source=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f11-flashattention%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [AIInfra] FlashAttention 深度解析：从数学原理到工程实现 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f11-flashattention%2f&title=%5bAIInfra%5d%20FlashAttention%20%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%ef%bc%9a%e4%bb%8e%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86%e5%88%b0%e5%b7%a5%e7%a8%8b%e5%ae%9e%e7%8e%b0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [AIInfra] FlashAttention 深度解析：从数学原理到工程实现 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f11-flashattention%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [AIInfra] FlashAttention 深度解析：从数学原理到工程实现 on whatsapp" href="https://api.whatsapp.com/send?text=%5bAIInfra%5d%20FlashAttention%20%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%ef%bc%9a%e4%bb%8e%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86%e5%88%b0%e5%b7%a5%e7%a8%8b%e5%ae%9e%e7%8e%b0%20-%20https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f11-flashattention%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [AIInfra] FlashAttention 深度解析：从数学原理到工程实现 on telegram" href="https://telegram.me/share/url?text=%5bAIInfra%5d%20FlashAttention%20%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%ef%bc%9a%e4%bb%8e%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86%e5%88%b0%e5%b7%a5%e7%a8%8b%e5%ae%9e%e7%8e%b0&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f11-flashattention%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [AIInfra] FlashAttention 深度解析：从数学原理到工程实现 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5bAIInfra%5d%20FlashAttention%20%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%ef%bc%9a%e4%bb%8e%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86%e5%88%b0%e5%b7%a5%e7%a8%8b%e5%ae%9e%e7%8e%b0&u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f11-flashattention%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul><div class=related-posts><div class=related-series><h3>同系列文章</h3><ul><li><a href=/posts/aiinfra/14-deterministic-rl/>[Deterministic RL] 确定性问题的来源 & Reproducible RL</a>
<span class=meta>2025-11-20
· 6 min read</span></li></ul></div><div class=related-tags><h3>相关文章</h3><ul></ul></div></div></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://pillumina.github.io/>CctoctoFX</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div class=reading-progress-bar></div><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelector(".reading-progress-bar");if(!t)return;const n=document.querySelector(".post-single");if(!n)return;function s(){const e=n.getBoundingClientRect(),s=e.height,o=window.innerHeight,i=window.scrollY||window.pageYOffset,a=i/(s-o)*100;t.style.width=`${Math.min(100,Math.max(0,a))}%`}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){s(),e=!1}),e=!0)}),s()}),document.addEventListener("DOMContentLoaded",function(){mediumZoom("article img:not(.nozoom)",{margin:24,background:"var(--theme)",scrollOffset:0})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>