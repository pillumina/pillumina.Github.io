<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[Deterministic RL] deterministic问题的来源和相关工作总结 | CctoctoFX</title><meta name=keywords content="deterministic,RL"><meta name=description content="理解LLM推理中deterministic问题来源
Wiki上对deterministic算法的定义是:

“a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output.”
而我们在文中要讨论的，即对于LLM这个context下的deterministic问题，我会先从inference角度（即重复给定一个确定的input，模型的推理为什么无法给定确定的输出）进行问题的理解，再进一步讨论RL工程中的training & inference之间差异，可能会导致RL训练的崩溃问题，并继续讨论业界现在已有的解决方案、与还在working-in-progress的工作。
浮点数的非结合性
thinking machines lab针对batch invariant讨论的文章，详细地解释了在LLM推理中不确定性的来原，即因为精度有限，GPU浮点数运算中的结合性通常不成立：
$$(a+b)+c \neq a+(b+c) $$
这篇arxiv文章，则更深入得说明了这个问题：

Floating-point arithmetic in GPUs exhibits non-associativity, meaning (a+b)+c≠a+(b+c)(a+b)+c=a+(b+c) due to finite precision and rounding errors. This property directly impacts the computation of attention scores and logits in the transformer architecture, where parallel operations across multiple threads can yield different results based on execution order."><meta name=author content="Me"><link rel=canonical href=https://pillumina.github.io/posts/aiinfra/14-deterministic-rl/><link crossorigin=anonymous href=/assets/css/stylesheet.9d388901283682bb45dd422fcaa0d0a2054a3c8ff47c9cc6b2baab15508b1b90.css integrity="sha256-nTiJASg2grtF3UIvyqDQogVKPI/0fJzGsrqrFVCLG5A=" rel="preload stylesheet" as=style><link rel=icon href=https://pillumina.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://pillumina.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://pillumina.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://pillumina.github.io/apple-touch-icon.png><link rel=mask-icon href=https://pillumina.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://pillumina.github.io/posts/aiinfra/14-deterministic-rl/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>(function(){function t(){return document.querySelector(".post-content")||document.querySelector(".post-single")||document.body}function n(e){return/\$\$[\s\S]+?\$\$|\\\(|\\\)|\\\[|\\\]/.test(e)}function s(e){if(window.__mathjaxLoaded)return;window.__mathjaxLoaded=!0,window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code","tt"],ignoreHtmlClass:"no-math"}};var t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js",t.defer=!0,t.onload=function(){window.MathJax&&window.MathJax.typesetPromise&&window.MathJax.typesetPromise([e]).catch(function(e){console.warn("MathJax typeset error",e)})},document.head.appendChild(t)}function e(){try{if(typeof renderMathInElement=="function"){const e=t();renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,strict:!1,trust:!0,ignoredTags:["script","noscript","style","textarea","pre","code","tt"],ignoredClasses:["no-math"],macros:{"\\boldsymbol":"\\mathbf{#1}","\\bm":"\\mathbf{#1}"}}),setTimeout(function(){n(e.innerHTML)&&s(e)},200)}}catch(e){console.warn("KaTeX render error:",e)}}document.addEventListener("DOMContentLoaded",function(){e(),setTimeout(e,200)}),window.addEventListener("load",function(){setTimeout(e,0)})})()</script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>const config={startOnLoad:!0,theme:"neutral",themeVariables:{lineColor:"#0f0f0f"},flowchart:{useMaxWidth:!1,htmlLabels:!0}};mermaid.initialize(config),window.onload=()=>{window.mermaid.init(0[0],document.querySelectorAll(".language-mermaid"))}</script><link rel=stylesheet href=/css/custom.min.bda7229c4269a242639e058fb11a4782f02f8d77071ba16609befee67cc41c49.css integrity="sha256-vacinEJpokJjngWPsRpHgvAvjXcHG6FmCb7+5nzEHEk="><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]"),n=document.querySelectorAll(".toc a");if(t.length===0||n.length===0)return;const s={};t.forEach(e=>{s[e.id]=e.offsetTop});function i(){const t=window.scrollY+100;let e="";for(const[n,o]of Object.entries(s))if(t>=o)e=n;else break;return e}function o(){const e=i();if(n.forEach(e=>{e.classList.remove("active")}),e){const t=document.querySelector(`.toc a[href="#${e}"]`);t&&t.classList.add("active")}}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){o(),e=!1}),e=!0)}),o()})</script><meta property="og:url" content="https://pillumina.github.io/posts/aiinfra/14-deterministic-rl/"><meta property="og:site_name" content="CctoctoFX"><meta property="og:title" content="[Deterministic RL] deterministic问题的来源和相关工作总结"><meta property="og:description" content="理解LLM推理中deterministic问题来源 Wiki上对deterministic算法的定义是:
“a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output.”
而我们在文中要讨论的，即对于LLM这个context下的deterministic问题，我会先从inference角度（即重复给定一个确定的input，模型的推理为什么无法给定确定的输出）进行问题的理解，再进一步讨论RL工程中的training & inference之间差异，可能会导致RL训练的崩溃问题，并继续讨论业界现在已有的解决方案、与还在working-in-progress的工作。
浮点数的非结合性 thinking machines lab针对batch invariant讨论的文章，详细地解释了在LLM推理中不确定性的来原，即因为精度有限，GPU浮点数运算中的结合性通常不成立：
$$(a+b)+c \neq a+(b+c) $$
这篇arxiv文章，则更深入得说明了这个问题：
Floating-point arithmetic in GPUs exhibits non-associativity, meaning (a+b)+c≠a+(b+c)(a+b)+c=a+(b+c) due to finite precision and rounding errors. This property directly impacts the computation of attention scores and logits in the transformer architecture, where parallel operations across multiple threads can yield different results based on execution order."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-20T11:30:12+08:00"><meta property="article:modified_time" content="2025-11-20T11:30:12+08:00"><meta property="article:tag" content="Deterministic"><meta property="article:tag" content="RL"><meta property="og:image" content="https://pillumina.github.io/imgs/icon_head.png"><meta property="og:see_also" content="https://pillumina.github.io/posts/aiinfra/11-flashattention/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://pillumina.github.io/imgs/icon_head.png"><meta name=twitter:title content="[Deterministic RL] deterministic问题的来源和相关工作总结"><meta name=twitter:description content="理解LLM推理中deterministic问题来源
Wiki上对deterministic算法的定义是:

“a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output.”
而我们在文中要讨论的，即对于LLM这个context下的deterministic问题，我会先从inference角度（即重复给定一个确定的input，模型的推理为什么无法给定确定的输出）进行问题的理解，再进一步讨论RL工程中的training & inference之间差异，可能会导致RL训练的崩溃问题，并继续讨论业界现在已有的解决方案、与还在working-in-progress的工作。
浮点数的非结合性
thinking machines lab针对batch invariant讨论的文章，详细地解释了在LLM推理中不确定性的来原，即因为精度有限，GPU浮点数运算中的结合性通常不成立：
$$(a+b)+c \neq a+(b+c) $$
这篇arxiv文章，则更深入得说明了这个问题：

Floating-point arithmetic in GPUs exhibits non-associativity, meaning (a+b)+c≠a+(b+c)(a+b)+c=a+(b+c) due to finite precision and rounding errors. This property directly impacts the computation of attention scores and logits in the transformer architecture, where parallel operations across multiple threads can yield different results based on execution order."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://pillumina.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI Infra","item":"https://pillumina.github.io/posts/aiinfra/"},{"@type":"ListItem","position":3,"name":"[Deterministic RL] deterministic问题的来源和相关工作总结","item":"https://pillumina.github.io/posts/aiinfra/14-deterministic-rl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[Deterministic RL] deterministic问题的来源和相关工作总结","name":"[Deterministic RL] deterministic问题的来源和相关工作总结","description":"理解LLM推理中deterministic问题来源 Wiki上对deterministic算法的定义是:\n“a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output.”\n而我们在文中要讨论的，即对于LLM这个context下的deterministic问题，我会先从inference角度（即重复给定一个确定的input，模型的推理为什么无法给定确定的输出）进行问题的理解，再进一步讨论RL工程中的training \u0026amp; inference之间差异，可能会导致RL训练的崩溃问题，并继续讨论业界现在已有的解决方案、与还在working-in-progress的工作。\n浮点数的非结合性 thinking machines lab针对batch invariant讨论的文章，详细地解释了在LLM推理中不确定性的来原，即因为精度有限，GPU浮点数运算中的结合性通常不成立：\n$$(a+b)+c \\neq a+(b+c) $$\n这篇arxiv文章，则更深入得说明了这个问题：\nFloating-point arithmetic in GPUs exhibits non-associativity, meaning (a+b)+c≠a+(b+c)(a+b)+c=a+(b+c) due to finite precision and rounding errors. This property directly impacts the computation of attention scores and logits in the transformer architecture, where parallel operations across multiple threads can yield different results based on execution order.\n","keywords":["deterministic","RL"],"articleBody":"理解LLM推理中deterministic问题来源 Wiki上对deterministic算法的定义是:\n“a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output.”\n而我们在文中要讨论的，即对于LLM这个context下的deterministic问题，我会先从inference角度（即重复给定一个确定的input，模型的推理为什么无法给定确定的输出）进行问题的理解，再进一步讨论RL工程中的training \u0026 inference之间差异，可能会导致RL训练的崩溃问题，并继续讨论业界现在已有的解决方案、与还在working-in-progress的工作。\n浮点数的非结合性 thinking machines lab针对batch invariant讨论的文章，详细地解释了在LLM推理中不确定性的来原，即因为精度有限，GPU浮点数运算中的结合性通常不成立：\n$$(a+b)+c \\neq a+(b+c) $$\n这篇arxiv文章，则更深入得说明了这个问题：\nFloating-point arithmetic in GPUs exhibits non-associativity, meaning (a+b)+c≠a+(b+c)(a+b)+c=a+(b+c) due to finite precision and rounding errors. This property directly impacts the computation of attention scores and logits in the transformer architecture, where parallel operations across multiple threads can yield different results based on execution order.\n浮点数通常可用科学计数的表示来表征大/小数，例如格式$mantissa *10^{exponent}$，如果指数项是不同的，也就是文中说的add at different scales，那不同累加序导致的精度损失会更加明显，而这种不同scale的累加是最常见的场景。\n但是尽管这是不一致输出的根本原因，但是并没有回答不确定性源自何处。无法帮助我们去理解：浮点数值为何会以不同的顺序相加、这种情况何时会发生，已经如何避免这种情况。\n为何计算内核不同序add numbers？ 一个常见的假说是“并发执行随机性 + 浮点运算误差”。这个假说的核心观点，就是如果并发线程的结束顺序是非确定的，并且数值累加顺序如果依赖于并发线程的结束顺序（例如使用atomic add操作），那么最终数值累加的顺序也是非确定的。\n什么时候真正需要atomic add？ 但是问题是，LLM前向的GPU内核实际上很少用atomic add操作。\n简单解释下Atomic Add的含义：GPU 会把同一段程序同时扔到很多“小核”（SM）上去跑。这些小核之间天生没有步调一致的机制，谁快谁慢完全看当时心情。于是，如果它们需要把结果写到同一个地方，就会出问题。那atomic add就是，硬件保证所有人的结果最终都会加进去，但谁先谁后、按什么顺序加，完全不保证，因此每次跑出来的累加顺序都可能不一样。\n再举个例子，通过torch.sum()对100个数求和，GPU 可以让 100 个小核各读一个数，这一步完全并行。可最后总得把 100 个数合并成 1 个总和。若用原子加，就是让每个小核随便谁先到，就先把它的数塞到同一个累加器里。硬件只负责“不会丢数”，却不负责“按固定顺序加”。于是同样跑两遍，先加谁后加谁可能不同，结果也就可能出现那一点点浮点误差。\n我们回想通常定义的不确定性的含义：同一段 kernel、同一批输入，跑两遍却得到两个略有差异的结果。这叫“run-to-run 非确定性”——哪怕 Python 脚本、依赖库、硬件都没变，第二次跑就是能给你不一样的数。虽然atomic add会导致这个问题，但是更通常的情况是，LLM一次典型的forward，通常一个atomic add也没有。\n这主要有两个原因：\nbatch维度上实际上已经“人多势众”，根本不需要在reduction维度再去并行。 大多数neural networking library也用了很多技巧来实现“既保障确定性又快”，例如“分段树形归约”（split/tree reduction），可以先把100个数拆成5组，每组20个数，5个小核并行计算一个局部和。剩下 5 个局部和要么交给一个核顺序“扫尾”（元素少，开销可忽略），要么用信号量（semaphore）按固定顺序让不同线程块依次累加，从而保证先后顺序一致，结果也就 deterministic 了。 不过，在pytorch中的scatter_add操作（a[b] += c），如果不用atomic add性能会特别慢，而LLM中唯一踩这个坑的，是FlashAttention的反向传播。\n不过当前Triton版本FA反向实现，其实和Tri Dao原论文里的算法不完全一样，Triton版为了躲开atomic add，额外多算了一遍中间结果–FLOPS直接多了40%，但是换来了determinstic，也算明码标价。\n但是正向传播里，LLM中根本就没有非得用atomic add的算子，所以结论就是：LLM 的前向推理，跑两次、跑一百次，结果比特级完全一致；真正可能“每次不一样”的，只出在反向训练阶段，而且基本就 FlashAttention 一家。（也就是前向是“run-to-run deterministic”的）。\n系统级别批次不变性的缺失（batch invariant） 前向kernel函数的确定性，实际上不等于整个推理服务对外表现确定，也就是还存在额外的系统级非确定性。因为真正喂给前向的张量内容还可能被其他“外部输入”左右。\n举个经典里batch norm的坑：早期 BatchNorm 把“整批统计量”（均值/方差）当常量参与计算。\n同一句话，单条跑 μ₁ σ₁，跟 32 条一起跑 μ₃₂ σ₃₂，算出来的隐藏值就不一样，于是最终 token 概率也不同。\n站在“单条请求”视角：它完全无法预知今晚会不会有 31 个请求来搭伙，所以即使自己 prompt 固定，输出依旧“看运气”——这就是上面所说的系统级非确定性。\nLLM 推理里虽然早就把 BatchNorm 踢了出去，却仍缺“批次不变性”（batch invariance）：同一请求、同一模型权重，只要推理时动态批大小不同，可能会导致tilesize不同，导致reduce的计算结果不同。例如，vLLM在不同规模的batch下，把prompt送往不同的batch，而GPU的并行调度器SIMT会把矩阵送往不同的sm、warp，这样计算路径就每次都不一样。\n所以针对推理引擎，比如要在kernel层面实现batch invariant才能解决serve层面不确定性的问题。\n和并行策略相关的Reduction不确定性 TODO: 通信库的通信算子带来的规约不确定性。\nexport HCCL_DETERMINISTIC=true 开启HCCL的规约类算子的确定性计算。\nBatch Invariant的相关工作 batch invariant ops Thinking Machines Lab发布了batch invariant的部分kernel算子实现。\n而从原blog里，提出了三种难度递增的实现。\nbatch invariant的RMSNorm 直接让每个Batch元素的reduction顺序固定，不受batch大小影响。\nbatch大时，把单个batch元素分配给单个核心，reduction运算在单核心内完成，batch增大时让核心依次处理多个元素，保持reduction策略不变。 batch小时，若采用\"split reduction\"（多核心分担reduction以提升并行度）会破坏batch invariant，可以选择忽略小batch优化（小batch本身执行就快，性能损失可以接受），或者采用固定reduction策略（牺牲部分性能来保证batch invariant）。 batch invariant的矩阵乘法 将输出张量拆分为2D tiles，每个tile分配给单个核心，reduction在单个核心内部完成。编译固定配置的内核以适配所有形状，虽然会损失20%性能（和cuBLAS相比），但在LLM推理中通常可以接受，因为模型维度（N）比较大，对split-k的需求较低。\nbatch invariant的注意力计算 采用data-parallel策略（沿着Q张量并行，reduction在单核心内完成），更新KV缓存和页表以保证KV布局一致，不受处理token数量的影响。\ndecode阶段Q长度小，需要拆分KV维度（Split-KV），采用固定拆分大小策略（而非固定拆分数量），确保reduction顺序不变，比如把1000长度的KV拆成3个256长度和1个232长度的片段，而不是4个250长度的片段。\nSglang / vLLM 实现deterministic inference SGLang团队的博客里记录了实现的细节，主要是针对batch invariantkernel上，针对chunked prefill、cuda graph等特性做了兼容，具体可以参考RoadMap。\nvLLM参考Enabling Batch Invariant文档，也可以参考RFC #28326，#27433。\nOn-policy RL训练中的训推不一致问题 当讨论on-policy RL训练的时候，有研究指出 train / inference engine之间的不一致也会隐形导致on-policy假设的RL实际变成off-policy。所以当我们追求\"真正的\" on-policy RL训练时，需要知道：如果不能从两个完全一致的推理请求中获取bitwise相等的结果，那么当然也无法保障训推之间的bitwise一致性。所以基于之前我们对确定性推理实现讨论，直觉上可以知道如果保证了确定性推理，那么通过修改训练这部分stack，也能够实现在bitwise上训推的一致性，从而实现真正的on-policy RL训练。\n而业界对这个问题的解决思路上主要分为两种：\n在训练引擎侧，基于推理引擎(vllm/sglang)确定性推理内核前向实现，进行反向传递的实现，通过对齐kernel的实现，做到训练和采样部分的bitwise一致性（i.e. 0 KL divergence）。\n拥抱训推分布的不一致（考虑到训练bitwise实现在工程上的工作量，和不同模型适配的工作量），在算法上为off-policy做off-policy correction，进行训推KL散度的偏差抑制，在大多数场景也能实现RL训练的平滑和目标效果。\n后续会分别着重分析这两种解决思路。\n不一致问题分析 这篇文章 从实验的角度来对rollout-training不一致问题进行了分析，主要得出的结论是，不同的并行策略以及更长的响应长度会增大二者之间的mismatch，而选择不同的推理后端的影响比较小。\n这些消融实验可以带来一些经验的归纳，也就是说明现象。但是笔者认为并不能让我们完全理解mismatch产生的原因。笔者认为，不一致性的主要来源，还是因为训练（FSDP、Megatron等）和推理（vLLM、SGLang等）是针对不同计算pattern进行了各自侧重的优化，不论是前向的kernel算子差异带来的数值精度误差累积，还是切分策略带来的通信算子规约顺序带来的精度误差累计，都是mismatch的原因一部分。\n而像MoE模型的稀疏以及动态路由特性，会带来比Dense模型更大的mismatch，因为路由机制本身就是数值精度敏感的，一些微小的数值差异，会带来差异巨大的专家激活。除此之外，MoE模型本身的稀疏特性，和Dense模型相比一般规模会更大，而现代的推理引擎，通常针对MoE模型有独特的优化手段（计算、通信），也会放大训推引擎之间的不一致性。\n而字节团队的这篇文章，对训推不一致问题进行了更加深入的理论、实验分析，针对不一致的现象，也提出了更genearal的叙述：\nTo achieve the massive throughput required, modern inference engines (e.g., vLLM, SGLang, TensorRT-LLM) employ aggressive optimization strategies like speculative decoding, low-precision computation (INT8/FP8), and specialized, batch-variant CUDA kernels. While maintaining sampling fidelity, the primary objective of modern inference engines is to maximize throughput, often measured in tokens per second. Conversely, training frameworks (e.g., FSDP, DeepSpeed, Megatron-LM) must strike a different balance, prioritizing numerical stability and precision for gradient computation, often using higher-precision formats like FP32 for master weights and optimizer states. This divergence in optimization priorities and constraints creates an inevitable training-inference mismatch.\n因此，我们可以回到一开始提到的业界解决on-policy RL训推不一致问题的两个思路，实际上是在性能和一致性上trade-off的取舍，如果希望对齐训推计算（例如之前讨论的batch invariant），势必会带来性能上的劣化。\n从这篇文档，能得到很多有用的takeaways，比如实验中衡量不一致性的用的是下面的vllm-klmetric：\n$$\\small{\\mathbb{E}_{s\\sim d_{\\textcolor{red}{\\pi^\\text{vllm}_\\theta}}}\\left[\\text{KL}\\left(\\textcolor{red}{\\pi^\\text{vllm}_\\theta}\\left(\\cdot|s\\right),\\textcolor{blue}{\\pi^\\text{fsdp}_\\theta}\\left(\\cdot|s\\right)\\right)\\right] = \\mathbb{E}_{s\\sim d_{\\textcolor{red}{\\pi^\\text{vllm}_\\theta}},a\\sim {\\textcolor{red}{\\pi^\\text{vllm}_\\theta}\\left(\\cdot|s\\right)}} \\left[\\log\\left(\\frac{\\textcolor{red}{\\pi^\\text{vllm}_\\theta}(a|s)}{\\textcolor{blue}{\\pi^\\text{fsdp}_\\theta}(a|s)}\\right)\\right],}$$\n这个metric的异常spike，通常能导致训练侧entropy和rewards的异常波动。 而vllm-kl的spike同时会导致fsdp-ppl和gradient norm的爆炸性波动，这表示FSDP engine给推理引擎采样的得到的tokens设置特别小的概率，导致梯度爆炸，从而让RL训练崩溃。\n以及mismatch不是均匀分布的，如果推理引擎得到的token概率越接近0，那在训练侧这个token的概率会更严重地被压小，让mismatch更大。\n除此之外，还发现OOD tools的返回比如multi-turn TIR会带来更大的mismatch等等问题。\n所以综上所述，在当前的RL框架中，训推引擎之间的不一致，是一个不可避免的问题，如果不一致问题非常严重，容易导致训练崩溃这样的严重后果（特别在长稳训练下）。\n接下来笔者详细介绍一下，业界针对不一致问题的解决思路和方案。\n硬对齐训推前反向不同kernel TorchTitan + vLLM TorchTitan项目探索了基于vllm的确定性RL的实现，基于vllm的确定性前向实现，补充了vllm operations的反向传播。其具体的实现为：\n利用vLLM的batch invariant前向实现。 1 2 3 # These operations are deterministic when batch_invariance is enabled y = torch.matmul(a, b) # Uses vLLM's deterministic matmul output = flash_attn_varlen_func(q, k, v, num_splits=1) # Deterministic FA 实现了自定义的反向函数进行梯度计算。 1 2 3 4 5 6 7 8 9 10 11 class FlashAttnWithBackward(torch.autograd.Function): @staticmethod def forward(ctx, q, k, v, ...): # Use vLLM's forward implementation return flash_attn_varlen_func(q, k, v, num_splits=1, ...) @staticmethod def backward(ctx, grad_output): # Compute gradients deterministically # (re-compute attention weights and apply chain rule) return grad_q, grad_k, grad_v, ... 提供了torchtitan和vllm侧不同格式的权重转换能力。 Slime + SGLang SGLang团队在Thinking Machines Lab发布的批次不变算子基础之上，通过定制一系列注意力算子和采样逻辑，也实现了完全确定性推理。该实现同时保持与分块预填充 (chunked prefill)、CUDA Graph、Radix Cache 和非贪婪采样 (non-greedy sampling) 等关键功能的兼容性。SGLang侧的主要增强工作为:\n集成Thinking Machines Lab的批次不变(batch invariant)算子。 实现固定KV分割大小的批次不变注意力算子。支持多种后端，包括 FlashInfer、FlashAttention 3和Triton。 与关键推理性能相关功能完全兼容，例如分块预填充、CUDA图、基数缓存等，当启用确定性推理时，所有这些功能都仍受支持。 支持按请求设置采样种子(per-request sampling seed)，即使在temperature\u003e0的非贪婪采样模式下也能实现确定性推理。 而在slime侧，主要是进行了torch设置：\ntorch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False torch.use_deterministic_algorithms(True, warn_only=False) 以及环境变量：\nsetting environment variable NCCL_ALGO=Ring, NVTE_ALLOW_NONDETERMINISTIC_ALGO=0, CUBLAS_WORKSPACE_CONFIG=:4096:8; 针对megatron后端设置--deterministic-mode 详细可以看此PR。\nRL算法侧缓解差异（off-policy correction） Mismatch Importance Sampling TIS（截断重要性采样） 比较早的博客是(Yao et al.2025)，分析了用重要性采样从算法上缓解训推不一致性的问题。对REINFORCE的梯度表示：\n$$\\mathbb{E}_{a \\sim \\textcolor{red}{\\pi_{\\text{sampler}}}(\\theta)} [R(a)\\cdot \\nabla_\\theta \\log \\textcolor{blue}{\\pi_{\\text{learner}}}(a, \\theta)],$$转换为：\n$$\\mathbb{E}_{a \\sim \\textcolor{red}{\\pi_{\\text{sampler}}}(\\theta)} \\Bigl[\\frac{\\textcolor{blue}{\\pi_{\\text{learner}}}(a, \\theta)}{\\textcolor{red}{\\pi_{\\text{sampler}}}(a, \\theta)} \\cdot R(a)\\cdot \\nabla_\\theta \\log \\textcolor{blue}{\\pi_{\\text{learner}}}(a, \\theta)\\Bigr].$$\n而后基于比较经典的TIS方法，可以实现更稳定的重要性采样: $$\\mathbb{E}_{a \\sim \\textcolor{red}{\\pi_{\\text{sampler}}}(\\theta)} \\Bigl[\\underbrace{\\min\\Bigl(\\frac{\\textcolor{blue}{\\pi_{\\text{learner}}}(a, \\theta)}{\\textcolor{red}{\\pi_{\\text{sampler}}}(a, \\theta)}, C\\Bigr)}_{\\text{truncated importance ratio}} \\cdot R(a) \\cdot \\nabla_\\theta \\log \\textcolor{blue}{\\pi_{\\text{learner}}}(a, \\theta)\\Bigr],$$扩展到PPO算法，策略梯度为经典的公式: $$\\small{ \\mathbb{E}_{a\\sim\\pi_{\\theta_{\\mathrm{old}}}} \\Bigl[ \\nabla_\\theta \\min\\Bigl( \\frac{\\pi_\\theta(a)}{\\pi_{\\theta_{\\mathrm{old}}}(a)}\\,\\hat A, \\;\\mathrm{clip}\\bigl(\\frac{\\pi_\\theta(a)}{\\pi_{\\theta_{\\mathrm{old}}}(a)},\\,1-\\epsilon,\\,1+\\epsilon\\bigr)\\,\\hat A \\Bigr) \\Bigr]}.$$\n为了提升吞吐，Hybrid RL系统比如veRL使用vLLM这类推理引擎做rollout采样，而后回到训练侧用训练引擎再做一次 $\\pi_{\\theta old}$的recompute：\n$$ \\small{ \\mathbb{E}_{a\\sim\\textcolor{red}{\\pi_{\\text{sampler}}}(\\theta_{\\mathrm{old}})} \\Bigl[ \\nabla_\\theta \\min\\Bigl( \\frac{\\textcolor{blue}{\\pi_{\\text{learner}}}(a, \\theta)}{\\textcolor{blue}{\\pi_{\\text{learner}}}(a, \\theta_{\\mathrm{old}})}\\,\\hat A, \\;\\mathrm{clip}\\bigl(\\frac{\\textcolor{blue}{\\pi_{\\text{learner}}}(a, \\theta)}{\\textcolor{blue}{\\pi_{\\text{learner}}}(a, \\theta_{\\mathrm{old}})},\\,1-\\epsilon,\\,1+\\epsilon\\bigr)\\,\\hat A \\Bigr) \\Bigr] }.$$\n同样的，这种训练和推理的mismatch会出现，那么可以使用TIS进行校准：\n$$\\small{\\mathbb{E}_{a\\sim\\textcolor{red}{\\pi_{\\mathrm{sampler}}}(\\theta_{\\mathrm{old}})}\\Bigl[\\underbrace{\\min\\Bigl( \\frac{\\textcolor{blue}{\\pi_{\\mathrm{learner}}}(a,\\theta_{\\mathrm{old}})}{\\textcolor{red}{\\pi_{\\mathrm{sampler}}}(a,\\theta_{\\mathrm{old}})}, C\\Bigr)}_{\\text{truncated importance ratio}}\\cdot\\nabla_{\\theta}\\,\\min\\Bigl( \\frac{\\textcolor{blue}{\\pi_{\\mathrm{learner}}}(a,\\;\\theta)}{\\textcolor{blue}{\\pi_{\\mathrm{learner}}}(a,\\;\\theta_{\\mathrm{old}})}\\,\\hat{A}, \\mathrm{clip}\\Bigl( \\frac{\\textcolor{blue}{\\pi_{\\mathrm{learner}}}(a,\\;\\theta)}{\\textcolor{blue}{\\pi_{\\mathrm{learner}}}(a,\\;\\theta_{\\mathrm{old}})}, 1-\\epsilon,\\;1+\\epsilon \\Bigr)\\,\\hat{A}\\Bigr)\\Bigr]}​$$\n文中也做了一些对比实验，表示此类校准确实能减少训推之间的计算分布差异:\n除此之外，不同的IS变种的效果也有所不同。例如Colossal框架使用的PPO-IS格式: $$\\small{ \\mathbb{E}_{a\\sim\\textcolor{red}{\\pi_{\\mathrm{sampler}}}(\\theta_{\\mathrm{old}})}\\Bigl[\\nabla_{\\theta}\\,\\min\\Bigl( \\frac{\\textcolor{blue}{\\pi_{\\mathrm{learner}}}(a,\\;\\theta)}{\\textcolor{red}{\\pi_{\\mathrm{sampler}}}(a,\\;\\theta_{\\mathrm{old}})}\\,\\hat{A}, \\mathrm{clip}\\Bigl( \\frac{\\textcolor{blue}{\\pi_{\\mathrm{learner}}}(a,\\;\\theta)}{\\textcolor{red}{\\pi_{\\mathrm{sampler}}}(a,\\;\\theta_{\\mathrm{old}})}, 1-\\epsilon,\\;1+\\epsilon \\Bigr)\\,\\hat{A}\\Bigr)\\Bigr]}$$\n以及Nemo-RL框架使用的格式：\n$$\\small{\\mathbb{E}_{\\textcolor{red}{\\pi_{\\mathrm{sampler}}}(\\theta_{\\mathrm{old}})}\\Bigl[\\underbrace{\\frac{\\textcolor{blue}{\\pi_{\\mathrm{learner}}}(a,\\theta_{\\mathrm{old}})}{\\textcolor{red}{\\pi_{\\mathrm{sampler}}}(a,\\theta_{\\mathrm{old}})} }_{\\text{importance ratio}}\\cdot\\nabla_{\\theta}\\,\\min\\Bigl( \\frac{\\textcolor{blue}{\\pi_{\\mathrm{learner}}}(a,\\;\\theta)}{\\textcolor{blue}{\\pi_{\\mathrm{learner}}}(a,\\;\\theta_{\\mathrm{old}})}\\,\\hat{A}, \\mathrm{clip}\\Bigl( \\frac{\\textcolor{blue}{\\pi_{\\mathrm{learner}}}(a,\\;\\theta)}{\\textcolor{blue}{\\pi_{\\mathrm{learner}}}(a,\\;\\theta_{\\mathrm{old}})}, 1-\\epsilon,\\;1+\\epsilon \\Bigr)\\,\\hat{A}\\Bigr)\\Bigr]}​$$\n对比下来还是TIS更加稳定，特别是在训推不同量化这种场景下（e.g. FP8/INT8），更加明显。\n更多的IS变种 更进一步的，前面介绍的字节的这篇工作还是更细致分析了不同IS：\nToken-level / Sequence-level TIS\n给定upper和lower bound，针对weights超过这个部分的做clip。 Token-level / Sequence-level MIS\n给定upper和lower bound，将超出这个范围的weights置为0，相当与mask out掉，这个策略更加激进，适合处理极端的mismatch\n简单来说，有以下的几个结论。 Token-level的IS是理论有偏（biased）的估计，而Sequence-level的IS是无偏的估计，通常能有更稳定的训练。 在复杂的场景（例如TIR），token-level的TIS还是会failed，但是在简单的reasoning RL，当mismatch较小的时候（比如on-policy GRPO）, token-level的TIS够用，可以防止梯度爆炸，但是训练稳定性、训练摸高效果可能由于梯度的bias会有限制。 MIS（masked IS）效果通常比TIS要更好，同样sequence-level的测试，不论在training reward还是评估分数，都能超过不用IS的原始训练。 sequence-level的MIS在更复杂、更长上下文的自回归任务上，表现的还是比token-level要好，这符合理论预期。 VeRL的rollout correction实现 建议直接参考verl rollout correction文档\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 algorithm: rollout_correction: rollout_is: token # IS weights: \"token\", \"sequence\", or null rollout_is_threshold: 2.0 # Upper threshold for IS weights rollout_is_batch_normalize: false # Batch normalize IS weights to mean=1.0 rollout_rs: null # Rejection sampling: \"token\", \"sequence\", \"geometric\", or null rollout_rs_threshold: null # RS upper threshold (required if rollout_rs is enabled) rollout_rs_threshold_lower: null # RS lower threshold (auto-reciprocal if null) rollout_token_veto_threshold: null # Per-token veto threshold (null = disabled) bypass_mode: false # Skip old_log_prob computation use_policy_gradient: false # Use policy gradient loss (vs PPO loss) # REQUIRED: Enable log prob calculation actor_rollout_ref: rollout: calculate_log_probs: true 具体分为BypassMode以及IS/RS的实现，实现放在verl.trainer.ppo.rollout_corr_helper.py中。\napply_rollout_correction在verl.trainer.ppo.ray_trainer.py的RayPPOTrainer的fit中调用，调用场景是bypass_mode被使能，也就是不重计算old_log_prob，apply以后，实际上设置了loss_mode为rollout_correction，后续计算的时候, loss_fn会选择compute_policy_loss_with_rollout_correction（见verl.trainer.ppo.core_algos.py），在这个函数中，会on-the-fly调用compute_rollout_correction_and_rejection_mask计算IS之后的weight和modified的response mask。\n而对bypass_mode关闭的模式，也就是decoupled-ppo模式，会重计算old_log_prob，相当于每个mini batch都要调用compute_rollout_correction_and_rejection_mask计算IS的weight和response make，然后添加到batch中（union），然后正常走正常的流程，比如调用compute_policy_loss_vanilla里会处理。\nSlime的IS实现 可以参考合入PR，和verl类似，都实现了token/sequence/geometric mean级别的TIS、MIS等校准策略。\n--use-train-infer-is: Enable training-inference importance sampling --train-infer-is-level: Aggregation level (token/sequence/geometric) --train-infer-is-mode: Processing mode (truncate/mask/clip) --train-infer-is-lower-bound/--train-infer-is-upper-bound: Weight bounds --train-infer-is-veto-threshold: Catastrophic token threshold Routing Replay https://arxiv.org/html/2510.11370v1\nhttps://arxiv.org/html/2510.23027v1 RSPO routing fluctuations\nRollout Routing Replay 主要解决在专家混合（MoE）大模型中，因其路由机制在训练和推理阶段的行为不一致，导致训练和推理的 logprob产生比较大的差异进而引起强化学习（RL）训练不稳定甚至崩溃的问题。\nRollout Routing Replay 会在模型进行推理时（Rollout 阶段），记录下每个 token 的 router 分布，然后在后续的训练过程中使用这些 router 分布进行计算。通过这种方式，强制训练过程模仿并对齐推理时的 router 行为，从而弥合两者之间的差异。\n需要注意的是, GSPO论文中提到的Routing Replay, 是训练侧old和target策略之间，如果进行token-level的重要性采样，可能导致专家激活模式在新旧策略之间有差异，这种路由波动可能破坏训练稳定性，GSPO因为引入了seq-level的重要性采样，对单个token的专家波动不敏感，可以不需要routing replay（而GRPO不引入routing replay容易训崩）。而上面讨论的routing replay，主要还是解决训推不一致导致的路由波动带来的问题。\n值得注意的其他研究 BF16切换为FP16 有一篇更新的(2025/10/30)的来自Sea AI Lab的文章Defeating the Training-Inference Mismatch via FP16 提出了一个更简单的statement：我们不需要上面这些复杂的算法修正，只需要把精度从BF16切回FP16，不一致带来的问题就会解决。\n研究认为，虽然FP16和BF16都使用总计16位，单比特分布不一样，尾数项和指数项的差异，导致FP16的精度为BF16的8倍。但带来的trade-off就是FP16能表征的数值范围就小的多，容易会overflow，可能需要额外的技巧比如loss scaling（反向之前把计算得到的loss乘大的缩放因子，反向时用放大了的loss计算梯度，落入fp16可表征的范围，利用梯度更新权重之前把梯度再还原回去）等稳定性技术进行缓解。\n文章作者认为，在RL后训练的阶段，模型权重基本已经经过了pre-training，数值分布相对稳定，配合loss scalling等技巧fp16可以保证训练稳定。实际上这个是一个非常重要的场景预设，也是这个研究的出发点。因为BF16格式在LLM训练中，被广泛使用的原因，就是其既能够表征FP32的数值范围（指数位都是8），从而在预训练的时候，甚至不需要loss scaling这种增加复杂度的工程实现，也可能做到训练的稳定性，而且显存占用和全精度比也省一半。这种流行，是建立在pre-training对精度问题的“宽容度”比较高这个前提上的：\n在pre-training阶段，模型在海量的数据上学习通用的统计规律，pre-training过程有很强的鲁棒性，对数值噪声不敏感。\n和pre-training阶段只关心“预测的next token是否准确”相对的，RL post-training更关注的是策略的更新幅度，例如PPO就依赖新旧策略的的ratio，通过限制ratio不要偏离1太远来保证训练稳定，这对训练时的精度需求就会更为严苛，如果由于精度表征问题，导致本来有差异的新旧策略，被截断成了ratio=1，那么策略就不更新了。而且针对KL散度约束这种设计到log计算的，精度差异带来的影响会在对数域被放大，那KL算不准，reward model给出奖励信号可能导致模型迅速过拟合到某个错误的pattern上，或者训练非常不稳定。 因此这里的takeaway简单来说，就是在RL场景下，梯度的数值范围通常没有pre-training方差那么大，对梯度的精度要求更高，所谓为了获得比bf16高8倍的精度表征，用fp16 + loss scaling增加的工程成本是值得的。\n","wordCount":"816","inLanguage":"en","image":"https://pillumina.github.io/imgs/icon_head.png","datePublished":"2025-11-20T11:30:12+08:00","dateModified":"2025-11-20T11:30:12+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://pillumina.github.io/posts/aiinfra/14-deterministic-rl/"},"publisher":{"@type":"Organization","name":"CctoctoFX","logo":{"@type":"ImageObject","url":"https://pillumina.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://pillumina.github.io/ accesskey=h title="CctoctoFX (Alt + H)"><img src=https://pillumina.github.io/apple-touch-icon.png alt aria-label=logo height=30>CctoctoFX</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://pillumina.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://pillumina.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://pillumina.github.io/posts/aiinfra/ title="AI Infra"><span>AI Infra</span></a></li><li><a href=https://pillumina.github.io/posts/llmtheory/ title=Thoery><span>Thoery</span></a></li><li><a href=https://pillumina.github.io/posts/programming/ title=Programming><span>Programming</span></a></li><li><a href=https://pillumina.github.io/social/ title=Social><span>Social</span></a></li><li><a href=https://pillumina.github.io/open_courses/ title=Study><span>Study</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://pillumina.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/aiinfra/>AI Infra</a></div><h1 class="post-title entry-hint-parent">[Deterministic RL] deterministic问题的来源和相关工作总结</h1><div class=post-meta><span title='2025-11-20 11:30:12 +0800 CST'>November 20, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;816 words&nbsp;·&nbsp;Me</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#浮点数的非结合性>浮点数的非结合性</a></li><li><a href=#为何计算内核不同序add-numbers>为何计算内核不同序add numbers？</a></li><li><a href=#什么时候真正需要atomic-add>什么时候真正需要atomic add？</a></li><li><a href=#系统级别批次不变性的缺失batch-invariant>系统级别批次不变性的缺失（batch invariant）</a></li><li><a href=#和并行策略相关的reduction不确定性>和并行策略相关的Reduction不确定性</a></li></ul><ul><li><a href=#batch-invariant-ops><code>batch invariant ops</code></a><ul><li><a href=#batch-invariant的rmsnorm>batch invariant的<code>RMSNorm</code></a></li><li><a href=#batch-invariant的矩阵乘法>batch invariant的矩阵乘法</a></li><li><a href=#batch-invariant的注意力计算>batch invariant的注意力计算</a></li><li><a href=#sglang--vllm-实现deterministic-inference>Sglang / vLLM 实现deterministic inference</a></li></ul></li></ul><ul><li><a href=#不一致问题分析>不一致问题分析</a></li><li><a href=#硬对齐训推前反向不同kernel>硬对齐训推前反向不同kernel</a><ul><li><a href=#torchtitan--vllm>TorchTitan + vLLM</a></li><li><a href=#slime--sglang>Slime + SGLang</a></li></ul></li><li><a href=#rl算法侧缓解差异off-policy-correction>RL算法侧缓解差异（off-policy correction）</a><ul><li><a href=#mismatch-importance-sampling>Mismatch Importance Sampling</a></li><li><a href=#routing-replay>Routing Replay</a></li></ul></li><li><a href=#值得注意的其他研究>值得注意的其他研究</a><ul><li><a href=#bf16切换为fp16>BF16切换为FP16</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=理解llm推理中deterministic问题来源>理解LLM推理中deterministic问题来源<a hidden class=anchor aria-hidden=true href=#理解llm推理中deterministic问题来源>#</a></h1><p>Wiki上对deterministic算法的定义是:</p><blockquote><p>“a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output.”</p></blockquote><p>而我们在文中要讨论的，即对于LLM这个context下的deterministic问题，我会先从inference角度（即重复给定一个确定的input，模型的推理为什么无法给定确定的输出）进行问题的理解，再进一步讨论RL工程中的training & inference之间差异，可能会导致RL训练的崩溃问题，并继续讨论业界现在已有的解决方案、与还在<code>working-in-progress</code>的工作。</p><h2 id=浮点数的非结合性>浮点数的非结合性<a hidden class=anchor aria-hidden=true href=#浮点数的非结合性>#</a></h2><p><a href=https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/>thinking machines lab针对batch invariant讨论的文章</a>，详细地解释了在LLM推理中不确定性的来原，即因为精度有限，GPU浮点数运算中的结合性通常不成立：</p>$$(a+b)+c \neq a+(b+c) $$<p><br><a href=https://arxiv.org/abs/2506.09501>这篇arxiv文章</a>，则更深入得说明了这个问题：</p><blockquote><p>Floating-point arithmetic in GPUs exhibits non-associativity, meaning (a+b)+c≠a+(b+c)(a+b)+c=a+(b+c) due to finite precision and rounding errors. This property directly impacts the computation of attention scores and logits in the transformer architecture, where parallel operations across multiple threads can yield different results based on execution order.</p></blockquote><p>浮点数通常可用科学计数的表示来表征大/小数，例如格式$mantissa *10^{exponent}$，如果指数项是不同的，也就是文中说的<code>add at different scales</code>，那不同累加序导致的精度损失会更加明显，而这种不同scale的累加是最常见的场景。</p><p>但是尽管这是不一致输出的根本原因，但是并没有回答不确定性源自何处。无法帮助我们去理解：浮点数值为何会以不同的顺序相加、这种情况何时会发生，已经如何避免这种情况。</p><h2 id=为何计算内核不同序add-numbers>为何计算内核不同序add numbers？<a hidden class=anchor aria-hidden=true href=#为何计算内核不同序add-numbers>#</a></h2><p>一个常见的假说是“<strong>并发执行随机性 + 浮点运算误差</strong>”。这个假说的核心观点，就是如果并发线程的结束顺序是非确定的，并且数值累加顺序如果依赖于并发线程的结束顺序（例如使用atomic add操作），那么最终数值累加的顺序也是非确定的。</p><h2 id=什么时候真正需要atomic-add>什么时候真正需要atomic add？<a hidden class=anchor aria-hidden=true href=#什么时候真正需要atomic-add>#</a></h2><p>但是问题是，LLM前向的GPU内核实际上很少用atomic add操作。</p><blockquote><p>简单解释下Atomic Add的含义：GPU 会把同一段程序同时扔到很多“小核”（SM）上去跑。这些小核之间天生没有步调一致的机制，谁快谁慢完全看当时心情。于是，如果它们需要把结果写到同一个地方，就会出问题。那atomic add就是，硬件保证所有人的结果最终都会加进去，但谁先谁后、按什么顺序加，完全不保证，因此每次跑出来的累加顺序都可能不一样。<br>再举个例子，通过torch.sum()对100个数求和，GPU 可以让 100 个小核各读一个数，这一步完全并行。可最后总得把 100 个数合并成 1 个总和。若用原子加，就是让每个小核随便谁先到，就先把它的数塞到同一个累加器里。硬件只负责“不会丢数”，却不负责“按固定顺序加”。于是同样跑两遍，先加谁后加谁可能不同，结果也就可能出现那一点点浮点误差。</p></blockquote><p>我们回想通常定义的不确定性的含义：同一段 kernel、同一批输入，跑两遍却得到两个略有差异的结果。这叫“run-to-run 非确定性”——哪怕 Python 脚本、依赖库、硬件都没变，第二次跑就是能给你不一样的数。虽然atomic add会导致这个问题，但是更通常的情况是，LLM一次典型的forward，通常一个atomic add也没有。</p><p>这主要有两个原因：</p><ol><li>batch维度上实际上已经“人多势众”，根本不需要在reduction维度再去并行。</li><li>大多数neural networking library也用了很多技巧来实现“既保障确定性又快”，例如“分段树形归约”（split/tree reduction），可以先把100个数拆成5组，每组20个数，5个小核并行计算一个局部和。剩下 5 个局部和要么交给一个核顺序“扫尾”（元素少，开销可忽略），要么用信号量（semaphore）按固定顺序让不同线程块依次累加，从而保证先后顺序一致，结果也就 deterministic 了。</li></ol><p>不过，在<code>pytorch</code>中的<code>scatter_add</code>操作（<code>a[b] += c</code>），如果不用atomic add性能会特别慢，而LLM中唯一踩这个坑的，是<code>FlashAttention</code>的反向传播。</p><blockquote><p>不过当前Triton版本FA反向实现，其实和Tri Dao原论文里的算法不完全一样，Triton版为了躲开atomic add，额外多算了一遍中间结果&ndash;FLOPS直接多了40%，但是换来了determinstic，也算明码标价。</p></blockquote><p>但是正向传播里，LLM中根本就没有非得用atomic add的算子，所以结论就是：LLM 的前向推理，跑两次、跑一百次，结果<strong>比特级完全一致</strong>；真正可能“每次不一样”的，只出在反向训练阶段，而且基本就 FlashAttention 一家。（也就是前向是“run-to-run deterministic”的）。</p><h2 id=系统级别批次不变性的缺失batch-invariant>系统级别批次不变性的缺失（batch invariant）<a hidden class=anchor aria-hidden=true href=#系统级别批次不变性的缺失batch-invariant>#</a></h2><p>前向kernel函数的确定性，实际上不等于整个推理服务对外表现确定，也就是还存在额外的<strong>系统级非确定性</strong>。因为真正喂给前向的<strong>张量内容</strong>还可能被其他“外部输入”左右。</p><p>举个经典里batch norm的坑：早期 BatchNorm 把“整批统计量”（均值/方差）当常量参与计算。<br>同一句话，单条跑 μ₁ σ₁，跟 32 条一起跑 μ₃₂ σ₃₂，算出来的隐藏值就不一样，于是最终 token 概率也不同。<br>站在“单条请求”视角：它完全无法预知今晚会不会有 31 个请求来搭伙，所以即使自己 prompt 固定，输出依旧“看运气”——这就是上面所说的<strong>系统级非确定性</strong>。</p><p>LLM 推理里虽然早就把 BatchNorm 踢了出去，却<strong>仍缺“批次不变性”（batch invariance）</strong>：同一请求、同一模型权重，只要推理时动态批大小不同，可能会导致<code>tilesize</code>不同，导致reduce的计算结果不同。例如，vLLM在不同规模的batch下，把prompt送往不同的batch，而GPU的并行调度器SIMT会把矩阵送往不同的sm、warp，这样计算路径就每次都不一样。</p><p>所以针对推理引擎，比如要在kernel层面实现batch invariant才能解决serve层面不确定性的问题。</p><h2 id=和并行策略相关的reduction不确定性>和并行策略相关的Reduction不确定性<a hidden class=anchor aria-hidden=true href=#和并行策略相关的reduction不确定性>#</a></h2><p>TODO: 通信库的通信算子带来的规约不确定性。</p><pre tabindex=0><code>export HCCL_DETERMINISTIC=true
</code></pre><p>开启HCCL的规约类算子的确定性计算。</p><h1 id=batch-invariant的相关工作><code>Batch Invariant</code>的相关工作<a hidden class=anchor aria-hidden=true href=#batch-invariant的相关工作>#</a></h1><h2 id=batch-invariant-ops><code>batch invariant ops</code><a hidden class=anchor aria-hidden=true href=#batch-invariant-ops>#</a></h2><p>Thinking Machines Lab发布了<code>batch invariant</code>的<a href=https://github.com/thinking-machines-lab/batch_invariant_ops/tree/main>部分kernel算子实现</a>。<br>而从原blog里，提出了三种难度递增的实现。</p><h3 id=batch-invariant的rmsnorm>batch invariant的<code>RMSNorm</code><a hidden class=anchor aria-hidden=true href=#batch-invariant的rmsnorm>#</a></h3><p>直接让每个Batch元素的reduction顺序固定，不受batch大小影响。</p><ul><li>batch大时，把单个batch元素分配给单个核心，reduction运算在单核心内完成，batch增大时让核心依次处理多个元素，保持reduction策略不变。</li><li>batch小时，若采用"split reduction"（多核心分担reduction以提升并行度）会破坏batch invariant，可以选择忽略小batch优化（小batch本身执行就快，性能损失可以接受），或者采用固定reduction策略（牺牲部分性能来保证batch invariant）。</li></ul><p><img alt=batch-inv-rmsnorm loading=lazy src="https://pic1.zhimg.com/80/v2-ce80537a575835d21972fe5b063f5bb9_1440w.webp?source=1def8aca" data-zoomable></p><h3 id=batch-invariant的矩阵乘法>batch invariant的矩阵乘法<a hidden class=anchor aria-hidden=true href=#batch-invariant的矩阵乘法>#</a></h3><p>将输出张量拆分为2D tiles，每个tile分配给单个核心，reduction在单个核心内部完成。编译固定配置的内核以适配所有形状，虽然会损失20%性能（和cuBLAS相比），但在LLM推理中通常可以接受，因为模型维度（N）比较大，对split-k的需求较低。</p><p><img alt=batch-inv-gemm loading=lazy src="https://pic1.zhimg.com/80/v2-7a754f390567bf5c6d92ccf2a4267c0a_1440w.webp?source=1def8aca" data-zoomable></p><h3 id=batch-invariant的注意力计算>batch invariant的注意力计算<a hidden class=anchor aria-hidden=true href=#batch-invariant的注意力计算>#</a></h3><p>采用data-parallel策略（沿着Q张量并行，reduction在单核心内完成），更新KV缓存和页表以保证KV布局一致，不受处理token数量的影响。</p><p>decode阶段Q长度小，需要拆分KV维度（Split-KV），采用固定拆分大小策略（而非固定拆分数量），确保reduction顺序不变，比如把1000长度的KV拆成3个256长度和1个232长度的片段，而不是4个250长度的片段。</p><p><img alt=batch-inv-attn loading=lazy src="https://picx.zhimg.com/80/v2-9b088207a9c3ed23e018f1416897134e_1440w.webp?source=1def8aca" data-zoomable></p><h3 id=sglang--vllm-实现deterministic-inference>Sglang / vLLM 实现deterministic inference<a hidden class=anchor aria-hidden=true href=#sglang--vllm-实现deterministic-inference>#</a></h3><p>SGLang团队的<a href=https://lmsys.org/blog/2025-09-22-sglang-deterministic/>博客</a>里记录了实现的细节，主要是针对<code>batch invariant</code>kernel上，针对chunked prefill、cuda graph等特性做了兼容，具体可以参考<a href=https://github.com/sgl-project/sglang/issues/10278>RoadMap</a>。</p><p>vLLM参考<a href=https://docs.vllm.ai/en/latest/features/batch_invariance/>Enabling Batch Invariant文档</a>，也可以参考RFC <a href=https://github.com/vllm-project/vllm/issues/28326>#28326</a>，<a href=https://github.com/vllm-project/vllm/issues/27433>#27433</a>。</p><h1 id=on-policy-rl训练中的训推不一致问题>On-policy RL训练中的训推不一致问题<a hidden class=anchor aria-hidden=true href=#on-policy-rl训练中的训推不一致问题>#</a></h1><p>当讨论on-policy RL训练的时候，<a href=https://fengyao.notion.site/off-policy-rl>有研究指出</a> train / inference engine之间的不一致也会隐形导致on-policy假设的RL实际变成off-policy。所以当我们追求"真正的" on-policy RL训练时，需要知道：如果不能从两个完全一致的推理请求中获取bitwise相等的结果，那么当然也无法保障训推之间的bitwise一致性。所以基于之前我们对确定性推理实现讨论，直觉上可以知道如果保证了确定性推理，那么通过修改训练这部分stack，也能够实现在bitwise上训推的一致性，从而实现真正的on-policy RL训练。</p><p>而业界对这个问题的解决思路上主要分为两种：</p><ul><li><p>在训练引擎侧，基于推理引擎(vllm/sglang)确定性推理内核前向实现，进行反向传递的实现，通过对齐kernel的实现，做到训练和采样部分的bitwise一致性（i.e. 0 KL divergence）。</p></li><li><p>拥抱训推分布的不一致（考虑到训练bitwise实现在工程上的工作量，和不同模型适配的工作量），在算法上为off-policy做off-policy correction，进行训推KL散度的偏差抑制，在大多数场景也能实现RL训练的平滑和目标效果。</p></li></ul><p>后续会分别着重分析这两种解决思路。</p><h2 id=不一致问题分析>不一致问题分析<a hidden class=anchor aria-hidden=true href=#不一致问题分析>#</a></h2><p><a href=https://fengyao.notion.site/off-policy-rl>这篇文章</a> 从实验的角度来对rollout-training不一致问题进行了分析，主要得出的结论是，<strong>不同的并行策略</strong>以及<strong>更长的响应长度</strong>会增大二者之间的mismatch，而选择不同的推理后端的影响比较小。</p><p><img alt=mismatch-parallel loading=lazy src="https://fengyao.notion.site/image/attachment%3A82d124b2-e301-497d-8e8d-5c8b08c12a72%3Avllm_megatron_parallelism.png?table=block&id=279721e3-f6c4-806e-9215-f3811bd6544e&spaceId=5cbd2ef3-859d-42c5-86d3-a8382485dc0e&width=1420&userId=&cache=v2" data-zoomable></p><p><img alt=mismatch-reponse-length loading=lazy src="https://fengyao.notion.site/image/attachment%3Ac030a2b2-299e-4449-8438-d01a6145dc8b%3Amax_mean_20_4.png?table=block&id=279721e3-f6c4-80ce-9b09-ee4a9355a7b8&spaceId=5cbd2ef3-859d-42c5-86d3-a8382485dc0e&width=1420&userId=&cache=v2" data-zoomable></p><p><img alt=mismatch-sampler-backend-dapo-32b loading=lazy src="https://fengyao.notion.site/image/attachment%3A1f820845-a43e-4ed6-8061-98af976d6b8f%3Asglang_vllm_dapo.png?table=block&id=279721e3-f6c4-80f0-ab62-c1cf298e8c71&spaceId=5cbd2ef3-859d-42c5-86d3-a8382485dc0e&width=1420&userId=&cache=v2" data-zoomable></p><p><img alt=mismatch-sampler-backend-polaris-7b loading=lazy src="https://fengyao.notion.site/image/attachment%3A65d1350d-89b3-44f7-8305-bd6e0c2d8672%3Asglang_vllm_pol.png?table=block&id=279721e3-f6c4-8030-95d6-f5ad53d497b9&spaceId=5cbd2ef3-859d-42c5-86d3-a8382485dc0e&width=1420&userId=&cache=v2" data-zoomable></p><blockquote><p>这些消融实验可以带来一些经验的归纳，也就是说明现象。但是笔者认为并不能让我们完全理解mismatch产生的原因。笔者认为，不一致性的主要来源，还是因为训练（FSDP、Megatron等）和推理（vLLM、SGLang等）是针对不同计算pattern进行了各自侧重的优化，不论是前向的kernel算子差异带来的数值精度误差累积，还是切分策略带来的通信算子规约顺序带来的精度误差累计，都是mismatch的原因一部分。</p></blockquote><p>而像MoE模型的稀疏以及动态路由特性，会带来比Dense模型更大的mismatch，因为路由机制本身就是数值精度敏感的，一些微小的数值差异，会带来差异巨大的专家激活。除此之外，MoE模型本身的稀疏特性，和Dense模型相比一般规模会更大，而现代的推理引擎，通常针对MoE模型有独特的优化手段（计算、通信），也会放大训推引擎之间的不一致性。</p><p>而字节团队的<a href=https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda>这篇文章</a>，对训推不一致问题进行了更加深入的理论、实验分析，针对不一致的现象，也提出了更genearal的叙述：</p><blockquote><p>To achieve the massive throughput required, modern inference engines (e.g., vLLM, SGLang, TensorRT-LLM) employ aggressive optimization strategies like speculative decoding, low-precision computation (INT8/FP8), and specialized, batch-variant CUDA kernels. While maintaining sampling fidelity, the primary objective of modern inference engines is to maximize throughput, often measured in tokens per second. Conversely, training frameworks (e.g., FSDP, DeepSpeed, Megatron-LM) must strike a different balance, prioritizing numerical stability and precision for gradient computation, often using higher-precision formats like FP32 for master weights and optimizer states. This divergence in optimization priorities and constraints creates an inevitable training-inference mismatch.</p></blockquote><p>因此，我们可以回到一开始提到的业界解决on-policy RL训推不一致问题的两个思路，实际上是在性能和一致性上trade-off的取舍，如果希望对齐训推计算（例如之前讨论的batch invariant），势必会带来性能上的劣化。</p><p>从这篇文档，能得到很多有用的takeaways，比如实验中衡量不一致性的用的是下面的<code>vllm-kl</code>metric：</p>$$\small{\mathbb{E}_{s\sim d_{\textcolor{red}{\pi^\text{vllm}_\theta}}}\left[\text{KL}\left(\textcolor{red}{\pi^\text{vllm}_\theta}\left(\cdot|s\right),\textcolor{blue}{\pi^\text{fsdp}_\theta}\left(\cdot|s\right)\right)\right] = \mathbb{E}_{s\sim d_{\textcolor{red}{\pi^\text{vllm}_\theta}},a\sim {\textcolor{red}{\pi^\text{vllm}_\theta}\left(\cdot|s\right)}} \left[\log\left(\frac{\textcolor{red}{\pi^\text{vllm}_\theta}(a|s)}{\textcolor{blue}{\pi^\text{fsdp}_\theta}(a|s)}\right)\right],}$$<p><br>这个metric的异常spike，通常能导致训练侧entropy和rewards的异常波动。<img alt=vllm-kl-trigger loading=lazy src="https://yingru.notion.site/image/attachment%3A3e26ea60-291f-470c-b2ec-79e7a8815f7f%3Aimage.png?table=block&id=271211a5-58b7-8055-8f35-c4a618ee5fed&spaceId=effaf72e-4449-4e46-8824-1cc2f447196b&width=1420&userId=&cache=v2" data-zoomable></p><p>而<code>vllm-kl</code>的spike同时会导致<code>fsdp-ppl</code>和gradient norm的爆炸性波动，这表示<code>FSDP</code> engine给推理引擎采样的得到的tokens设置特别小的概率，导致梯度爆炸，从而让RL训练崩溃。</p><p>以及mismatch不是均匀分布的，如果推理引擎得到的token概率越接近0，那在训练侧这个token的概率会更严重地被压小，让mismatch更大。</p><p><img alt=uniform-mismatch loading=lazy src="https://yingru.notion.site/image/attachment%3Aa94ce922-399e-4e5e-b5a0-2ea829327d92%3Aimage.png?table=block&id=271211a5-58b7-8084-87e0-d30121785272&spaceId=effaf72e-4449-4e46-8824-1cc2f447196b&width=1420&userId=&cache=v2" data-zoomable></p><p>除此之外，还发现OOD tools的返回比如multi-turn TIR会带来更大的mismatch等等问题。</p><p>所以综上所述，在当前的RL框架中，训推引擎之间的不一致，是一个不可避免的问题，如果不一致问题非常严重，容易导致训练崩溃这样的严重后果（特别在长稳训练下）。</p><p>接下来笔者详细介绍一下，业界针对不一致问题的解决思路和方案。</p><h2 id=硬对齐训推前反向不同kernel>硬对齐训推前反向不同kernel<a hidden class=anchor aria-hidden=true href=#硬对齐训推前反向不同kernel>#</a></h2><h3 id=torchtitan--vllm>TorchTitan + vLLM<a hidden class=anchor aria-hidden=true href=#torchtitan--vllm>#</a></h3><p><a href=https://github.com/pytorch/torchtitan/tree/main/torchtitan/experiments/deterministic_vllm_rl>TorchTitan项目</a>探索了基于vllm的确定性RL的实现，基于vllm的确定性前向实现，补充了vllm operations的反向传播。其具体的实现为：</p><ul><li>利用vLLM的<code>batch invariant</code>前向实现。</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># These operations are deterministic when batch_invariance is enabled</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>  <span class=c1># Uses vLLM&#39;s deterministic matmul</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>flash_attn_varlen_func</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span><span class=p>,</span> <span class=n>num_splits</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># Deterministic FA</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>实现了自定义的反向函数进行梯度计算。</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>FlashAttnWithBackward</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>autograd</span><span class=o>.</span><span class=n>Function</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nd>@staticmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span><span class=p>,</span> <span class=o>...</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Use vLLM&#39;s forward implementation</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>flash_attn_varlen_func</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span><span class=p>,</span> <span class=n>num_splits</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@staticmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>grad_output</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Compute gradients deterministically</span>
</span></span><span class=line><span class=cl>        <span class=c1># (re-compute attention weights and apply chain rule)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>grad_q</span><span class=p>,</span> <span class=n>grad_k</span><span class=p>,</span> <span class=n>grad_v</span><span class=p>,</span> <span class=o>...</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>提供了torchtitan和vllm侧不同格式的权重转换能力。</li></ul><h3 id=slime--sglang>Slime + SGLang<a hidden class=anchor aria-hidden=true href=#slime--sglang>#</a></h3><p>SGLang团队在Thinking Machines Lab发布的批次不变算子基础之上，通过定制一系列注意力算子和采样逻辑，也<strong>实现了完全确定性推理</strong>。该实现同时保持与<strong>分块预填充 (chunked prefill)、CUDA Graph、Radix Cache 和非贪婪采样 (non-greedy sampling)</strong> 等关键功能的兼容性。SGLang侧的主要增强工作为:</p><ul><li>集成Thinking Machines Lab的批次不变(batch invariant)算子。</li><li>实现固定KV分割大小的批次不变注意力算子。支持多种后端，包括 FlashInfer、FlashAttention 3和<a href="https://zhida.zhihu.com/search?content_id=263564186&amp;content_type=Article&amp;match_order=1&amp;q=Triton&amp;zhida_source=entity">Triton</a>。</li><li>与关键推理性能相关功能完全兼容，例如分块预填充、CUDA图、基数缓存等，当启用确定性推理时，所有这些功能都仍受支持。</li><li>支持按请求设置采样种子(per-request sampling seed)，即使在temperature>0的非贪婪采样模式下也能实现确定性推理。</li></ul><p>而在slime侧，主要是进行了torch设置：</p><pre tabindex=0><code>        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        torch.use_deterministic_algorithms(True, warn_only=False)
</code></pre><p>以及环境变量：</p><ul><li>setting environment variable <code>NCCL_ALGO=Ring</code>, <code>NVTE_ALLOW_NONDETERMINISTIC_ALGO=0</code>, <code>CUBLAS_WORKSPACE_CONFIG=:4096:8</code>;</li><li>针对megatron后端设置<code>--deterministic-mode</code></li></ul><p>详细可以看此<a href=https://github.com/THUDM/slime/pull/370>PR</a>。</p><h2 id=rl算法侧缓解差异off-policy-correction>RL算法侧缓解差异（off-policy correction）<a hidden class=anchor aria-hidden=true href=#rl算法侧缓解差异off-policy-correction>#</a></h2><h3 id=mismatch-importance-sampling>Mismatch Importance Sampling<a hidden class=anchor aria-hidden=true href=#mismatch-importance-sampling>#</a></h3><h4 id=tis截断重要性采样>TIS（截断重要性采样）<a hidden class=anchor aria-hidden=true href=#tis截断重要性采样>#</a></h4><p>比较早的博客是<a href=https://fengyao.notion.site/off-policy-rl>(Yao et al.2025)</a>，分析了用重要性采样从算法上缓解训推不一致性的问题。对REINFORCE的梯度表示：</p>$$\mathbb{E}_{a \sim \textcolor{red}{\pi_{\text{sampler}}}(\theta)} [R(a)\cdot \nabla_\theta \log \textcolor{blue}{\pi_{\text{learner}}}(a, \theta)],$$<p>转换为：</p>$$\mathbb{E}_{a \sim \textcolor{red}{\pi_{\text{sampler}}}(\theta)} \Bigl[\frac{\textcolor{blue}{\pi_{\text{learner}}}(a, \theta)}{\textcolor{red}{\pi_{\text{sampler}}}(a, \theta)} \cdot R(a)\cdot \nabla_\theta \log \textcolor{blue}{\pi_{\text{learner}}}(a, \theta)\Bigr].$$<p><br>而后基于比较经典的<a href=https://ionides.github.io/pubs/ionides08-jcgs.pdf>TIS方法</a>，可以实现更稳定的重要性采样:</p>$$\mathbb{E}_{a \sim \textcolor{red}{\pi_{\text{sampler}}}(\theta)} \Bigl[\underbrace{\min\Bigl(\frac{\textcolor{blue}{\pi_{\text{learner}}}(a, \theta)}{\textcolor{red}{\pi_{\text{sampler}}}(a, \theta)}, C\Bigr)}_{\text{truncated importance ratio}} \cdot R(a) \cdot \nabla_\theta \log \textcolor{blue}{\pi_{\text{learner}}}(a, \theta)\Bigr],$$<p>扩展到PPO算法，策略梯度为经典的公式:</p>$$\small{ \mathbb{E}_{a\sim\pi_{\theta_{\mathrm{old}}}} \Bigl[ \nabla_\theta \min\Bigl( \frac{\pi_\theta(a)}{\pi_{\theta_{\mathrm{old}}}(a)}\,\hat A, \;\mathrm{clip}\bigl(\frac{\pi_\theta(a)}{\pi_{\theta_{\mathrm{old}}}(a)},\,1-\epsilon,\,1+\epsilon\bigr)\,\hat A \Bigr) \Bigr]}.$$<p><br>为了提升吞吐，Hybrid RL系统比如veRL使用vLLM这类推理引擎做rollout采样，而后回到训练侧用训练引擎再做一次 $\pi_{\theta old}$的recompute：</p>$$ \small{ \mathbb{E}_{a\sim\textcolor{red}{\pi_{\text{sampler}}}(\theta_{\mathrm{old}})} \Bigl[ \nabla_\theta \min\Bigl( \frac{\textcolor{blue}{\pi_{\text{learner}}}(a, \theta)}{\textcolor{blue}{\pi_{\text{learner}}}(a, \theta_{\mathrm{old}})}\,\hat A, \;\mathrm{clip}\bigl(\frac{\textcolor{blue}{\pi_{\text{learner}}}(a, \theta)}{\textcolor{blue}{\pi_{\text{learner}}}(a, \theta_{\mathrm{old}})},\,1-\epsilon,\,1+\epsilon\bigr)\,\hat A \Bigr) \Bigr] }.$$<p><br>同样的，这种训练和推理的mismatch会出现，那么可以使用TIS进行校准：</p>$$\small{\mathbb{E}_{a\sim\textcolor{red}{\pi_{\mathrm{sampler}}}(\theta_{\mathrm{old}})}\Bigl[\underbrace{\min\Bigl( \frac{\textcolor{blue}{\pi_{\mathrm{learner}}}(a,\theta_{\mathrm{old}})}{\textcolor{red}{\pi_{\mathrm{sampler}}}(a,\theta_{\mathrm{old}})}, C\Bigr)}_{\text{truncated importance ratio}}\cdot\nabla_{\theta}\,\min\Bigl( \frac{\textcolor{blue}{\pi_{\mathrm{learner}}}(a,\;\theta)}{\textcolor{blue}{\pi_{\mathrm{learner}}}(a,\;\theta_{\mathrm{old}})}\,\hat{A}, \mathrm{clip}\Bigl( \frac{\textcolor{blue}{\pi_{\mathrm{learner}}}(a,\;\theta)}{\textcolor{blue}{\pi_{\mathrm{learner}}}(a,\;\theta_{\mathrm{old}})}, 1-\epsilon,\;1+\epsilon \Bigr)\,\hat{A}\Bigr)\Bigr]}​$$<p><br>文中也做了一些对比实验，表示此类校准确实能减少训推之间的计算分布差异:<br><img alt=tis-analysis loading=lazy src="https://fengyao.notion.site/image/attachment%3A766b9627-d7c4-4f0d-ba10-6eda045390a1%3Agsm8k_int8.png?table=block&id=246721e3-f6c4-803f-b9f1-c1e707b64b02&spaceId=5cbd2ef3-859d-42c5-86d3-a8382485dc0e&width=1420&userId=&cache=v2" data-zoomable></p><p>除此之外，不同的IS变种的效果也有所不同。例如Colossal框架使用的PPO-IS格式:</p>$$\small{ \mathbb{E}_{a\sim\textcolor{red}{\pi_{\mathrm{sampler}}}(\theta_{\mathrm{old}})}\Bigl[\nabla_{\theta}\,\min\Bigl( \frac{\textcolor{blue}{\pi_{\mathrm{learner}}}(a,\;\theta)}{\textcolor{red}{\pi_{\mathrm{sampler}}}(a,\;\theta_{\mathrm{old}})}\,\hat{A}, \mathrm{clip}\Bigl( \frac{\textcolor{blue}{\pi_{\mathrm{learner}}}(a,\;\theta)}{\textcolor{red}{\pi_{\mathrm{sampler}}}(a,\;\theta_{\mathrm{old}})}, 1-\epsilon,\;1+\epsilon \Bigr)\,\hat{A}\Bigr)\Bigr]}$$<p><br>以及Nemo-RL框架使用的格式：</p>$$\small{\mathbb{E}_{\textcolor{red}{\pi_{\mathrm{sampler}}}(\theta_{\mathrm{old}})}\Bigl[\underbrace{\frac{\textcolor{blue}{\pi_{\mathrm{learner}}}(a,\theta_{\mathrm{old}})}{\textcolor{red}{\pi_{\mathrm{sampler}}}(a,\theta_{\mathrm{old}})} }_{\text{importance ratio}}\cdot\nabla_{\theta}\,\min\Bigl( \frac{\textcolor{blue}{\pi_{\mathrm{learner}}}(a,\;\theta)}{\textcolor{blue}{\pi_{\mathrm{learner}}}(a,\;\theta_{\mathrm{old}})}\,\hat{A}, \mathrm{clip}\Bigl( \frac{\textcolor{blue}{\pi_{\mathrm{learner}}}(a,\;\theta)}{\textcolor{blue}{\pi_{\mathrm{learner}}}(a,\;\theta_{\mathrm{old}})}, 1-\epsilon,\;1+\epsilon \Bigr)\,\hat{A}\Bigr)\Bigr]}​$$<p><br>对比下来还是TIS更加稳定，特别是在训推不同量化这种场景下（e.g. FP8/INT8），更加明显。</p><h4 id=更多的is变种>更多的IS变种<a hidden class=anchor aria-hidden=true href=#更多的is变种>#</a></h4><p>更进一步的，前面介绍的字节的<a href=https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda>这篇工作</a>还是更细致分析了不同IS：</p><ul><li><p>Token-level / Sequence-level TIS</p><ul><li>给定upper和lower bound，针对weights超过这个部分的做clip。</li></ul></li><li><p>Token-level / Sequence-level MIS</p><ul><li>给定upper和lower bound，将超出这个范围的weights置为0，相当与mask out掉，这个策略更加激进，适合处理极端的mismatch<br>简单来说，有以下的几个结论。</li></ul></li><li><p>Token-level的IS是理论<strong>有偏</strong>（biased）的估计，而Sequence-level的IS是<strong>无偏</strong>的估计，通常能有更稳定的训练。<img alt=token-seq-compare loading=lazy src="https://yingru.notion.site/image/attachment%3A6e07f8c0-50bb-4418-8fa2-878ea8b5283b%3Aimage.png?table=block&id=271211a5-58b7-8008-aab6-ca175a776bdd&spaceId=effaf72e-4449-4e46-8824-1cc2f447196b&width=1420&userId=&cache=v2" data-zoomable></p></li><li><p>在复杂的场景（例如TIR），token-level的TIS还是会failed，但是在简单的reasoning RL，当mismatch较小的时候（比如on-policy GRPO）, token-level的TIS够用，可以防止梯度爆炸，但是训练稳定性、训练摸高效果可能由于梯度的bias会有限制。<img alt=simper-tis-case loading=lazy src="https://yingru.notion.site/image/attachment%3A15dcdfb6-18f5-4b92-91de-fe32f3e4c7d0%3Aimg_v3_02qd_87d5ed2e-e1f2-4b43-9eb5-d52ad649685g.jpg?table=block&id=27b211a5-58b7-80b6-8007-d80f67c22b85&spaceId=effaf72e-4449-4e46-8824-1cc2f447196b&width=1420&userId=&cache=v2" data-zoomable></p></li><li><p>MIS（masked IS）效果通常比TIS要更好，同样sequence-level的测试，不论在training reward还是评估分数，都能超过不用IS的原始训练。<img alt=tis-mis loading=lazy src="https://yingru.notion.site/image/attachment%3A07340732-81a9-4d0e-9dc8-0e2450a386b8%3Aimage.png?table=block&id=27b211a5-58b7-808b-92f0-f53e0439f9d6&spaceId=effaf72e-4449-4e46-8824-1cc2f447196b&width=1420&userId=&cache=v2" data-zoomable></p></li><li><p>sequence-level的MIS在更复杂、更长上下文的自回归任务上，表现的还是比token-level要好，这符合理论预期。 <img alt=seq-mis-token-mis loading=lazy src="https://yingru.notion.site/image/attachment%3A6269fa85-8a0c-40ab-8485-9238805b69ce%3Aimg_v3_02qb_d5ff66e7-2585-4275-a6f7-db4a65fbe60g.jpg?table=block&id=27b211a5-58b7-80a9-9837-dc6b5f980e1b&spaceId=effaf72e-4449-4e46-8824-1cc2f447196b&width=1420&userId=&cache=v2" data-zoomable></p></li></ul><h4 id=verl的rollout-correction实现><code>VeRL</code>的rollout correction实现<a hidden class=anchor aria-hidden=true href=#verl的rollout-correction实现>#</a></h4><blockquote><p>建议直接参考<a href=https://verl.readthedocs.io/en/latest/algo/rollout_corr.html>verl rollout correction文档</a></p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>algorithm</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>rollout_correction</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>rollout_is: token                      # IS weights</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;token&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;sequence&#34;</span><span class=p>,</span><span class=w> </span><span class=l>or null</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>rollout_is_threshold</span><span class=p>:</span><span class=w> </span><span class=m>2.0</span><span class=w>              </span><span class=c># Upper threshold for IS weights</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>rollout_is_batch_normalize</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>      </span><span class=c># Batch normalize IS weights to mean=1.0</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>rollout_rs: null                       # Rejection sampling</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;token&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;sequence&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;geometric&#34;</span><span class=p>,</span><span class=w> </span><span class=l>or null</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>rollout_rs_threshold</span><span class=p>:</span><span class=w> </span><span class=kc>null</span><span class=w>             </span><span class=c># RS upper threshold (required if rollout_rs is enabled)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>rollout_rs_threshold_lower</span><span class=p>:</span><span class=w> </span><span class=kc>null</span><span class=w>       </span><span class=c># RS lower threshold (auto-reciprocal if null)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>rollout_token_veto_threshold</span><span class=p>:</span><span class=w> </span><span class=kc>null</span><span class=w>     </span><span class=c># Per-token veto threshold (null = disabled)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>bypass_mode</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>  </span><span class=c># Skip old_log_prob computation</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>use_policy_gradient</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>     </span><span class=c># Use policy gradient loss (vs PPO loss)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># REQUIRED: Enable log prob calculation</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>actor_rollout_ref</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>rollout</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>calculate_log_probs</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>具体分为<code>BypassMode</code>以及<code>IS/RS</code>的实现，实现放在<code>verl.trainer.ppo.rollout_corr_helper.py</code>中。</p><p><code>apply_rollout_correction</code>在<code>verl.trainer.ppo.ray_trainer.py</code>的<code>RayPPOTrainer</code>的<code>fit</code>中调用，调用场景是<code>bypass_mode</code>被使能，也就是不重计算<code>old_log_prob</code>，apply以后，实际上设置了<code>loss_mode</code>为<code>rollout_correction</code>，后续计算的时候, <code>loss_fn</code>会选择<code>compute_policy_loss_with_rollout_correction</code>（见<code>verl.trainer.ppo.core_algos.py</code>），在这个函数中，会<code>on-the-fly</code>调用<code>compute_rollout_correction_and_rejection_mask</code>计算IS之后的weight和modified的response mask。</p><p>而对<code>bypass_mode</code>关闭的模式，也就是<code>decoupled-ppo</code>模式，会重计算<code>old_log_prob</code>，相当于每个<code>mini batch</code>都要调用<code>compute_rollout_correction_and_rejection_mask</code>计算IS的weight和response make，然后添加到batch中（union），然后正常走正常的流程，比如调用<code>compute_policy_loss_vanilla</code>里会处理。</p><h4 id=slime的is实现><code>Slime</code>的IS实现<a hidden class=anchor aria-hidden=true href=#slime的is实现>#</a></h4><blockquote><p>可以参考<a href=https://github.com/THUDM/slime/pull/429>合入PR</a>，和verl类似，都实现了token/sequence/geometric mean级别的TIS、MIS等校准策略。</p></blockquote><ul><li><code>--use-train-infer-is</code>: Enable training-inference importance sampling</li><li><code>--train-infer-is-level</code>: Aggregation level (token/sequence/geometric)</li><li><code>--train-infer-is-mode</code>: Processing mode (truncate/mask/clip)</li><li><code>--train-infer-is-lower-bound</code>/<code>--train-infer-is-upper-bound</code>: Weight bounds</li><li><code>--train-infer-is-veto-threshold</code>: Catastrophic token threshold</li></ul><h3 id=routing-replay>Routing Replay<a hidden class=anchor aria-hidden=true href=#routing-replay>#</a></h3><blockquote><p><a href=https://arxiv.org/html/2510.11370v1>https://arxiv.org/html/2510.11370v1</a><br><a href=https://arxiv.org/html/2510.23027v1>https://arxiv.org/html/2510.23027v1</a> RSPO routing fluctuations</p></blockquote><p>Rollout Routing Replay 主要解决在专家混合（MoE）大模型中，因其路由机制在训练和推理阶段的行为不一致，导致训练和推理的 logprob产生比较大的差异进而引起强化学习（RL）训练不稳定甚至崩溃的问题。<br><img alt=router-discrepancy loading=lazy src="https://pic1.zhimg.com/80/v2-8cb9c9ea1fd6dc7b8bdd8e5cc68d9031_1440w.webp?source=2c26e567" data-zoomable></p><p>Rollout Routing Replay 会在模型进行推理时（Rollout 阶段），记录下每个 token 的 router 分布，然后在后续的训练过程中使用这些 router 分布进行计算。通过这种方式，强制训练过程模仿并对齐推理时的 router 行为，从而弥合两者之间的差异。</p><p><img alt=routing-replay loading=lazy src="https://picx.zhimg.com/80/v2-ee7c64bc4737825e244d334a3d1d04eb_1440w.webp?source=2c26e567" data-zoomable></p><blockquote><p>需要注意的是, GSPO论文中提到的Routing Replay, 是训练侧old和target策略之间，如果进行token-level的重要性采样，可能导致专家激活模式在新旧策略之间有差异，这种路由波动可能破坏训练稳定性，GSPO因为引入了seq-level的重要性采样，对单个token的专家波动不敏感，可以不需要routing replay（而GRPO不引入routing replay容易训崩）。而上面讨论的routing replay，主要还是解决训推不一致导致的路由波动带来的问题。</p></blockquote><h2 id=值得注意的其他研究>值得注意的其他研究<a hidden class=anchor aria-hidden=true href=#值得注意的其他研究>#</a></h2><h3 id=bf16切换为fp16>BF16切换为FP16<a hidden class=anchor aria-hidden=true href=#bf16切换为fp16>#</a></h3><p>有一篇更新的(2025/10/30)的来自Sea AI Lab的文章<a href=https://arxiv.org/html/2510.26788v1>Defeating the Training-Inference Mismatch via FP16</a> 提出了一个更简单的statement：<strong>我们不需要上面这些复杂的算法修正，只需要把精度从BF16切回FP16，不一致带来的问题就会解决</strong>。</p><p><img alt=fp16-bf16 loading=lazy src=https://arxiv.org/html/2510.26788v1/x1.png data-zoomable></p><p>研究认为，虽然FP16和BF16都使用总计16位，单比特分布不一样，尾数项和指数项的差异，导致FP16的精度为BF16的8倍。但带来的trade-off就是FP16能表征的数值范围就小的多，容易会overflow，可能需要额外的技巧比如loss scaling（反向之前把计算得到的loss乘大的缩放因子，反向时用放大了的loss计算梯度，落入fp16可表征的范围，利用梯度更新权重之前把梯度再还原回去）等稳定性技术进行缓解。</p><p>文章作者认为，在RL后训练的阶段，模型权重基本已经经过了pre-training，<strong>数值分布相对稳定</strong>，配合loss scalling等技巧fp16可以保证训练稳定。实际上这个是一个非常重要的场景预设，也是这个研究的出发点。因为BF16格式在LLM训练中，被广泛使用的原因，就是其既能够表征FP32的数值范围（指数位都是8），从而在预训练的时候，甚至不需要loss scaling这种增加复杂度的工程实现，也可能做到训练的稳定性，而且显存占用和全精度比也省一半。<strong>这种流行，是建立在pre-training对精度问题的“宽容度”比较高这个前提上的</strong>：</p><ul><li>在pre-training阶段，模型在海量的数据上学习通用的统计规律，pre-training过程有很强的鲁棒性，对数值噪声不敏感。<br>和pre-training阶段只关心“预测的next token是否准确”相对的，RL post-training更关注的是策略的更新幅度，例如PPO就依赖新旧策略的的ratio，通过限制ratio不要偏离1太远来保证训练稳定，<strong>这对训练时的精度需求就会更为严苛</strong>，如果由于精度表征问题，导致本来有差异的新旧策略，被截断成了ratio=1，那么策略就不更新了。而且针对KL散度约束这种设计到log计算的，精度差异带来的影响会在对数域被放大，那KL算不准，reward model给出奖励信号可能导致模型迅速过拟合到某个错误的pattern上，或者训练非常不稳定。</li></ul><p>因此这里的takeaway简单来说，就是在RL场景下，梯度的数值范围通常没有pre-training方差那么大，对梯度的精度要求更高，所谓为了获得比bf16高8倍的精度表征，用fp16 + loss scaling增加的工程成本是值得的。</p><p><img alt=offline-analysis loading=lazy src=https://arxiv.org/html/2510.26788v1/x2.png data-zoomable></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://pillumina.github.io/tags/deterministic/>Deterministic</a></li><li><a href=https://pillumina.github.io/tags/rl/>RL</a></li></ul><nav class=paginav><a class=next href=https://pillumina.github.io/posts/aiinfra/13-vllmascend-mc2/><span class=title>Next »</span><br><span>[vLLM-Ascend] MC2技术深度解析：从MoE架构到通信融合优化</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [Deterministic RL] deterministic问题的来源和相关工作总结 on x" href="https://x.com/intent/tweet/?text=%5bDeterministic%20RL%5d%20deterministic%e9%97%ae%e9%a2%98%e7%9a%84%e6%9d%a5%e6%ba%90%e5%92%8c%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c%e6%80%bb%e7%bb%93&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f14-deterministic-rl%2f&amp;hashtags=deterministic%2cRL"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [Deterministic RL] deterministic问题的来源和相关工作总结 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f14-deterministic-rl%2f&amp;title=%5bDeterministic%20RL%5d%20deterministic%e9%97%ae%e9%a2%98%e7%9a%84%e6%9d%a5%e6%ba%90%e5%92%8c%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c%e6%80%bb%e7%bb%93&amp;summary=%5bDeterministic%20RL%5d%20deterministic%e9%97%ae%e9%a2%98%e7%9a%84%e6%9d%a5%e6%ba%90%e5%92%8c%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c%e6%80%bb%e7%bb%93&amp;source=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f14-deterministic-rl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [Deterministic RL] deterministic问题的来源和相关工作总结 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f14-deterministic-rl%2f&title=%5bDeterministic%20RL%5d%20deterministic%e9%97%ae%e9%a2%98%e7%9a%84%e6%9d%a5%e6%ba%90%e5%92%8c%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c%e6%80%bb%e7%bb%93"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [Deterministic RL] deterministic问题的来源和相关工作总结 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f14-deterministic-rl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [Deterministic RL] deterministic问题的来源和相关工作总结 on whatsapp" href="https://api.whatsapp.com/send?text=%5bDeterministic%20RL%5d%20deterministic%e9%97%ae%e9%a2%98%e7%9a%84%e6%9d%a5%e6%ba%90%e5%92%8c%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c%e6%80%bb%e7%bb%93%20-%20https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f14-deterministic-rl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [Deterministic RL] deterministic问题的来源和相关工作总结 on telegram" href="https://telegram.me/share/url?text=%5bDeterministic%20RL%5d%20deterministic%e9%97%ae%e9%a2%98%e7%9a%84%e6%9d%a5%e6%ba%90%e5%92%8c%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c%e6%80%bb%e7%bb%93&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f14-deterministic-rl%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [Deterministic RL] deterministic问题的来源和相关工作总结 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5bDeterministic%20RL%5d%20deterministic%e9%97%ae%e9%a2%98%e7%9a%84%e6%9d%a5%e6%ba%90%e5%92%8c%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c%e6%80%bb%e7%bb%93&u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f14-deterministic-rl%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul><div class=related-posts><div class=related-series><h3>同系列文章</h3><ul><li><a href=/posts/aiinfra/11-flashattention/>[AIInfra] FlashAttention 深度解析：从数学原理到工程实现</a>
<span class=meta>2025-09-15
· 11 min read</span></li></ul></div><div class=related-tags><h3>相关文章</h3><ul><li><a href=/posts/aiinfra/02-slime/>[RL4LLM] 异步RL框架: Slime</a>
<span class=meta>2025-08-07
· 15 min read
· Tags: framework, LLM, RL</span></li><li><a href=/posts/aiinfra/03-areal/>[RL4LLM] 异步RL框架: Areal</a>
<span class=meta>2025-08-07
· 23 min read
· Tags: framework, LLM, RL</span></li></ul></div></div></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://pillumina.github.io/>CctoctoFX</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div class=reading-progress-bar></div><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelector(".reading-progress-bar");if(!t)return;const n=document.querySelector(".post-single");if(!n)return;function s(){const e=n.getBoundingClientRect(),s=e.height,o=window.innerHeight,i=window.scrollY||window.pageYOffset,a=i/(s-o)*100;t.style.width=`${Math.min(100,Math.max(0,a))}%`}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){s(),e=!1}),e=!0)}),s()}),document.addEventListener("DOMContentLoaded",function(){mediumZoom("article img:not(.nozoom)",{margin:24,background:"var(--theme)",scrollOffset:0})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>