<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AI Infra on CctoctoFX</title>
    <link>https://pillumina.github.io/posts/aiinfra/</link>
    <description>Recent content in AI Infra on CctoctoFX</description>
    <image>
      <title>CctoctoFX</title>
      <url>https://pillumina.github.io/imgs/icon_head.png</url>
      <link>https://pillumina.github.io/imgs/icon_head.png</link>
    </image>
    <generator>Hugo -- 0.148.2</generator>
    <language>en</language>
    <lastBuildDate>Thu, 14 Aug 2025 11:30:12 +0800</lastBuildDate>
    <atom:link href="https://pillumina.github.io/posts/aiinfra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[VeRL] AgentLoop源码走读</title>
      <link>https://pillumina.github.io/posts/aiinfra/09-verl-agentloop/</link>
      <pubDate>Thu, 14 Aug 2025 11:30:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/09-verl-agentloop/</guid>
      <description>&lt;p&gt;最近 RL sys 圈子的吴锡斌老师在 verl 上设计了将 rollout 与 tool 调用解耦的 AgentLoop，实现了自由灵活的 mutli-turn RL。在每个 AgentLoop 内部，rollout engine 只对外提供一个 token-in-token-out 的接口，而 tool 调用则通过 &lt;code&gt;ToolAgentLoop&lt;/code&gt; 来实现。我个人比较喜欢这样解耦的设计，同时，AgentLoop 的代码结构也比较清晰。我个人学习了一次整个代码后，觉着 AgentLoop 的设计甚是不错，但是 &lt;code&gt;ActorRolloutRefWorker&lt;/code&gt; 的历史包袱还是很重。&lt;/p&gt;
&lt;p&gt;本文简单分析了 agent loop 的源码，并给出了一些自己的看法。&lt;/p&gt;
&lt;p&gt;如果我们把整个 &lt;code&gt;ActorRolloutRefWorker&lt;/code&gt; 当做一个 &lt;code&gt;sgl.Engine&lt;/code&gt; 的话，AgentLoop 里面包装的两层 &lt;code&gt;AsyncSGLangServer&lt;/code&gt; 和 &lt;code&gt;AsyncLLMServerManager&lt;/code&gt;。&lt;code&gt;AsyncSGLangServer&lt;/code&gt; 相当于在 &lt;code&gt;sgl.Engine&lt;/code&gt; 上包装了 &lt;code&gt;fastapi&lt;/code&gt; 成了 server，而 &lt;code&gt;AsyncLLMServerManager&lt;/code&gt; 是在 server 上包了一层 router 做 load balance，相当于 sglang 的 router。这两层设计都是合理的，主要麻烦的是 &lt;code&gt;ActorRolloutRefWorker&lt;/code&gt;，层层调用，最后一共经过 7 个 class 才调到 &lt;code&gt;sgl.Engine&lt;/code&gt;，最近 verl 团队也在致力于对这块 worker class 的重构，敬请期待。最后，&lt;code&gt;AgentLoopManager&lt;/code&gt;，&lt;code&gt;AgentLoopWorker&lt;/code&gt; 和 &lt;code&gt;AgentLoop&lt;/code&gt; 这三层，我觉得 &lt;code&gt;AgentLoopWorker&lt;/code&gt; 可能未必有必要，其他两层挺合理的。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[VeRL] 参数速览</title>
      <link>https://pillumina.github.io/posts/aiinfra/05-verl-params/</link>
      <pubDate>Thu, 14 Aug 2025 10:20:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/05-verl-params/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;VeRL框架的参数众多，基于当前（2025.8.5）主线分支整理，附带了相关的理解，一些描述不一定完全正确，供学习参考。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;batch-size&#34;&gt;Batch Size&lt;/h2&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;参数名称&lt;/th&gt;
          &lt;th&gt;详细解释&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;data.train_batch_size&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;作用&lt;/strong&gt;：定义了单次训练发送给 Rollout Engine 的样本数量，也即这是在每个 PPO 迭代开始时，从训练数据集中采样的提示 （Prompt）数量。&lt;br&gt;&lt;br&gt;&lt;strong&gt;详细解释&lt;/strong&gt;：这个值是 RL 训练中的基本样本数量。例如，设置为 1024 意味着在一次迭代中会：&lt;br&gt;1. 从数据集中随机抽取 1024 个 prompt。&lt;br&gt; 2. 将这 1024 个 prompt 发送给当前的 Rollout Engine 中，从而得到 1024 组完整的 trajectories（prompt, response）。&lt;br&gt;3. 接下来，这 1024 个 trajectories 进行经验计算（make experience），后续用于 Actor 和 Critic 模型的更新。&lt;br&gt;&lt;br&gt;&lt;strong&gt;影响与权衡&lt;/strong&gt;：影响总共训练的样本量。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;data.val_batch_size&lt;/code&gt; （Deprecated)&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;作用&lt;/strong&gt;：在 Validation 阶段使用的批次大小。&lt;br&gt;&lt;br&gt;&lt;strong&gt;详细解释&lt;/strong&gt;：这与 &lt;code&gt;train_batch_size&lt;/code&gt; 类似，但仅用于评估模型性能，不参与训练。如果设置为 &lt;code&gt;null&lt;/code&gt;，会使用验证集的大小作为默认值。Note: 已经deprecated，推荐设置为 null。此时，整个 validation dataset 一次性发给 SGLang engines，自行进行内存管理。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;actor_rollout_ref.actor.ppo_mini_batch_size&lt;/code&gt; &lt;br&gt; &lt;code&gt;critic.ppo_mini_batch_size&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;作用&lt;/strong&gt;：定义了 PPO 训练更新中的 mini-batch 大小。&lt;br&gt;&lt;br&gt;&lt;strong&gt;详细解释&lt;/strong&gt;：&lt;code&gt;data.train_batch_size&lt;/code&gt; 收集到的全部经验数据将被分割成多个 mini-batch，每块的大小就是 &lt;code&gt;ppo_mini_batch_size&lt;/code&gt;。模型每处理完一个 mini-batch，才会进行一次参数更新。&lt;br&gt;例如，如果 &lt;code&gt;train_batch_size = 1024&lt;/code&gt;，&lt;code&gt;ppo_mini_batch_size = 256&lt;/code&gt;，那么在一个 PPO Epoch 中，模型会进行 &lt;code&gt;1024 / 256 = 4&lt;/code&gt; 次参数更新。&lt;br&gt;&lt;br&gt;&lt;strong&gt;影响与权衡&lt;/strong&gt;：增大 mini-batch，单次更新的梯度更稳定，但更新频率更低，更新次数减少。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu&lt;/code&gt; &lt;br&gt; &lt;code&gt;critic.ppo_micro_batch_size_per_gpu&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;作用&lt;/strong&gt;：定义了在单个 GPU 上进行一次 forward/backward 的数据大小。&lt;br&gt;&lt;br&gt;&lt;strong&gt;详细解释&lt;/strong&gt;：这是实现梯度累积的核心参数。mini-batch 会被再次切分为若干个 micro-batch。例如，在单卡上，&lt;code&gt;ppo_mini_batch_size = 256&lt;/code&gt;，&lt;code&gt;ppo_micro_batch_size_per_gpu = 32&lt;/code&gt;，那么梯度累积的步数就是 &lt;code&gt;256 / 32 = 8&lt;/code&gt;。这意味着模型会运行 8 次 forward 得到 loss，然后 backward 的到 gradient。每次处理 32 个样本，直到累积完整个 mini-batch 计算出的梯度。此时，使用累积的总梯度，对模型参数进行一次更新（&lt;code&gt;optimizer.step()&lt;/code&gt;）。这个值必须根据显存大小来严格调整，是防止 OOM 的关键。&lt;br&gt;&lt;br&gt;&lt;strong&gt;影响与权衡&lt;/strong&gt;：增大此值，减少了梯度累积的次数，可以提高训练的吞吐量，增大显存消耗。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;actor_rollout_ref.actor.ppo_micro_batch_size&lt;/code&gt; &lt;br&gt; &lt;code&gt;critic.ppo_micro_batch_size&lt;/code&gt;（Deprecated)&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;作用&lt;/strong&gt;：已弃用，被 &lt;code&gt;per_gpu&lt;/code&gt; 版本取代，因为它能更好地适应分布式训练环境。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;dynamic-batch-size&#34;&gt;Dynamic Batch Size&lt;/h2&gt;
&lt;p&gt;当样本长度差异很大时，按样本数量划分批次可能导致不同批次的计算量极不均衡，而基于 token 总数来控制 batch size 是一种平衡每个 batch 训练时间的方案。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[SGLang] 后端代码速览</title>
      <link>https://pillumina.github.io/posts/aiinfra/06-sglang-backend/</link>
      <pubDate>Wed, 13 Aug 2025 10:30:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/06-sglang-backend/</guid>
      <description>&lt;p&gt;本文档为开发者提供 SGLang 后端代码的代码梳理，按照一个请求从输入到最后输出的顺序进行讲解。下图简要介绍了这一流程：&lt;/p&gt;
&lt;div style=&#34;text-align: center; width: 100%; margin: 0 auto;&#34;&gt;
    &lt;img src=&#34;https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/raw/main/sglang/code-walk-through/sglang-architecture.svg&#34; alt=&#34;SGLang 架构图&#34; style=&#34;width: 100%; height: auto;&#34;&gt;
&lt;/div&gt;
&lt;p&gt;具体而言，请求的处理过程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;用户启动 Server ，初始化 FastAPI App、TokenizerManager、DetokenizerManager 和 Scheduler，每个组件运行各自的无限事件循环（infinite event loop）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用户向 FastAPI Server 发送 &lt;code&gt;/v1/chat/completions&lt;/code&gt; 请求，Server 通过 &lt;code&gt;v1_chat_completions&lt;/code&gt; endpoint 将请求转发到 TokenizerManager。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;v1_chat_completions&lt;/code&gt; 函数将请求转换为 &lt;code&gt;ChatCompletionRequest&lt;/code&gt;，再转换为 &lt;code&gt;GenerateReqInput&lt;/code&gt;，并调用 TokenizerManager 的 &lt;code&gt;generate_request&lt;/code&gt; 方法。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TokenizerManager 对请求进行 tokenization，并以 Python 对象（&lt;code&gt;pyobj&lt;/code&gt;）形式将其转发给 Scheduler，同时调用 TokenizerManager 的 &lt;code&gt;_wait_one_response&lt;/code&gt; 方法。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scheduler 在事件循环 &lt;code&gt;event_loop_normal&lt;/code&gt; 中处理请求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scheduler 通过 &lt;code&gt;recv_requests&lt;/code&gt; 接收请求，调用 &lt;code&gt;process_input_requests&lt;/code&gt; 处理输入，通过 &lt;code&gt;handle_generate_request&lt;/code&gt; 管理生成请求的逻辑，并将其加入 &lt;code&gt;waiting_queue&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;从 &lt;code&gt;waiting_queue&lt;/code&gt; 中，Scheduler 使用 &lt;code&gt;get_next_batch_to_run&lt;/code&gt; 为即将处理的请求创建 &lt;code&gt;ScheduleBatch&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;Scheduler 执行 &lt;code&gt;run_batch&lt;/code&gt; 函数，将 &lt;code&gt;ScheduleBatch&lt;/code&gt; 转换为 &lt;code&gt;ModelWorkerBatch&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;Scheduler 调用 TpModelWorker 的 &lt;code&gt;forward_batch_generation&lt;/code&gt;，等待 &lt;code&gt;logits_output&lt;/code&gt; 和 &lt;code&gt;next_token_ids&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;TpModelWorker 初始化 &lt;code&gt;ForwardBatch&lt;/code&gt;，将其转发至 ModelRunner，并等待 &lt;code&gt;logits_output&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;ModelRunner 处理 &lt;code&gt;ForwardBatch&lt;/code&gt;，调用 &lt;code&gt;forward_extend&lt;/code&gt; 执行模型的前向计算（forward pass）。&lt;/li&gt;
&lt;li&gt;模型通过 &lt;code&gt;AttentionBackend&lt;/code&gt; 加速生成 logits，返回给 ModelRunner，进而返回给 TpModelWorker。&lt;/li&gt;
&lt;li&gt;TpModelWorker 从 ModelRunner 接收 &lt;code&gt;logits_output&lt;/code&gt;，调用 ModelRunner 的 &lt;code&gt;sample&lt;/code&gt; 方法生成 &lt;code&gt;next_token_ids&lt;/code&gt;，并将其发送回 Scheduler。&lt;/li&gt;
&lt;li&gt;Scheduler 通过 &lt;code&gt;process_batch_result&lt;/code&gt; 处理批次结果，使用 &lt;code&gt;tree_cache.cache_finished_req(req)&lt;/code&gt; 缓存请求，并通过 &lt;code&gt;check_finished&lt;/code&gt; 验证完成状态。对于未完成的请求，Scheduler 继续其事件循环，直到这个请求满足结束条件；对于已完成的请求，则转发到 Scheduler 的 &lt;code&gt;stream_output&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;在 &lt;code&gt;stream_output&lt;/code&gt; 函数中，Scheduler 处理输出，将其包装成 &lt;code&gt;BatchTokenIDOut&lt;/code&gt;，并发送给 DetokenizerManager。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DetokenizerManager 在其事件循环中接收 &lt;code&gt;BatchTokenIDOut&lt;/code&gt;，处理后生成 &lt;code&gt;BatchStrOut&lt;/code&gt; 并返回给 TokenizerManager。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[RL4LLM] 异步RL框架: Slime</title>
      <link>https://pillumina.github.io/posts/aiinfra/02-slime/</link>
      <pubDate>Thu, 07 Aug 2025 17:10:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/02-slime/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/THUDM/slime&#34;&gt;https://github.com/THUDM/slime&lt;/a&gt;&lt;br&gt;
一个异步实现但是非完全异步的RL框架&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;总体架构&#34;&gt;总体架构&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;从源码模块划分，有三大核心模块：
&lt;ul&gt;
&lt;li&gt;training（Megatron）：主训练流程，负责模型参数更新。&lt;/li&gt;
&lt;li&gt;rollout（SGLang + router）：负责采样、奖励/验证生成，产生训练数据。&lt;/li&gt;
&lt;li&gt;data buffer：桥接训练与采样，管理数据流、缓存与生成方式。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;分布式调度：关于资源分配、actor启动、任务调度都由于Ray管理，支持异步训练和采样&lt;/li&gt;
&lt;li&gt;插件机制：支持自定义buffer、模型、模型格式转换（mbridge）&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;mermaid&#34;&gt;
  flowchart LR
    subgraph Ray[Ray 分布式调度]
        A1[Actor Group&amp;lt;br&amp;gt;训练 Actor]
        A2[Rollout Group&amp;lt;br&amp;gt;采样/生成 Actor]
        A3[Placement Group&amp;lt;br&amp;gt;资源分配]
    end
    subgraph Training[Training &amp;lt;Megatron&amp;gt;]
        T1[模型训练]
        T2[权重同步]
        T3[评估/保存]
    end
    subgraph Rollout[Rollout &amp;lt;SGLang+Router&amp;gt;]
        R1[采样/生成]
        R2[奖励模型]
        R3[过滤器]
    end
    subgraph Buffer[Data Buffer]
        B1[数据缓存]
        B2[数据流转]
        B3[Offload/Onload]
    end
    subgraph Plugins[插件机制]
        P1[Buffer 插件]
        P2[Model 插件]
        P3[mbridge 格式转换]
    end

    A1--&amp;gt;|训练数据|B1
    A2--&amp;gt;|生成数据|B1
    B1--&amp;gt;|数据流|A1
    B1--&amp;gt;|数据流|A2
    A1--&amp;gt;|权重同步|A2
    A1--&amp;gt;|评估/保存|T3
    A2--&amp;gt;|采样/奖励/过滤|R1
    R1--&amp;gt;|奖励|R2
    R1--&amp;gt;|过滤|R3
    B1--&amp;gt;|插件扩展|P1
    A1--&amp;gt;|模型扩展|P2
    A1--&amp;gt;|格式转换|P3
    A3--&amp;gt;|资源分配|A1
    A3--&amp;gt;|资源分配|A2
&lt;/pre&gt;

&lt;h2 id=&#34;各模块视角的关系图&#34;&gt;各模块视角的关系图&lt;/h2&gt;
&lt;h3 id=&#34;slimerollout-组件图&#34;&gt;slime/rollout 组件图&lt;/h3&gt;
&lt;p&gt;rollout 负责采样、奖励、过滤，支持多种采样/奖励/过滤策略。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[RL4LLM] 异步RL框架: Areal</title>
      <link>https://pillumina.github.io/posts/aiinfra/03-areal/</link>
      <pubDate>Thu, 07 Aug 2025 14:40:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/03-areal/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/inclusionAI/AReaL&#34;&gt;https://github.com/inclusionAI/AReaL&lt;/a&gt;&lt;br&gt;
纯异步RL方案&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;异步ppo训练调用流程&#34;&gt;异步PPO训练调用流程&lt;/h2&gt;
&lt;pre class=&#34;mermaid&#34;&gt;
  graph TD
    A[用户执行: examples/run_async_ppo.sh] --&amp;gt; B[training/main_async_ppo.py]
    B --&amp;gt; C[AsyncPPOMATHConfig配置解析]
    C --&amp;gt; D[training/utils.py: run_experiment]
    
    D --&amp;gt; E[Ray初始化]
    E --&amp;gt; F[exp_cfg.initial_setup]
    F --&amp;gt; G[AsyncRLExperimentConfig.initial_setup]
    G --&amp;gt; H[创建ExperimentConfig]
    
    H --&amp;gt; I[启动Workers]
    I --&amp;gt; J[MasterWorker]
    I --&amp;gt; K[ModelWorker]
    I --&amp;gt; L[GenerationServer]
    I --&amp;gt; M[GserverManager]
    I --&amp;gt; N[RolloutWorker]
    
    %% MasterWorker训练流程
    J --&amp;gt; J1[MasterWorker._poll_async]
    J1 --&amp;gt; J2[FunctionExecutor.execute_step]
    J2 --&amp;gt; J3[执行数据流图遍历]
    J3 --&amp;gt; J4[发送训练请求到ModelWorker]
    
    %% ModelWorker处理流程
    K --&amp;gt; K1[ModelWorker._poll]
    K1 --&amp;gt; K2[接收MasterWorker请求]
    K2 --&amp;gt; K3[处理训练/推理请求]
    K3 --&amp;gt; K4[执行模型前向/反向传播]
    
    %% Rollout流程
    N --&amp;gt; N1[RolloutWorker._poll_async]
    N1 --&amp;gt; N2[load_next_data]
    N2 --&amp;gt; N3[allocate_new_rollout]
    N3 --&amp;gt; N4[agent.collect_trajectory]
    N4 --&amp;gt; N5[env.step计算奖励]
    N5 --&amp;gt; N6[推送数据到训练端]
    
    %% 生成服务器流程
    L --&amp;gt; L1[GenerationServer._poll]
    L1 --&amp;gt; L2[启动SGLang子进程]
    L2 --&amp;gt; L3[处理生成请求]
    
    %% 生成服务器管理器
    M --&amp;gt; M1[GserverManager._poll]
    M1 --&amp;gt; M2[HTTP服务线程]
    M2 --&amp;gt; M3[请求调度和权重更新]
    
    %% 数据流
    N6 --&amp;gt; O[stream_dataset.py]
    O --&amp;gt; J4
    
    %% 异步通信
    J4 -.-&amp;gt;|异步请求| K2
    N3 -.-&amp;gt;|HTTP请求| M2
    M2 -.-&amp;gt;|调度请求| L3
    
    %% 权重更新
    K4 --&amp;gt; P[参数更新]
    P --&amp;gt; Q[权重同步]
    Q --&amp;gt; M3
    M3 --&amp;gt; R[更新生成服务器权重]
    
    style A fill:#e1f5fe
    style J fill:#f3e5f5
    style K fill:#e8f5e8
    style L fill:#fff3e0
    style M fill:#fce4ec
    style N fill:#f1f8e9
&lt;/pre&gt;

&lt;h3 id=&#34;用户入口到配置解析&#34;&gt;用户入口到配置解析&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;examples/run_async_ppo.sh&lt;/code&gt; → &lt;code&gt;training/main_async_ppo.py&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>昇腾超节点CloudMatrix384论文拆解</title>
      <link>https://pillumina.github.io/posts/aiinfra/01-ascend-cloudmatrix/</link>
      <pubDate>Thu, 07 Aug 2025 10:40:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/01-ascend-cloudmatrix/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;6.19发布的CloudMatrix384论文拆解，从宏观到基础概念&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;核心指标和计算方式&#34;&gt;核心指标和计算方式&lt;/h2&gt;
&lt;h3 id=&#34;tpot-time-per-output-token&#34;&gt;&lt;strong&gt;TPOT (Time Per Output Token)&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;公式&lt;/strong&gt;： $$TPOT= \frac{Decode总耗时}{生成Token数量}$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;测量方式&lt;/strong&gt;： 从第一个输出Token开始计时，到生成结束（含MoE通信/KV读取）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;为什么重要&lt;/strong&gt;： 直接决定用户体验（如Chatbot响应速度），论文要求 &lt;strong&gt;&amp;lt;50ms&lt;/strong&gt;（严格模式&amp;lt;15ms）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深层意义&lt;/strong&gt;： 反映&lt;strong&gt;系统通信+计算综合能力&lt;/strong&gt;，EP320下TPOT=42ms证明UB网络突破MoE通信墙&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;计算效率-tokenss-per-tflops&#34;&gt;&lt;strong&gt;计算效率 (Tokens/s per TFLOPS)&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;公式&lt;/strong&gt;： $$计算效率=\frac {吞吐量(tokens/s)} {NPU峰值算力(TFLOPS)}$$​&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;论文数据&lt;/strong&gt;：&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;阶段&lt;/th&gt;
          &lt;th&gt;值&lt;/th&gt;
          &lt;th&gt;对比基准&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Prefill&lt;/td&gt;
          &lt;td&gt;4.45&lt;/td&gt;
          &lt;td&gt;超NVIDIA H100+SGLang(3.8)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Decode&lt;/td&gt;
          &lt;td&gt;1.29&lt;/td&gt;
          &lt;td&gt;超NVIDIA H800+DeepSeek(0.9)&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;为什么重要&lt;/strong&gt;： 揭示&lt;strong&gt;硬件利用率&lt;/strong&gt;，1.0以上表明软硬件协同极致优化&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深层意义&lt;/strong&gt;： Decode阶段1.29 → 昇腾910的Cube引擎利用率达 &lt;strong&gt;86%&lt;/strong&gt;（传统GPU仅60%)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;缓存访问延迟-kv-cache-access-latency&#34;&gt;&lt;strong&gt;缓存访问延迟 (KV Cache Access Latency)&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;公式&lt;/strong&gt;： $$延迟=TMMU_{查询}+TUB_{传输}+TDRAM_{读取}​$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;论文数据&lt;/strong&gt;：&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;场景&lt;/th&gt;
          &lt;th&gt;延迟&lt;/th&gt;
          &lt;th&gt;对比传统&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;本地HBM命中&lt;/td&gt;
          &lt;td&gt;0.2μs&lt;/td&gt;
          &lt;td&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;远程DRAM访问(UB)&lt;/td&gt;
          &lt;td&gt;1.5μs&lt;/td&gt;
          &lt;td&gt;&amp;gt;10μs (PCIe+IB)&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;为什么重要&lt;/strong&gt;： 长上下文推理中&lt;strong&gt;70%时间花在KV缓存访问&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深层意义&lt;/strong&gt;： UB统一内存将远程访问性能提升至&lt;strong&gt;近本地水平&lt;/strong&gt;，支撑百万Token上下文。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;专家并行扩展性-ep-degree&#34;&gt;&lt;strong&gt;专家并行扩展性 (EP Degree)&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：单个MoE层可分布的专家数量&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;论文突破&lt;/strong&gt;：&lt;strong&gt;EP320&lt;/strong&gt;（每个昇腾Die托管1个专家）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;支撑公式&lt;/strong&gt;： $$可扩展性=\frac {UB总带宽}{单个专家通信需求}$$ $$EPmax=\frac {384×392GB/s} {8B/token×10^6token/s}=320$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;为什么重要&lt;/strong&gt;： EP&amp;gt;100时传统网络崩溃，EP320证明UB突破通信可扩展性极限&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;int8量化收益&#34;&gt;&lt;strong&gt;INT8量化收益&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;公式&lt;/strong&gt;：$$ 加速比=\frac {FP16吞吐}{INT8吞吐}×精度保持率$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;论文数据&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;吞吐提升：&lt;strong&gt;1.8倍&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;精度损失：&lt;strong&gt;&amp;lt;0.5%&lt;/strong&gt;（16个基准测试）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;为什么重要&lt;/strong&gt;： Decode阶段&lt;strong&gt;内存带宽减少50%&lt;/strong&gt;，解决NPU的“内存墙”问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;qa辅助理解&#34;&gt;QA辅助理解&lt;/h3&gt;
&lt;h4 id=&#34;为什么用tpot而非qps&#34;&gt;&lt;strong&gt;为什么用TPOT而非QPS？&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;TPOT剥离Batch Size影响，&lt;strong&gt;纯粹衡量单次生成效率&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;更直观反映SLA（用户感知的延迟）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;为什么强调计算效率而非绝对吞吐&#34;&gt;&lt;strong&gt;为什么强调计算效率而非绝对吞吐？&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;排除工艺优势（7nm vs 5nm），&lt;strong&gt;聚焦架构创新价值&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;1.29 tokens/s/TFLOPS → 证明UB+LEP设计优于NVLink+GPU&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;为什么测量远程dram访问延迟&#34;&gt;&lt;strong&gt;为什么测量远程DRAM访问延迟？&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;验证&lt;strong&gt;内存池化&lt;/strong&gt;的实际效果，这是打破“内存墙”的核心&lt;/li&gt;
&lt;li&gt;1.5μs延迟 → 实现“全集群如单机”的硬件基础&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;超节点架构&#34;&gt;超节点架构&lt;/h2&gt;
&lt;h3 id=&#34;三级网络平面的物理隔离&#34;&gt;三级网络平面的物理隔离&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;硬件隔离原理&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>[VeRL] Multi-Turn RL训练源码走读（2）</title>
      <link>https://pillumina.github.io/posts/aiinfra/08-verl-multiturn-2/</link>
      <pubDate>Sun, 03 Aug 2025 17:30:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/08-verl-multiturn-2/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;在 Part 1 中，我们介绍了 verl 的初始化过程，我们进一步介绍 verl 的训练过程，包括rollout部分、make experience部分以及training部分。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;在 GRPO 中，单个 step 包含四个阶段：load data -&amp;gt; rollout -&amp;gt; make experience -&amp;gt; update model。区别于前一节的详述，本节会使用伪代码结合源码的方式进行阐述。&lt;/p&gt;
&lt;pre class=&#34;mermaid&#34;&gt;
  flowchart LR
subgraph W2[&amp;#34;Initialize&amp;#34;]
WP[Process Data] --&amp;gt; A
direction TB D1[Data Prepare] --&amp;gt; A
A[TaskRunner] --&amp;gt; B1[RayPPOTrainer]
B1 --&amp;gt; Workers

    subgraph Workers[&amp;#34;Workers&amp;#34;]
        direction TB
                WA[ActorRolloutWorker] --&amp;gt; WD[FSDP Engine]
        WB[CriticWorker] --&amp;gt; WD
        WC[RewardModelWorker] --&amp;gt; WD
        WD --&amp;gt; WE[SGLang Engine]
    end
    
    Workers --&amp;gt; C1[Hybrid Engine]
end 

subgraph W3[&amp;#34;Train Loop&amp;#34;]
    direction TB
    E[DataLoader] --&amp;gt; RolloutBox
    
    subgraph RolloutBox[&amp;#34;Rollout&amp;#34;]
        F1[Prepare Data] --&amp;gt; F2[SGLang Async Rollout]
        F2 --&amp;gt; F3[Multi-turn Chat Process]
    end
    
    RolloutBox --&amp;gt; ExpBox
    
    subgraph ExpBox[&amp;#34;Make Experience&amp;#34;]
        G1[Recompute Log Probs] --&amp;gt; G2[Compute Reward]
        G2 --&amp;gt; G3[Compute Advantage]
    end
    
    ExpBox --&amp;gt; UpdateBox
    
    subgraph UpdateBox[&amp;#34;Train The Model&amp;#34;]
        H1[Load FSDP Model Weight] --&amp;gt; H2[Compute Gradient]
        H2 --&amp;gt; H3[Weights Update]
        H3 --&amp;gt; H4[Sync Weights]
    end
    
    UpdateBox --&amp;gt; E
end

W2 --&amp;gt; W3
&lt;/pre&gt;

&lt;h2 id=&#34;数据加载与预处理&#34;&gt;数据加载与预处理&lt;/h2&gt;
&lt;p&gt;verl 通过 &lt;code&gt;DataProto&lt;/code&gt; 和 &lt;code&gt;RLHFDataset&lt;/code&gt; 来实现数据处理。具体来说，在 &lt;a href=&#34;https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L193&#34;&gt;&lt;code&gt;main_ppo.py&lt;/code&gt;&lt;/a&gt; 中，我们观察这个函数：&lt;/p&gt;</description>
    </item>
    <item>
      <title>[VeRL] Multi-Turn RL训练源码走读（1）</title>
      <link>https://pillumina.github.io/posts/aiinfra/07-verl-multiturn-1/</link>
      <pubDate>Sun, 03 Aug 2025 15:30:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/07-verl-multiturn-1/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;该part主要聚焦相关模块初始化部分&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;还是以 verl 出发，分析其 end to end mutli-turn RL 训练的全过程。整体上，我希望覆盖所有重要的 class 以及函数，更细粒度的代码不再展开。&lt;/p&gt;
&lt;p&gt;为了前后内容的一致性，基于 &lt;a href=&#34;https://github.com/volcengine/verl/commit/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39&#34;&gt;76f63cffa5&lt;/a&gt; 的 commit 进行分析。&lt;/p&gt;
&lt;p&gt;虽然本文以分析 verl 的代码为主，写完之后我才意识到，系统设计问题是非常通用的。诸如“log probs 重计算”，“Rollout Engine 显存管理”等等系统设计，是各大 RL 框架都需要考虑的核心问题。&lt;/p&gt;
&lt;p&gt;此外因为最近在学习SGLang的实现，本文的推理后端选择的是SGLang展开分析。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;整个训练的示意图如下，我们会具体展开每个部分。&lt;/p&gt;
&lt;pre class=&#34;mermaid&#34;&gt;
  flowchart LR
subgraph W2[&amp;#34;Initialize&amp;#34;]
WP[Process Data] --&amp;gt; A
direction TB D1[Data Prepare] --&amp;gt; A
A[TaskRunner] --&amp;gt; B1[RayPPOTrainer]
B1 --&amp;gt; Workers

    subgraph Workers[&amp;#34;Workers&amp;#34;]
        direction TB
                WA[ActorRolloutWorker] --&amp;gt; WD[FSDP Engine]
        WB[CriticWorker] --&amp;gt; WD
        WC[RewardModelWorker] --&amp;gt; WD
        WD --&amp;gt; WE[SGLang Engine]
    end
    
    Workers --&amp;gt; C1[Hybrid Engine]
end

subgraph W3[&amp;#34;Train Loop&amp;#34;]
    direction TB
    E[DataLoader] --&amp;gt; RolloutBox
    
    subgraph RolloutBox[&amp;#34;Rollout&amp;#34;]
        F1[Prepare Data] --&amp;gt; F2[SGLang Async Rollout]
        F2 --&amp;gt; F3[Multi-turn Chat Process]
    end
    
    RolloutBox --&amp;gt; ExpBox
    
    subgraph ExpBox[&amp;#34;Make Experience&amp;#34;]
        G1[Recompute Log Probs] --&amp;gt; G2[Compute Reward]
        G2 --&amp;gt; G3[Compute Advantage]
    end
    
    ExpBox --&amp;gt; UpdateBox
    
    subgraph UpdateBox[&amp;#34;Train The Model&amp;#34;]
        H1[Load FSDP Model Weight] --&amp;gt; H2[Compute Gradient]
        H2 --&amp;gt; H3[Weights Update]
        H3 --&amp;gt; H4[Sync Weights]
    end
    
    UpdateBox --&amp;gt; E
end

W2 --&amp;gt; W3
&lt;/pre&gt;

&lt;h2 id=&#34;数据预处理&#34;&gt;&lt;strong&gt;数据预处理&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;以 &lt;a href=&#34;https://huggingface.co/datasets/openai/gsm8k&#34;&gt;GSM8K&lt;/a&gt; 为例，预处理脚本是 &lt;code&gt;examples/data_preprocess/gsm8k_multiturn_w_tool.py&lt;/code&gt;。整个脚本只做了经典的 huggingface datasets mapping，核心逻辑如下：&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI Infra：颠覆性创新，还是经典工程范式的华丽转身？</title>
      <link>https://pillumina.github.io/posts/aiinfra/04-aiinfra-thinking/</link>
      <pubDate>Fri, 01 Aug 2025 10:05:12 +0800</pubDate>
      <guid>https://pillumina.github.io/posts/aiinfra/04-aiinfra-thinking/</guid>
      <description>&lt;p&gt;近期看到一些关于传统基础设施（Traditional Infrastructure）与人工智能基础设施（AI Infrastructure，尤其大模型领域）差异的评论。其核心观点直指两者间的巨大鸿沟：许多精于网络、计算、存储等传统领域的工程师，在面对GPU集群、KV Cache管理、3D并行等全新概念时，常感过往经验难以直接套用，甚至产生踏入一个全然不同技术体系的“割裂感”。&lt;/p&gt;
&lt;p&gt;这些看法颇具代表性，精准捕捉了工程师初探AI Infra时的普遍印象：&lt;strong&gt;陌生、高门槛、范式迥异&lt;/strong&gt;。本文旨在分享我对此的一些初步思考：AI Infra究竟是颠覆传统的全新体系，抑或是既有Infra经验在智能时代的一次深度演化？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;（&lt;em&gt;免责声明：本文纯属个人观点，旨在抛砖引玉，欢迎指正谬误！&lt;/em&gt;）&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;我的核心论点：AI Infra并非平地起高楼，它实质上是传统Infra工程智慧在新场景下的重构与系统性延展。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;表象差异新术语与新挑战带来的视觉冲击&#34;&gt;表象差异：新术语与新挑战带来的“视觉冲击”&lt;/h3&gt;
&lt;p&gt;乍看之下，AI Infra与传统Infra确实分野明显：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;核心任务不同：&lt;/strong&gt; 传统Infra聚焦于处理海量Web请求（毫秒级、无状态）、保障数据持久化存储、实现分布式服务协调。而AI Infra（尤以大模型为甚）则围绕&lt;strong&gt;GPU驱动的模型训练/推理&lt;/strong&gt;、&lt;strong&gt;KV Cache的高效管理&lt;/strong&gt;、&lt;strong&gt;百亿/千亿级参数的分布式执行框架&lt;/strong&gt;展开。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;请求形态迥异：&lt;/strong&gt; Web请求追求瞬时响应（毫秒级）、天然无状态。大模型（LLM）推理则常承载&lt;strong&gt;持续的会话交互&lt;/strong&gt;（秒级乃至更长，随上下文窗口扩展而递增），需&lt;strong&gt;动态维护细粒度的Token级状态&lt;/strong&gt;（KV Cache）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;技术栈迭代：&lt;/strong&gt; 熟悉的Kubernetes + Docker堆栈旁，涌现出GPU硬件抽象、vLLM、DeepSpeed、FlashAttention、Triton、NCCL等&lt;strong&gt;专为AI设计、名号“高深”的组件&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由此观之，认为传统经验难以直接迁移，确有其表象依据。但这仅仅是“水面之上的冰山”，远非其底层基石。&lt;/p&gt;
&lt;h3 id=&#34;本质共性工程核心挑战的永恒回归&#34;&gt;本质共性：工程核心挑战的永恒回归&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;拨开“AI专属”的面纱，工程实践的核心命题依然如故：系统设计与资源调度的精妙艺术。&lt;/strong&gt; 我们面临的，仍是那些传统Infra领域中反复锤炼的同类问题，只是约束条件和优化目标发生了变化：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;资源调度：&lt;/strong&gt; 核心资源从CPU/内存/磁盘IO，&lt;strong&gt;转向了更稀缺、更昂贵的GPU显存与算力&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;负载处理：&lt;/strong&gt; 承载对象从HTTP资源请求，&lt;strong&gt;变为密集的Prompt请求与大规模训练任务&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心目标：&lt;/strong&gt; 高效、稳定、低成本地协调跨节点资源的核心诉求&lt;strong&gt;丝毫未变&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;概念的映射：经典范式的AI实践&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这种延续性，清晰地体现在关键概念的对应关系上：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;传统 Infra 概念&lt;/th&gt;
          &lt;th&gt;AI Infra 对应实践&lt;/th&gt;
          &lt;th&gt;核心思想应用&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;数据分片 (Data Sharding)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;数据并行 (Data Parallelism)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;数据集拆分，多副本并行处理&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;负载均衡 (Load Balancer)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;MoE Router (Mixture of Experts)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;动态分配请求（Token）至专家网络，避免热点&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;操作系统分页 (OS Paging)&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;vLLM KV Cache Paging&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;虚拟化显存空间，高效管理请求状态&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;以vLLM为例：&lt;/strong&gt; 其核心创新在于将&lt;strong&gt;操作系统经典的内存管理机制（分页、交换）&lt;/strong&gt;，创造性地应用于管理LLM推理中关键的&lt;strong&gt;KV Cache状态&lt;/strong&gt;。它如同为LLM定制了一个“显存操作系统”，管理“进程”（推理请求）和“内存页”（KV Cache Blocks），极致优化昂贵显存的利用率。这绝非凭空创造，而是&lt;strong&gt;经典系统原理在特定约束下的卓越应用&lt;/strong&gt;。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
