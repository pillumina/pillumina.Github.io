<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[VeRL] Multi-Turn RL训练源码走读（1） | CctoctoFX</title><meta name=keywords content="framework,verl,sglang"><meta name=description content='
该part主要聚焦相关模块初始化部分
还是以 verl 出发，分析其 end to end mutli-turn RL 训练的全过程。整体上，我希望覆盖所有重要的 class 以及函数，更细粒度的代码不再展开。
为了前后内容的一致性，基于 76f63cffa5 的 commit 进行分析。
虽然本文以分析 verl 的代码为主，写完之后我才意识到，系统设计问题是非常通用的。诸如“log probs 重计算”，“Rollout Engine 显存管理”等等系统设计，是各大 RL 框架都需要考虑的核心问题。
此外因为最近在学习SGLang的实现，本文的推理后端选择的是SGLang展开分析。

整个训练的示意图如下，我们会具体展开每个部分。

  flowchart LR
subgraph W2["Initialize"]
WP[Process Data] --> A
direction TB D1[Data Prepare] --> A
A[TaskRunner] --> B1[RayPPOTrainer]
B1 --> Workers

    subgraph Workers["Workers"]
        direction TB
                WA[ActorRolloutWorker] --> WD[FSDP Engine]
        WB[CriticWorker] --> WD
        WC[RewardModelWorker] --> WD
        WD --> WE[SGLang Engine]
    end
    
    Workers --> C1[Hybrid Engine]
end

subgraph W3["Train Loop"]
    direction TB
    E[DataLoader] --> RolloutBox
    
    subgraph RolloutBox["Rollout"]
        F1[Prepare Data] --> F2[SGLang Async Rollout]
        F2 --> F3[Multi-turn Chat Process]
    end
    
    RolloutBox --> ExpBox
    
    subgraph ExpBox["Make Experience"]
        G1[Recompute Log Probs] --> G2[Compute Reward]
        G2 --> G3[Compute Advantage]
    end
    
    ExpBox --> UpdateBox
    
    subgraph UpdateBox["Train The Model"]
        H1[Load FSDP Model Weight] --> H2[Compute Gradient]
        H2 --> H3[Weights Update]
        H3 --> H4[Sync Weights]
    end
    
    UpdateBox --> E
end

W2 --> W3


数据预处理
以 GSM8K 为例，预处理脚本是 examples/data_preprocess/gsm8k_multiturn_w_tool.py。整个脚本只做了经典的 huggingface datasets mapping，核心逻辑如下：'><meta name=author content="Me"><link rel=canonical href=https://pillumina.github.io/posts/aiinfra/07-verl-multiturn-1/><link crossorigin=anonymous href=/assets/css/stylesheet.9d388901283682bb45dd422fcaa0d0a2054a3c8ff47c9cc6b2baab15508b1b90.css integrity="sha256-nTiJASg2grtF3UIvyqDQogVKPI/0fJzGsrqrFVCLG5A=" rel="preload stylesheet" as=style><link rel=icon href=https://pillumina.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://pillumina.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://pillumina.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://pillumina.github.io/apple-touch-icon.png><link rel=mask-icon href=https://pillumina.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://pillumina.github.io/posts/aiinfra/07-verl-multiturn-1/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>(function(){function t(){return document.querySelector(".post-content")||document.querySelector(".post-single")||document.body}function n(e){return/\$\$[\s\S]+?\$\$|\\\(|\\\)|\\\[|\\\]/.test(e)}function s(e){if(window.__mathjaxLoaded)return;window.__mathjaxLoaded=!0,window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code","tt"],ignoreHtmlClass:"no-math"}};var t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js",t.defer=!0,t.onload=function(){window.MathJax&&window.MathJax.typesetPromise&&window.MathJax.typesetPromise([e]).catch(function(e){console.warn("MathJax typeset error",e)})},document.head.appendChild(t)}function e(){try{if(typeof renderMathInElement=="function"){const e=t();renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,strict:!1,trust:!0,ignoredTags:["script","noscript","style","textarea","pre","code","tt"],ignoredClasses:["no-math"],macros:{"\\boldsymbol":"\\mathbf{#1}","\\bm":"\\mathbf{#1}"}}),setTimeout(function(){n(e.innerHTML)&&s(e)},200)}}catch(e){console.warn("KaTeX render error:",e)}}document.addEventListener("DOMContentLoaded",function(){e(),setTimeout(e,200)}),window.addEventListener("load",function(){setTimeout(e,0)})})()</script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>const config={startOnLoad:!0,theme:"neutral",themeVariables:{lineColor:"#0f0f0f"},flowchart:{useMaxWidth:!1,htmlLabels:!0}};mermaid.initialize(config),window.onload=()=>{window.mermaid.init(0[0],document.querySelectorAll(".language-mermaid"))}</script><link rel=stylesheet href=/css/custom.min.bda7229c4269a242639e058fb11a4782f02f8d77071ba16609befee67cc41c49.css integrity="sha256-vacinEJpokJjngWPsRpHgvAvjXcHG6FmCb7+5nzEHEk="><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]"),n=document.querySelectorAll(".toc a");if(t.length===0||n.length===0)return;const s={};t.forEach(e=>{s[e.id]=e.offsetTop});function i(){const t=window.scrollY+100;let e="";for(const[n,o]of Object.entries(s))if(t>=o)e=n;else break;return e}function o(){const e=i();if(n.forEach(e=>{e.classList.remove("active")}),e){const t=document.querySelector(`.toc a[href="#${e}"]`);t&&t.classList.add("active")}}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){o(),e=!1}),e=!0)}),o()})</script><meta property="og:url" content="https://pillumina.github.io/posts/aiinfra/07-verl-multiturn-1/"><meta property="og:site_name" content="CctoctoFX"><meta property="og:title" content="[VeRL] Multi-Turn RL训练源码走读（1）"><meta property="og:description" content=' 该part主要聚焦相关模块初始化部分
还是以 verl 出发，分析其 end to end mutli-turn RL 训练的全过程。整体上，我希望覆盖所有重要的 class 以及函数，更细粒度的代码不再展开。
为了前后内容的一致性，基于 76f63cffa5 的 commit 进行分析。
虽然本文以分析 verl 的代码为主，写完之后我才意识到，系统设计问题是非常通用的。诸如“log probs 重计算”，“Rollout Engine 显存管理”等等系统设计，是各大 RL 框架都需要考虑的核心问题。
此外因为最近在学习SGLang的实现，本文的推理后端选择的是SGLang展开分析。
整个训练的示意图如下，我们会具体展开每个部分。
flowchart LR subgraph W2["Initialize"] WP[Process Data] --> A direction TB D1[Data Prepare] --> A A[TaskRunner] --> B1[RayPPOTrainer] B1 --> Workers subgraph Workers["Workers"] direction TB WA[ActorRolloutWorker] --> WD[FSDP Engine] WB[CriticWorker] --> WD WC[RewardModelWorker] --> WD WD --> WE[SGLang Engine] end Workers --> C1[Hybrid Engine] end subgraph W3["Train Loop"] direction TB E[DataLoader] --> RolloutBox subgraph RolloutBox["Rollout"] F1[Prepare Data] --> F2[SGLang Async Rollout] F2 --> F3[Multi-turn Chat Process] end RolloutBox --> ExpBox subgraph ExpBox["Make Experience"] G1[Recompute Log Probs] --> G2[Compute Reward] G2 --> G3[Compute Advantage] end ExpBox --> UpdateBox subgraph UpdateBox["Train The Model"] H1[Load FSDP Model Weight] --> H2[Compute Gradient] H2 --> H3[Weights Update] H3 --> H4[Sync Weights] end UpdateBox --> E end W2 --> W3 数据预处理 以 GSM8K 为例，预处理脚本是 examples/data_preprocess/gsm8k_multiturn_w_tool.py。整个脚本只做了经典的 huggingface datasets mapping，核心逻辑如下：'><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-03T15:30:12+08:00"><meta property="article:modified_time" content="2025-08-03T15:30:12+08:00"><meta property="article:tag" content="Framework"><meta property="article:tag" content="Verl"><meta property="article:tag" content="Sglang"><meta property="og:image" content="https://pillumina.github.io/imgs/icon_head.png"><meta property="og:see_also" content="https://pillumina.github.io/posts/aiinfra/09-verl-agentloop/"><meta property="og:see_also" content="https://pillumina.github.io/posts/aiinfra/05-verl-params/"><meta property="og:see_also" content="https://pillumina.github.io/posts/aiinfra/08-verl-multiturn-2/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://pillumina.github.io/imgs/icon_head.png"><meta name=twitter:title content="[VeRL] Multi-Turn RL训练源码走读（1）"><meta name=twitter:description content='
该part主要聚焦相关模块初始化部分
还是以 verl 出发，分析其 end to end mutli-turn RL 训练的全过程。整体上，我希望覆盖所有重要的 class 以及函数，更细粒度的代码不再展开。
为了前后内容的一致性，基于 76f63cffa5 的 commit 进行分析。
虽然本文以分析 verl 的代码为主，写完之后我才意识到，系统设计问题是非常通用的。诸如“log probs 重计算”，“Rollout Engine 显存管理”等等系统设计，是各大 RL 框架都需要考虑的核心问题。
此外因为最近在学习SGLang的实现，本文的推理后端选择的是SGLang展开分析。

整个训练的示意图如下，我们会具体展开每个部分。

  flowchart LR
subgraph W2["Initialize"]
WP[Process Data] --> A
direction TB D1[Data Prepare] --> A
A[TaskRunner] --> B1[RayPPOTrainer]
B1 --> Workers

    subgraph Workers["Workers"]
        direction TB
                WA[ActorRolloutWorker] --> WD[FSDP Engine]
        WB[CriticWorker] --> WD
        WC[RewardModelWorker] --> WD
        WD --> WE[SGLang Engine]
    end
    
    Workers --> C1[Hybrid Engine]
end

subgraph W3["Train Loop"]
    direction TB
    E[DataLoader] --> RolloutBox
    
    subgraph RolloutBox["Rollout"]
        F1[Prepare Data] --> F2[SGLang Async Rollout]
        F2 --> F3[Multi-turn Chat Process]
    end
    
    RolloutBox --> ExpBox
    
    subgraph ExpBox["Make Experience"]
        G1[Recompute Log Probs] --> G2[Compute Reward]
        G2 --> G3[Compute Advantage]
    end
    
    ExpBox --> UpdateBox
    
    subgraph UpdateBox["Train The Model"]
        H1[Load FSDP Model Weight] --> H2[Compute Gradient]
        H2 --> H3[Weights Update]
        H3 --> H4[Sync Weights]
    end
    
    UpdateBox --> E
end

W2 --> W3


数据预处理
以 GSM8K 为例，预处理脚本是 examples/data_preprocess/gsm8k_multiturn_w_tool.py。整个脚本只做了经典的 huggingface datasets mapping，核心逻辑如下：'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://pillumina.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI Infra","item":"https://pillumina.github.io/posts/aiinfra/"},{"@type":"ListItem","position":3,"name":"[VeRL] Multi-Turn RL训练源码走读（1）","item":"https://pillumina.github.io/posts/aiinfra/07-verl-multiturn-1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[VeRL] Multi-Turn RL训练源码走读（1）","name":"[VeRL] Multi-Turn RL训练源码走读（1）","description":" 该part主要聚焦相关模块初始化部分\n还是以 verl 出发，分析其 end to end mutli-turn RL 训练的全过程。整体上，我希望覆盖所有重要的 class 以及函数，更细粒度的代码不再展开。\n为了前后内容的一致性，基于 76f63cffa5 的 commit 进行分析。\n虽然本文以分析 verl 的代码为主，写完之后我才意识到，系统设计问题是非常通用的。诸如“log probs 重计算”，“Rollout Engine 显存管理”等等系统设计，是各大 RL 框架都需要考虑的核心问题。\n此外因为最近在学习SGLang的实现，本文的推理后端选择的是SGLang展开分析。\n整个训练的示意图如下，我们会具体展开每个部分。\nflowchart LR subgraph W2[\u0026#34;Initialize\u0026#34;] WP[Process Data] --\u0026gt; A direction TB D1[Data Prepare] --\u0026gt; A A[TaskRunner] --\u0026gt; B1[RayPPOTrainer] B1 --\u0026gt; Workers subgraph Workers[\u0026#34;Workers\u0026#34;] direction TB WA[ActorRolloutWorker] --\u0026gt; WD[FSDP Engine] WB[CriticWorker] --\u0026gt; WD WC[RewardModelWorker] --\u0026gt; WD WD --\u0026gt; WE[SGLang Engine] end Workers --\u0026gt; C1[Hybrid Engine] end subgraph W3[\u0026#34;Train Loop\u0026#34;] direction TB E[DataLoader] --\u0026gt; RolloutBox subgraph RolloutBox[\u0026#34;Rollout\u0026#34;] F1[Prepare Data] --\u0026gt; F2[SGLang Async Rollout] F2 --\u0026gt; F3[Multi-turn Chat Process] end RolloutBox --\u0026gt; ExpBox subgraph ExpBox[\u0026#34;Make Experience\u0026#34;] G1[Recompute Log Probs] --\u0026gt; G2[Compute Reward] G2 --\u0026gt; G3[Compute Advantage] end ExpBox --\u0026gt; UpdateBox subgraph UpdateBox[\u0026#34;Train The Model\u0026#34;] H1[Load FSDP Model Weight] --\u0026gt; H2[Compute Gradient] H2 --\u0026gt; H3[Weights Update] H3 --\u0026gt; H4[Sync Weights] end UpdateBox --\u0026gt; E end W2 --\u0026gt; W3 数据预处理 以 GSM8K 为例，预处理脚本是 examples/data_preprocess/gsm8k_multiturn_w_tool.py。整个脚本只做了经典的 huggingface datasets mapping，核心逻辑如下：\n","keywords":["framework","verl","sglang"],"articleBody":" 该part主要聚焦相关模块初始化部分\n还是以 verl 出发，分析其 end to end mutli-turn RL 训练的全过程。整体上，我希望覆盖所有重要的 class 以及函数，更细粒度的代码不再展开。\n为了前后内容的一致性，基于 76f63cffa5 的 commit 进行分析。\n虽然本文以分析 verl 的代码为主，写完之后我才意识到，系统设计问题是非常通用的。诸如“log probs 重计算”，“Rollout Engine 显存管理”等等系统设计，是各大 RL 框架都需要考虑的核心问题。\n此外因为最近在学习SGLang的实现，本文的推理后端选择的是SGLang展开分析。\n整个训练的示意图如下，我们会具体展开每个部分。\nflowchart LR subgraph W2[\"Initialize\"] WP[Process Data] --\u003e A direction TB D1[Data Prepare] --\u003e A A[TaskRunner] --\u003e B1[RayPPOTrainer] B1 --\u003e Workers subgraph Workers[\"Workers\"] direction TB WA[ActorRolloutWorker] --\u003e WD[FSDP Engine] WB[CriticWorker] --\u003e WD WC[RewardModelWorker] --\u003e WD WD --\u003e WE[SGLang Engine] end Workers --\u003e C1[Hybrid Engine] end subgraph W3[\"Train Loop\"] direction TB E[DataLoader] --\u003e RolloutBox subgraph RolloutBox[\"Rollout\"] F1[Prepare Data] --\u003e F2[SGLang Async Rollout] F2 --\u003e F3[Multi-turn Chat Process] end RolloutBox --\u003e ExpBox subgraph ExpBox[\"Make Experience\"] G1[Recompute Log Probs] --\u003e G2[Compute Reward] G2 --\u003e G3[Compute Advantage] end ExpBox --\u003e UpdateBox subgraph UpdateBox[\"Train The Model\"] H1[Load FSDP Model Weight] --\u003e H2[Compute Gradient] H2 --\u003e H3[Weights Update] H3 --\u003e H4[Sync Weights] end UpdateBox --\u003e E end W2 --\u003e W3 数据预处理 以 GSM8K 为例，预处理脚本是 examples/data_preprocess/gsm8k_multiturn_w_tool.py。整个脚本只做了经典的 huggingface datasets mapping，核心逻辑如下：\n加载 openai/gsm8k 原始数据集（train/test）。 对每条原始数据，生成带有工具调用要求的 prompt（比如在 user turn 强调模型可以调用 calc_gsm8k_reward 工具，每个qa至少调用一次）。 同样对于每条原始数据，解析答案；将 ground truth 写入 extra_info 字段。 存储为 parquet 文件，分别保留为 train.parquet 和 test.parquet，默认路径为 ~/data/gsm8k/。 启动训练 一个典型的启动命令如下：\n1 2 3 4 5 6 7 8 9 10 # now 用于生成实验启动的时间尾缀，避免重复启动实验时覆盖已有 wandb log function now() { date '+%Y-%m-%d-%H-%M' } export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 nohup bash examples/sglang_multiturn/run_qwen2.5-3b_gsm8k_multiturn.sh \\ trainer.experiment_name=qwen2.5-3b_rm-gsm8k-sgl-multiturn-$now \\ \u003e logs/gsm8k-$now.log 2\u003e\u00261 \u0026 脚本配置 verl 的各项参数实属复杂，我们会单独编写文档来分享对 verl 各类参数的理解。在这篇文档中，我们想要格外强调的是 verl 各类 config 的覆盖关系。verl 的配置文件利用 hydra 进行了分层覆盖的设计模式。\nHydra 简介 Hydra 是一个由 Facebook Research 开发的 Python 框架，旨在优雅地配置复杂的应用程序。它特别适用于需要管理大量参数和进行多组实验的场景，例如机器学习项目。Hydra 的核心特点在于其动态、分层和可组合的配置管理能力。Hydra 的核心优势：\n分层配置 (Hierarchical Configuration)：可以将配置分解成多个小型、模块化的 YAML 文件，并以目录结构进行组织。这使得配置更加清晰、易于管理和复用。 配置组合 (Configuration Composition)：Hydra 能够将这些独立的配置模块动态地组合起来，形成一个完整的配置对象。你可以通过在主配置文件中指定 defaults 列表来选择和组合不同的配置组件。 命令行覆盖 (Command-line Overrides)：这是 Hydra 最强大的功能之一。你可以在运行应用程序时，直接通过命令行参数来覆盖配置中的任何值。这使得进行实验和快速迭代变得非常方便，无需修改配置文件本身。 多运行模式 (Multi-run)：Hydra 允许你通过一个命令运行多个具有不同配置的实验。这对于超参数搜索和模型比较非常有用。 动态工作目录 (Dynamic Working Directory)：每次运行应用程序时，Hydra 都会自动创建一个独立的工作目录，并将当前运行的配置和输出保存到该目录中，确保实验的可复现性。 对象实例化 (Object Instantiation)：Hydra 可以直接从配置中实例化 Python 对象（类或函数），这大大简化了代码，使配置更具声明性。 Hydra 实现分层覆盖的主要机制是组合 (Composition) 和 命令行覆盖 (Command-line Overrides)。\n分层配置的组织： 通常会创建一个 conf 目录，并在其中组织配置。例如：\n1 2 3 4 5 6 7 8 9 10 . ├── my_app.py └── conf ├── config.yaml ├── model │ ├── cnn.yaml │ └── rnn.yaml └── dataset ├── cifar10.yaml └── imagenet.yaml config.yaml 是你的主配置文件。在 model 目录下，你可以定义不同的模型配置（如 cnn.yaml、rnn.yaml），在 dataset 目录下定义不同的数据集配置（如 cifar10.yaml、imagenet.yaml）。\ndefaults 列表进行组合： 在 config.yaml 中，你可以使用特殊的 defaults 列表来指定默认加载哪些配置组件。\nconf/config.yaml 示例：\n1 2 3 4 5 6 7 8 defaults: - model: cnn # 默认加载 conf/model/cnn.yaml - dataset: cifar10 # 默认加载 conf/dataset/cifar10.yaml - _self_ # 确保当前文件中的其他配置项也被加载 # 其他应用级别的默认配置 learning_rate: 0.001 epochs: 10 当 Hydra 加载 config.yaml 时，它会根据 defaults 列表中的指示，自动将 conf/model/cnn.yaml 和 conf/dataset/cifar10.yaml 的内容合并到最终的配置对象中。\n命令行覆盖： 这是实现灵活覆盖的关键。你可以通过命令行参数来覆盖任何已加载的配置值，包括在 defaults 列表中指定的组件或其内部的任何参数。\n覆盖整个配置组：\n要切换模型从 cnn 到 rnn，你可以在命令行中这样运行： 1 python my_app.py model=rnn 这将指示 Hydra 加载 conf/model/rnn.yaml，并用它来替换默认的 cnn 配置。\n覆盖特定参数：\n你可以深入到配置的任何层级来覆盖特定的参数。例如，如果你想修改学习率或数据集的某个参数： 1 python my_app.py learning_rate=0.01 dataset.batch_size=64 这里，learning_rate 直接覆盖了 config.yaml 中的值，而 dataset.batch_size 则覆盖了 conf/dataset/cifar10.yaml（或者你通过 dataset=imagenet 指定的其他数据集配置文件）中的 batch_size 参数。\n添加新参数 (使用 +)：\n如果你想添加一个在默认配置中不存在的新参数，可以使用 + 前缀： 1 python my_app.py +optimizer.name=AdamW 动态覆盖 (使用 ++)：\n如果你希望修改一个已有字段，或者在原配置中没有该字段时自动创建它，可以使用 ++。这种方式适用于需要动态添加或覆盖配置项的场景，确保字段总是被设置为你指定的值，无论它是否已存在。 1 python my_app.py ++model.num_layers=10 Hydra 内部使用 OmegaConf 库来处理这些配置对象，它提供了强大的合并和解析功能，使得分层覆盖和值插值（例如，引用其他配置值或环境变量）变得非常容易。\n回到 verl multi turn，在我们启动的 run_qwen2.5-3b_gsm8k_multiturn.sh 中，设置了：\n1 2 3 4 5 6 PROJECT_DIR=\"$(pwd)\" CONFIG_PATH=\"$PROJECT_DIR/examples/sglang_multiturn/config\" python3 -m verl.trainer.main_ppo \\ --config-path=\"$CONFIG_PATH\" \\ --config-name='gsm8k_multiturn_grpo' \\ 这意味着这次任务的默认 config 是 CONFIG_PATH 下的 gsm8k_multiturn_grpo.yaml，且接下来的参数会覆盖 gsm8k_multiturn_grpo.yaml 中的默认值。更进一步，我们来观察 gsm8k_multiturn_grpo.yaml 的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 hydra: searchpath: - file://verl/trainer/config defaults: - ppo_trainer - _self_ data: max_prompt_length: 1024 max_response_length: 1024 train_batch_size: 256 return_raw_chat: True actor_rollout_ref: hybrid_engine: True rollout: name: sglang multi_turn: enable: True max_turns: 5 # tool_config_path: \"./config/tool_config/gsm8k_tool_config.yaml\" 这里 hydra 语法，会去 verl/trainer/config 目录下寻找 ppo_trainer.yaml 作为基础配置，并且覆盖。因此，启动 run_qwen2.5-3b_gsm8k_multiturn.sh 时，先加载 gsm8k_multiturn_grpo.yaml 作为基础配置并覆盖，然后加载 ppo_trainer.yaml 并覆盖。最终合并这三级配置，得到最终的 config。\n最后，注意到在 run_qwen2.5-3b_gsm8k_multiturn.sh 的最后，我们，我们设置了 actor_rollout_ref.rollout.multi_turn.tool_config_path=\"$PROJECT_DIR/examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml\"，这里指定 multi_turn 的 tool_config_path 为 examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml。这一文件仅仅配置了 gsm8k 的 tool 调用，并不会覆盖之前训练的 config。\n训练主入口与初始化 Ray Actor，Ray Task 和 Ray Worker 在介绍 verl 的训练主入口之前，我们先介绍 Ray 的一些核心概念。Ray 是一个统一计算框架，旨在实现简单地从单机到大型分布式集群的扩展，提供构建和运行分布式应用的底层基础设施和一组核心原语。Ray 通过以下功能实现这一目标：\n统一 API：Ray 提供了一套简单易用的 Python API，将普通函数转换为分布式任务，将 Python 类转换为分布式服务，也即 Ray Actor。Ray Actor 内部持久存储的数据称为状态，可以在 Actor 的整个生命周期内被多次访问、修改和维护，而不会在每次方法调用结束后消失。 弹性伸缩：Ray 可以将应用从单个机器无缝扩展到拥有数千个节点的集群，并能根据需求自动扩缩容。 容错性：Ray 内置了容错机制，可以处理节点故障和任务失败，确保应用的健壮性。 性能优化：Ray 优化了分布式任务调度、内存管理和数据传输，以实现高效的并行计算。 Ray Task 和 Ray Actor 都是用于分布式计算的核心原语，但它们各自服务于不同的目的，主要区别在于是否维护状态。\nRay Task 是 Ray 中最基本的计算单元，代表一个无状态的远程函数。Ray Task 的每次执行都是独立的，不保留之前的任何信息。就像调用一个普通函数，执行完后就清除内部状态。我们调用一个 Ray Task 后，会立即返回得到一个 Ray ObjectRef，而不是实际的结果。主程序可以继续执行其他操作，而 Ray Task 则在后台并行运行。我们需要使用 ray.get() 来获取 Task 的实际结果。 Ray Task 非常适合并行执行大量独立、一次性的计算任务，譬如数据批处理、独立的模型推理等场景。\nRay Actor 是一种特殊的 Ray Task，正如前文所述，它是一个持续运行的、有自己的状态和方法的远程对象。当我们创建一个 Ray Actor 后，Ray 会在集群中的某个 Ray Worker 上启动一个专门的进程来托管这个对象。该进程会一直运行，直到被销毁。Actor 可以维护内部变量，并且这些变量在 Actor 的生命周期内是持久存在的。每次调用 Actor 的方法，都可以访问和修改这些状态。这与普通的 Ray Task 不同，普通 Task 执行完会清除内部状态。Ray Actor 支持并发请求，Ray 会负责将这些请求序列化执行，保证 Actor 内部状态的一致性和线程安全。我们可以通过 @ray.remote 装饰器将一个 Python 类转换为一个 Ray Actor 类，然后通过 .remote() 方法实例化一个远程 Actor。\n最后，Ray Worker 是 Ray 集群中真正执行代码的工作单元。一个 Ray 集群通常由一个 Head Node 和多个 Worker Nodes 组成。每个节点上都会运行一个或多个 Ray Worker 进程。无论是普通的 Ray Task 还是 Ray Actor 的方法，最终都是由 Ray Worker 进程来执行的。每个 Ray Worker 都会被分配一定的计算资源（如 CPU、GPU）。当你提交一个 Ray Task 或创建一个 Ray Actor 时，Ray 的调度器会找到一个有足够资源的 Worker 来运行它。Worker 进程之间以及 Worker 进程与头节点之间会进行通信，以协调任务执行、传输数据和管理状态。一个 Ray Worker 通常就是一个独立的 Python 进程。对于普通的 Ray Task，Ray Worker 相当于函数解释器，执行完任务后可能会被复用去执行其他任务。而对于 Ray Actor，Ray 会启动一个专门的 Worker 进程来托管这个 Actor，这个 Worker 进程的生命周期与 Actor 的生命周期绑定。\nrun_ppo() 和 TaskRunner.run() 有了 ray 的概念，我们回到整个 RL 训练流程的起点：verl.trainer.main_ppo.py 中的 run_ppo()，它负责初始化 Ray 集群，配置 CPU 资源和运行时环境变量，并创建远程 TaskRunner 实例。\n1 2 3 4 5 6 7 8 9 10 11 12 13 def run_ppo(config) -\u003e None: # 初始化 Ray 集群，配置 CPU 资源和运行时环境变量 ray.init( runtime_env={\"env_vars\": {...}}, num_cpus=config.ray_init.num_cpus, ) # 创建远程 TaskRunner 实例 # TaskRunner 是 Ray 中的一个远程 actor，它将在 Ray 集群上异步执行主要的训练任务 runner = TaskRunner.remote() # 异步执行远程任务 runner.run()，并等待其完成 # 通过 ray.get() 阻塞直到远程任务执行完毕，确保整个初始化流程的顺序性 ray.get(runner.run.remote(config)) ActorRolloutRefWorker 和 RayWorkerGroup 的相互关系 TaskRunner 是 verl 中实现 PPO/GRPO 训练的核心组件，它通过将整个 RL 训练流程封装在一个独立的 Ray Actor 中，实现了任务的封装、资源隔离和分布式协调。为了解释清楚 TaskRunner，我们将 verl 当中最让人费解且最复杂的 ActorRolloutRefWorker 和 RayWorkerGroup 这两个类提前解释清楚。\n我们先不讨论这两个类及其基类的具体意义，先讨论清楚其实例对象的创建过程。我们注意到这段 TaskRunner 的初始化中引入 ActorRolloutRefWorker 和 RayWorkerGroup 的相关代码：\nTaskRunner 中引入 ActorRolloutRefWorker 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # Define worker classes based on the actor strategy. if config.actor_rollout_ref.actor.strategy in [\"fsdp\", \"fsdp2\"]: assert config.critic.strategy in [\"fsdp\", \"fsdp2\"] from verl.single_controller.ray import RayWorkerGroup from verl.workers.fsdp_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == \"async\" else ActorRolloutRefWorker ray_worker_group_cls = RayWorkerGroup elif config.actor_rollout_ref.actor.strategy == \"megatron\": assert config.actor_rollout_ref.actor.strategy == config.critic.strategy from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup from verl.workers.megatron_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == \"async\" else ActorRolloutRefWorker ray_worker_group_cls = NVMegatronRayWorkerGroup else: raise NotImplementedError from verl.trainer.ppo.ray_trainer import ResourcePoolManager, Role # Map roles to their corresponding remote worker classes. role_worker_mapping = { Role.ActorRollout: ray.remote(actor_rollout_cls), Role.Critic: ray.remote(CriticWorker), } # Define the resource pool specification. # Map roles to the resource pool. global_pool_id = \"global_pool\" resource_pool_spec = { global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes, } mapping = { Role.ActorRollout: global_pool_id, Role.Critic: global_pool_id, } 可以观察到，在 TaskRunner 的初始化中，会根据各类配置引入对应的 ActorRolloutRefWorker / AsyncActorRolloutRefWorker 类以及 RayWorkerGroup / NVMegatronRayWorkerGroup 类。对于 SGLang 而言，不存在 AsyncActorRolloutRefWorker。ActorRolloutRefWorker 类直接通过 ray.remote(ActorRolloutRefWorker) 创建一个远程的 Ray Actor，将其包装成一个 Ray Actor 类。此时还还没有创建任何实例，也没有分配资源。那么，ActorRolloutRefWorker 类到底在哪儿实例化并分配资源的呢？\n实际上，在 main_ppo.py 的 172 行，构造了 RayPPOTrainer 类，随后调用了 RayPPOTrainer.init_workers() 方法，我们进一步查看 RayPPOTrainer.init_workers() 方法的相关代码，我们观察到，每一个 RL worker 类（比如 ActorRolloutRefWorker）都会创造一个 work group（verl 中的各种 wg 变量），随后调用每个 worker group 的 init_model() 方法，而这些 worker group 实际上都是 RayWorkerGroup 的实例。RayWorkerGroup 的核心作用是资源调度的核心中间层，统一了各种 RL worker（比如 ActorRolloutRefWorker、CriticWorker）的接口，进行统一管理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # RayWorkerGroup 实例，指定资源池 并规定角色和对应的类 wg_dict = self.ray_worker_group_cls( resource_pool=resource_pool, # 只需要指定资源池 ray_cls_with_init=worker_dict_cls, # 一个包含数个worker的类 （e.g. actor_roll， critic, ref） device_name=self.device_name, ) #通过.spawn()获取角色对Ray Actor实例的映射 wg_dict.spawn(prefix_set=class_dict.keys()) # 所有 worker 都通过相同的模式创建，我这里进行简化，实际上的代码比较繁琐 actor_rollout_wg = RayWorkerGroup(resource_pool, actor_rollout_cls) critic_wg = RayWorkerGroup(resource_pool, critic_cls) ref_policy_wg = RayWorkerGroup(resource_pool, ref_policy_cls) 各种 worker group 实际上的初始化 这部分代码在 ray_trainer.py 中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # 1. 为每个角色（例如 actor_rollout、critic、ref）指定用哪个类初始化 worker，并且说明在哪个资源池里分配它们 self.resource_pool_manager.create_resource_pool() self.resource_pool_to_cls = {pool: {} for pool in self.resource_pool_manager.resource_pool_dict.values()} resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout) actor_rollout_cls = RayClassWithInitArgs( cls=self.role_worker_mapping[Role.ActorRollout], config=self.config.actor_rollout_ref, role=\"actor_rollout\", ) self.resource_pool_to_cls[resource_pool][\"actor_rollout\"] = actor_rollout_cls # 2. 根据资源池和角色，批量创建多个 worker 实例（Ray Actor）并统一管理它们，赋予对应的职责 for resource_pool, class_dict in self.resource_pool_to_cls.items(): worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict) wg_dict = self.ray_worker_group_cls(resource_pool=resource_pool, ray_cls_with_init=worker_dict_cls, device_name=self.device_name, **wg_kwargs) spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys()) all_wg.update(spawn_wg) # 3.调用 init_model() 完成模型加载 if self.use_critic: self.critic_wg = all_wg[\"critic\"] self.critic_wg.init_model() if self.use_reference_policy and not self.ref_in_actor: self.ref_policy_wg = all_wg[\"ref\"] self.ref_policy_wg.init_model() if self.use_rm: self.rm_wg = all_wg[\"rm\"] self.rm_wg.init_model() # we should create rollout at the end so that vllm can have a better estimation of kv cache memory self.actor_rollout_wg = all_wg[\"actor_rollout\"] self.actor_rollout_wg.init_model() # create async rollout manager and request scheduler self.async_rollout_mode = False if self.config.actor_rollout_ref.rollout.mode == \"async\": from verl.workers.rollout.async_server import AsyncLLMServerManager self.async_rollout_mode = True self.async_rollout_manager = AsyncLLMServerManager( config=self.config, worker_group=self.actor_rollout_wg, ) 注意到 ray_worker_group_cls 就是 RayWorkerGroup 类，而 worker_dict_cls 就是 ActorRolloutRefWorker 类，所以我的简化是很合理的。\n如此以来，ActorRolloutRefWorker 委托给 RayWorkerGroup 进行初始化。RayWorkerGroup 这个类就是专门用于资源调度的。通过其统一的 _init_with_resource_pool 方法，为每个 GPU 创建一个 worker，最终实例化每种 RL worker 并分配资源。\n1 2 3 4 5 6 7 8 def _init_with_resource_pool(self, resource_pool, ray_cls_with_init, ...): # 从 Ray 申请 Placement Groups pgs = resource_pool.get_placement_groups(strategy=strategy, device_name=self.device_name) # 为每个 GPU 创建一个 worker for local_rank in range(local_world_size): worker = ray_cls_with_init(placement_group=pg, placement_group_bundle_idx=local_rank, ...) self._workers.append(worker) 读到这里，我们基本对 verl 有了一些感觉。注意到，在 verl 当中有两个带有 Worker 的 base class，一个就叫做 Worker，另一个叫做 WorkerGroup。Worker 是 RL 里面的逻辑类（比如 actor 和 critic）,实际管理 RL 的数据流，而 WorkerGroup 只用于分布式系统的资源调度。\n此外，从 actor_rollout_wg 和 ref_policy_wg 的实例化当中，也能看出一些学问。在 ActorRolloutRefWorker 的设计当中，Actor Training，Actor Rollout 和 Reference model 是用同一个 worker class 进行管理的。但是，之后委托给 RayWorkerGroup 创建 worker group 并且调用资源的时候，Actor Training 和 Actor Rollout 是由同一组 RayWorkerGroup 进行资源管理的（这二者本来就要被放在同一个资源组上做 hybird engine），而 Reference Model 是由另一组 RayWorkerGroup 管理资源的。\n最后，我去问了相关开发者，他们也认为把 Actor Rollout，Actor Training 和 Reference Model 放在同一个 worker 里是 bad design 😂，不用纠结这种设计是否有什么高瞻远瞩，完全没有。\nActorRolloutRefWorker.__init__() 如前文所说，ActorRolloutRefWorker 是 verl 中用于管理 Actor Training，Actor Rollout 和 Reference Model 的 worker class。我们具体来分析其逻辑上实现的功能。注意，本文档只分析 FSDP backend 下的实现，megatron 留作后文。\n调用 Worker 基类的构造函数，并保存配置。 如果 PyTorch 分布式环境尚未初始化，则进行初始化，包括设置通信后端和进程组。 为 FSDP 创建设备网格，用于模型参数的分片。 如果启用 Ulysses 序列并行，则初始化其设备网格。 根据传入的 role 参数设置 Worker 的具体角色（actor, rollout, ref）。 根据 Worker 角色配置 profiler，用于性能分析。 配置 parameter offload 和 optimizer offload。 为 Actor，Rollout 和 Reference 分别 normalize batch size。 第 8 步中配置了非常多的 batch size；verl 的 batch size 参数满天飞，虽然我个人认为名字基本是准确的，但是由于名字太像了，一定要做出一些区分。事实上，参数分析我们有单独的文档，我先把一部分内容提前公布了。\ndata.train_batch_size：在一次完整的 PPO 迭代（从 rollout 到 train）中，从数据集中采样并用于生成 experience 的总样本数量，决定了每次 policy 更新所依据的数据量。 actor_rollout_ref.actor.ppo_mini_batch_size：这个参数的名字其实是准确的，因为 mini batch SGD 就是数据到达了一个 mini batch 就更新一次模型参数。在 verl 中，模型会在数据累积到一个 mini batch 后更新一次参数。 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu：这里其实是 gradient accumulation 的参数。由于一个 mini batch 的数据量可能仍然太大，无法一次性前向和反向传播，因此需要将其进一步拆分为 micro batch。每个 micro batch 会计算一次梯度并且累计，但是不会立刻更新模型参数。处理完整个 mini batch 后，才用累积的梯度进行一次参数更新。 此外，在 verl 中，由于 verl 强调 SPMD 策略，可以理解为每个 RL worker 所占据的每个 GPU 上希望进行完全一致的操作，所以 verl 会要求每个 GPU 的 micro batch size 相同。因此，verl 会检查 train batch size / gpu 是否整除 (ref)，如果不整除，则报错。这个设定其实完全没必要；对于 rollout 而言，SGLang 完全不需要发送的请求数量整除 DP 或者 TP size，更何况直接要整除 gpu 数量呢？但是，因为 verl 会用 all gather 从 rollout 的每个 worker 里收集数据，这就要求 rollout 的每个 worker 上分到的数据一致。更进一步，为了 SPMD，又要求 rollout 的每个 gpu 上分到的数据一致。最终，这就导致 verl 的 train batch size 必须整除 gpu 数量；在 GRPO 下是 real train batch size 需要整除 n gpus，等于 train batch size * sampling params 中的 n。\n区分好 mini batch 和 micro batch 后，我也是最近才明白 PPO 中是如何维护 on policy 的。我之前一直以为我们都是在做严格 on policy 的训练，但是一个 train batch size 下有好几个 mini batch，似乎第一个 mini batch 结束之后，目标策略（target policy，被训练的 policy）和行为策略（behavior policy，用于在环境中采样的 policy）就不一致了。一次采样会训练很多个 mini batch，从第一个 mini batch 结束就不是 on policy 了。事实也是如此，我们注意到 PPO 的 loss function：\n$$ L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min(r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t) \\right] $$\n其中的 $r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)}$，这是一个对优势函数的矫正比例，而 $\\hat{A}t$ 就是 advantage。对于 LLM 的 PPO 而言，$\\pi{\\theta_{old}}(a_t | s_t)$ 代表着采样时 behavior policy 在给定 $s_t$ 时，选择 $a_t$ 的概率，而 $\\pi_\\theta(a_t | s_t)$ 就是 target policy 在训练中的每一步给定 $s_t$ 时，选择 $a_t$ 的概率。对 LLM 而言，s_t 是 prompt 前缀，而 a_t 仅仅是 prompt 后的那一个 token。这一概率其实就是 inference 得到的 log probs；我们将收集得到的 (prompt, action) 分别经过 target policy 和 behaviour policy 得到 log probs，然后二者 log probs 相减再取对数，就是矫正项的值。从而，即便第一个 mini batch 之后 target policy 就已经和 behaviour policy 不一致了，仍然可以通过 log probs 进行矫正，也即 importance sampling。\n这样一来，又有了两个问题：log probs 应该如何得到？实际上每次采样时都是发送给 rollout 固定数量的 requests，如果每个 (prompt, action) 对都会计算一次 loss 的话，岂不是更长的 sequence 会计算更多次？\n对于第一个问题，这又是经典的精度问题。如同我在链接到的文章中所说的，rollout engine 目前只有采样得到的 token 能用，而得到的 log probs 以及 reward 精度都不够，不能用于训练。behaviour policy 和 target policy 为了做 importance sampling 所需的 log probs 都得用 training engine 重算。不过要算起来也不麻烦，在第一个 mini batch 启动前，这时候 target behaviour 是一致的，重算 log probs 并且存下来即可。\n对于第二个问题，的确如此。一条很长的 prompt + answer 序列确实会产生非常多的 (prompt, action) 对，其中每个对都可以看作一个 (state, action) 对。而且理论上每个这样的 (prompt, action) 对都会参与 Loss 的计算。这确实可能导致长序列中的 token 会在 Loss 计算中占据更大的比例，让模型过度关注长序列的优化，而对短序列的优化不足。不过，verl 的 rollout engine 会自动对每个 (prompt, action) 对进行加权，从而让长序列和短序列的 token 在 Loss 计算中占据相同的权重。为了缓解这种情况，有很多相关方法：\n样本加权方法 序列级别加权： 一种直接的方法是在计算 Loss 时，给来自不同序列的样本赋予不同的权重。例如，给每个完整序列一个固定的权重（比如 1），然后将这个权重均匀分配给该序列中的每个 (prompt, action) 对。这样，无论序列多长，它对总 Loss 的贡献都相同。如果一个序列有 N 个 token，那么每个 (prompt, action) 对的权重就是 1/N。\n按长度分桶： 在数据收集后，可以根据序列长度对样本进行排序，并尝试将相似长度的序列放入同一个 mini-batch。这有助于提高计算效率，因为可以减少 padding，但对于解决 Loss 贡献不均衡的作用有限。\n固定 Token 数量的批次： 最常见且有效的方法是构建批次时，不固定样本数量，而是固定批次中的总 token 数量。这样，一个 mini-batch 可能包含 4 条长序列，也可能包含 40 条短序列，确保每次更新时处理的总计算量和梯度来源的总 token 数是恒定的，从而缓解长短序列的不均衡问题。\nLoss 归一化：在计算每个 mini-batch 的 Loss 时，可以将其除以该 mini-batch 中实际的 token 数量。这确保了 Loss 值不会仅仅因为批次中包含了更多 token 而增大，从而为不同大小的 mini-batches（如果不是按固定 token 数构建）提供一个公平的比较基础。\n截断：设定一个 max_length 参数，限制模型生成的最大 token 数量。虽然这不直接解决已有长序列的权重问题，但可以防止生成过长的序列，从而限制极端不均衡的发生。\nwhatever，解释了这么多，顺着理解 verl 的框架进一步学习了 RL 算法和系统，这里其实和 multi-turn 都还没有关系，我们还是回到 ActorRolloutRefWorker 的源码上。\nActorRolloutRefWorker.__init__ 源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__(self, config: DictConfig, role: str): # 初始化 Worker 基类 Worker.__init__(self) # 存储配置信息 self.config = config import torch.distributed # 如果分布式环境尚未初始化，则进行初始化 if not torch.distributed.is_initialized(): rank = int(os.environ.get(\"RANK\", 0)) world_size = int(os.environ.get(\"WORLD_SIZE\", 1)) torch.distributed.init_process_group(backend=f\"cpu:gloo,{get_device_name()}:{get_nccl_backend()}\", rank=rank, world_size=world_size) # 为 FSDP 构建设备网格 world_size = torch.distributed.get_world_size() self.device_mesh = create_device_mesh(world_size=world_size, fsdp_size=self.config.actor.fsdp_config.fsdp_size) # 为 Ulysses 序列并行构建设备网格 self.ulysses_device_mesh = None self.ulysses_sequence_parallel_size = self.config.actor.get(\"ulysses_sequence_parallel_size\", 1) dp = world_size // self.ulysses_sequence_parallel_size if self.ulysses_sequence_parallel_size \u003e 1: self.ulysses_device_mesh = init_device_mesh(device_name, mesh_shape=(dp, self.ulysses_sequence_parallel_size), mesh_dim_names=[\"dp\", \"sp\"]) # 初始化 Ulysses 分片管理器 self.ulysses_sharding_manager = FSDPUlyssesShardingManager(self.ulysses_device_mesh) # 获取 LoRA rank 和是否使用 LoRA 的标志 self._lora_rank = self.config.model.get(\"lora_rank\", 0) self._is_lora = self._lora_rank \u003e 0 # 设置 Worker 角色和相关标志 self.role = role assert self.role in [\"actor\", \"rollout\", \"ref\", \"actor_rollout\", \"actor_rollout_ref\"] self._is_actor = self.role in [\"actor\", \"actor_rollout\", \"actor_rollout_ref\"] self._is_rollout = self.role in [\"rollout\", \"actor_rollout\", \"actor_rollout_ref\"] self._is_ref = self.role in [\"ref\", \"actor_rollout_ref\"] profiler_config: Optional[ProfilerConfig] = None # 根据角色获取性能分析配置 if self._is_actor: profiler_config = omega_conf_to_dataclass(config.actor.get(\"profiler\", {}), ProfilerConfig) if self._is_rollout: profiler_config = omega_conf_to_dataclass(config.rollout.get(\"profiler\", {}), ProfilerConfig) if self._is_ref: profiler_config = omega_conf_to_dataclass(config.ref.get(\"profiler\", {}), ProfilerConfig) # 初始化分布式性能分析器 DistProfilerExtension.__init__(self, DistProfiler(rank=self.rank, config=profiler_config)) # 设置参数和优化器卸载标志 self._is_offload_param = False self._is_offload_optimizer = False if self._is_actor: self._is_offload_param = self.config.actor.fsdp_config.get(\"param_offload\", False) self._is_offload_optimizer = self.config.actor.fsdp_config.get(\"optimizer_offload\", False) elif self._is_ref: self._is_offload_param = self.config.ref.fsdp_config.get(\"param_offload\", False) # 规范化 actor 相关配置 if self._is_actor: self.config.actor.ppo_mini_batch_size *= self.config.rollout.n self.config.actor.ppo_mini_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size assert self.config.actor.ppo_mini_batch_size \u003e 0, f\"ppo_mini_batch_size {self.config.actor.ppo_mini_batch_size} should be larger than 0 after normalization\" # micro bsz if self.config.actor.ppo_micro_batch_size is not None: self.config.actor.ppo_micro_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size self.config.actor.ppo_micro_batch_size_per_gpu = self.config.actor.ppo_micro_batch_size if self.config.actor.ppo_micro_batch_size_per_gpu is not None: assert self.config.actor.ppo_mini_batch_size % self.config.actor.ppo_micro_batch_size_per_gpu == 0, f\"normalized ppo_mini_batch_size {self.config.actor.ppo_mini_batch_size} should be divisible by ppo_micro_batch_size_per_gpu {self.config.actor.ppo_micro_batch_size_per_gpu}\" assert self.config.actor.ppo_mini_batch_size // self.config.actor.ppo_micro_batch_size_per_gpu \u003e 0, f\"normalized ppo_mini_batch_size {self.config.actor.ppo_mini_batch_size} should be larger than ppo_micro_batch_size_per_gpu {self.config.actor.ppo_micro_batch_size_per_gpu}\" # 规范化 rollout 相关配置 if self._is_rollout and self.config.rollout.log_prob_micro_batch_size is not None: self.config.rollout.log_prob_micro_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size self.config.rollout.log_prob_micro_batch_size_per_gpu = self.config.rollout.log_prob_micro_batch_size # 规范化 ref 相关配置 if self._is_ref and self.config.ref.log_prob_micro_batch_size is not None: self.config.ref.log_prob_micro_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size self.config.ref.log_prob_micro_batch_size_per_gpu = self.config.ref.log_prob_micro_batch_size ActorRolloutRefWorker._build_model_optimizer() 这部分源码和类写的还是很直白的，不用太多解释：\n初始化 Hugging Face 配置，获取 Generation Config，并设置模型的数据类型（Actor 使用 fp32，Reference 使用 bf16）。 使用 Hugging Face 的 AutoModelForCausalLM 或 AutoModelForVision2Seq 从预训练模型加载基础模型。 应用各种优化技术，包括 Liger kernel、融合 kernel、梯度检查点、LoRA 等。 根据配置选择 FSDP 或 FSDP2 策略，将模型封装到分布式训练框架中，支持参数分片和混合精度训练。 如果当前 Worker 是 Actor 角色，则初始化 AdamW 优化器和学习率调度器。 ActorRolloutRefWorker._build_model_optimizer 源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def _build_model_optimizer( self, model_path, fsdp_config, optim_config, override_model_config, use_remove_padding=False, use_fused_kernels=False, enable_gradient_checkpointing=False, trust_remote_code=False, use_liger=False, role=\"actor\", enable_activation_offload=False, ): from torch import optim from torch.distributed.fsdp import CPUOffload, MixedPrecision from transformers import AutoConfig, AutoModelForCausalLM, AutoModelForVision2Seq from verl.utils.model import get_generation_config, print_model_size, update_model_config from verl.utils.torch_dtypes import PrecisionType assert role in [\"actor\", \"ref\"] log_gpu_memory_usage(f\"Before init {role} from HF AutoModel\", logger=logger) local_path = model_path self.tokenizer = hf_tokenizer(local_path, trust_remote_code=trust_remote_code) self.processor = hf_processor(local_path, trust_remote_code=trust_remote_code) torch_dtype = fsdp_config.get(\"model_dtype\", None) if torch_dtype is None: torch_dtype = torch.float32 if self._is_actor else torch.bfloat16 else: torch_dtype = PrecisionType.to_dtype(torch_dtype) actor_model_config = AutoConfig.from_pretrained(local_path, trust_remote_code=trust_remote_code, attn_implementation=\"flash_attention_2\") if getattr(actor_model_config, \"model_type\", None) == \"kimi_vl\": actor_model_config.text_config.topk_method = \"greedy\" self.generation_config = get_generation_config(local_path, trust_remote_code=trust_remote_code) override_config_kwargs = { \"bos_token_id\": self.tokenizer.bos_token_id, \"eos_token_id\": self.tokenizer.eos_token_id, \"pad_token_id\": self.tokenizer.pad_token_id, } override_config_kwargs.update(override_model_config) update_model_config(actor_model_config, override_config_kwargs=override_config_kwargs) # 如果是 rank 0 进程，打印更新后的模型配置 if self.rank == 0: print(f\"Model config after override: {actor_model_config}\") init_context = get_init_weight_context_manager(use_meta_tensor=not actor_model_config.tie_word_embeddings, mesh=self.device_mesh) with init_context(), warnings.catch_warnings(): warnings.simplefilter(\"ignore\") if type(actor_model_config) in AutoModelForVision2Seq._model_mapping.keys(): actor_module_class = AutoModelForVision2Seq else: actor_module_class = AutoModelForCausalLM actor_module = actor_module_class.from_pretrained( pretrained_model_name_or_path=local_path, torch_dtype=torch_dtype, config=actor_model_config, trust_remote_code=trust_remote_code, ) if use_liger: from liger_kernel.transformers.monkey_patch import _apply_liger_kernel_to_instance _apply_liger_kernel_to_instance(model=actor_module) fused_kernel_options = self.config.model.get(\"fused_kernel_options\", None) fused_kernels_backend = fused_kernel_options.get(\"impl_backend\", None) if fused_kernel_options is not None else None apply_monkey_patch( model=actor_module, use_remove_padding=use_remove_padding, ulysses_sp_size=self.ulysses_sequence_parallel_size, use_fused_kernels=use_fused_kernels, fused_kernels_backend=fused_kernels_backend, ) actor_module.to(torch_dtype) if enable_gradient_checkpointing: actor_module.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False}) if self._is_lora: print(\"Applying LoRA to actor module\") actor_module.enable_input_require_grads() lora_config = {\"task_type\": TaskType.CAUSAL_LM, \"r\": self.config.model.lora_rank, \"lora_alpha\": self.config.model.lora_alpha, \"target_modules\": convert_to_regular_types(self.config.model.target_modules), \"bias\": \"none\"} actor_module = get_peft_model(actor_module, LoraConfig(**lora_config)) torch.distributed.barrier() if self.rank == 0: print_model_size(actor_module) log_gpu_memory_usage(f\"After init {role} from HF AutoModel\", logger=logger) mixed_precision_config = fsdp_config.get(\"mixed_precision\", None) if mixed_precision_config is not None: param_dtype = PrecisionType.to_dtype(mixed_precision_config.get(\"param_dtype\", \"bf16\")) reduce_dtype = PrecisionType.to_dtype(mixed_precision_config.get(\"reduce_dtype\", \"fp32\")) buffer_dtype = PrecisionType.to_dtype(mixed_precision_config.get(\"buffer_dtype\", \"fp32\")) else: param_dtype = torch.bfloat16 reduce_dtype = torch.float32 buffer_dtype = torch.float32 mixed_precision = MixedPrecision(param_dtype=param_dtype, reduce_dtype=reduce_dtype, buffer_dtype=buffer_dtype) auto_wrap_policy = get_fsdp_wrap_policy(module=actor_module, config=fsdp_config.get(\"wrap_policy\", None), is_lora=self.config.model.get(\"lora_rank\", 0) \u003e 0) # TODO(zhangchi.usc1992, shengguangming) fix me. Current, auto_wrap_policy causes HFRollout to hang in Gemma if self._is_rollout and self.config.rollout.name == \"hf\": auto_wrap_policy = None # 如果是 rank 0 进程，打印包装策略 if self.rank == 0: print(f\"wrap_policy: {auto_wrap_policy}\") fsdp_mesh = self.device_mesh sharding_strategy = get_sharding_strategy(fsdp_mesh) # TODO: 添加 transformer 策略 # 我们强制 reference policy 使用 CPUOffload 来节省内存 # 我们强制关闭 actor 的 CPUOffload，因为它在使用 grad accumulation 时会导致不正确的结果 cpu_offload = None if role == \"actor\" else CPUOffload(offload_params=True) # 根据配置的策略，将模型封装到 FSDP 中 fsdp_strategy = self.config.actor.strategy if fsdp_strategy == \"fsdp\": actor_module_fsdp = FSDP( actor_module, cpu_offload=cpu_offload, param_init_fn=init_fn, use_orig_params=False, auto_wrap_policy=auto_wrap_policy, device_id=get_device_id(), sharding_strategy=sharding_strategy, # zero3 mixed_precision=mixed_precision, sync_module_states=True, device_mesh=self.device_mesh, forward_prefetch=self.config.actor.fsdp_config.forward_prefetch, ) elif fsdp_strategy == \"fsdp2\": assert CPUOffloadPolicy is not None, \"PyTorch version \u003e= 2.4 is required for using fully_shard API (FSDP2)\" mp_policy = MixedPrecisionPolicy(param_dtype=param_dtype, reduce_dtype=reduce_dtype, cast_forward_inputs=True) if role == \"actor\" and fsdp_config.offload_policy: cpu_offload = CPUOffloadPolicy(pin_memory=True) self._is_offload_param = False self._is_offload_optimizer = False else: cpu_offload = None if role == \"actor\" else CPUOffloadPolicy(pin_memory=True) fsdp_kwargs = { \"mesh\": fsdp_mesh, \"mp_policy\": mp_policy, \"offload_policy\": cpu_offload, \"reshard_after_forward\": fsdp_config.reshard_after_forward, } full_state = actor_module.state_dict() apply_fsdp2(actor_module, fsdp_kwargs, fsdp_config) fsdp2_load_full_state_dict(actor_module, full_state, fsdp_mesh, cpu_offload) actor_module_fsdp = actor_module else: raise NotImplementedError(f\"not implement {fsdp_strategy}\") # 如果启用了激活卸载，则启用它 if enable_activation_offload: enable_activation_offloading(actor_module_fsdp, fsdp_strategy, enable_gradient_checkpointing) # 记录 FSDP 初始化之后的 GPU 内存使用情况 log_gpu_memory_usage(f\"After {role} FSDP init\", logger=logger) # TODO: add more optimizer args into config if role == \"actor\" and optim_config is not None: from verl.utils.torch_functional import get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup actor_optimizer = optim.AdamW( actor_module_fsdp.parameters(), lr=optim_config.lr, betas=optim_config.get(\"betas\", (0.9, 0.999)), weight_decay=optim_config.get(\"weight_decay\", 1e-2), ) total_steps = optim_config.get(\"total_training_steps\", 0) num_warmup_steps = int(optim_config.get(\"lr_warmup_steps\", -1)) warmup_style = optim_config.get(\"warmup_style\", \"constant\") min_lr_ratio = optim_config.get(\"min_lr_ratio\", 0.0) num_cycles = optim_config.get(\"num_cycles\", 0.5) if num_warmup_steps \u003c 0: num_warmup_steps_ratio = optim_config.get(\"lr_warmup_steps_ratio\", 0.0) num_warmup_steps = int(num_warmup_steps_ratio * total_steps) if self.rank == 0: print(f\"Total steps: {total_steps}, num_warmup_steps: {num_warmup_steps}\") if warmup_style == \"constant\": actor_lr_scheduler = get_constant_schedule_with_warmup(optimizer=actor_optimizer, num_warmup_steps=num_warmup_steps) elif warmup_style == \"cosine\": actor_lr_scheduler = get_cosine_schedule_with_warmup(optimizer=actor_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps, min_lr_ratio=min_lr_ratio, num_cycles=num_cycles) else: raise NotImplementedError(f\"Warmup style {warmup_style} is not supported\") log_gpu_memory_usage(f\"After {role} optimizer init\", logger=logger) else: actor_optimizer = None actor_lr_scheduler = None return actor_module_fsdp, actor_optimizer, actor_lr_scheduler, actor_model_config 这里代码很直白。有一个点值得单独拎出来讲一下：仔细观察 actor_module 的 dtype，直觉告诉我，actor_module 的 dtype 应该是 bf16 的，而 gradient 和 optimizer 的 dtype 是 fp32 的。可是 actor_module 的 default dtype 被设为了 fp32，然后从 fp32 load 了模型。实际上这是因为 pytorch 的各种 optimizer 都是直接和 parameter 绑定的，用 bf16 的 parameter 初始化的 optimizer 也是 bf16。所以 model 先 load 了 fp32，然后初始化 optimizer 作为混合精度，最后把 model 转成 bf16。\nActorRolloutRefWorker._build_rollout() 这是对我而言最清晰的地方，实际上也是最熟悉的。在这里终于引入了 SGLang：\n设备网格创建：为 Rollout 创建推理张量并行（infer_tp）设备网格。 SGLang Rollout 构建：导入并实例化 SGLangRollout 和 FSDPSGLangShardingManager。FSDPSGLangShardingManager 负责在 FSDP 训练格式和 SGLang 推理格式之间转换模型权重。 ActorRolloutRefWorker._build_rollout 部分源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def _build_rollout(self, trust_remote_code=False): from torch.distributed.device_mesh import init_device_mesh infer_tp = self.config.rollout.tensor_model_parallel_size dp = self.world_size // infer_tp assert self.world_size % infer_tp == 0, f\"rollout world_size: {self.world_size} is not divisible by infer_tp: {infer_tp}\" rollout_device_mesh = init_device_mesh(device_name, mesh_shape=(dp, infer_tp), mesh_dim_names=[\"dp\", \"infer_tp\"]) rollout_name = self.config.rollout.name if rollout_name in [\"sglang\", \"sglang_async\"]: if rollout_name == \"sglang_async\": warnings.warn( \"'sglang_async' has been deprecated and merged into 'sglang'. Please use 'sglang' going forward.\", DeprecationWarning, stacklevel=2, ) from verl.workers.rollout.sglang_rollout import SGLangRollout from verl.workers.sharding_manager.fsdp_sglang import FSDPSGLangShardingManager local_path = copy_to_local(self.config.model.path) log_gpu_memory_usage(f\"Before building {rollout_name} rollout\", logger=logger) rollout = SGLangRollout( actor_module=local_path, config=self.config.rollout, tokenizer=self.tokenizer, model_hf_config=self.actor_model_config, trust_remote_code=trust_remote_code, ) log_gpu_memory_usage(f\"After building {rollout_name} rollout\", logger=logger) if torch.distributed.get_world_size() == 1: self.config.rollout.load_format = \"dummy_hf\" rollout_sharding_manager = FSDPSGLangShardingManager( module=self.actor_module_fsdp, inference_engine=rollout._engine, model_config=self.actor_model_config, full_params=\"hf\" in self.config.rollout.load_format, device_mesh=rollout_device_mesh, offload_param=self._is_offload_param, ) log_gpu_memory_usage(\"After building sharding manager\", logger=logger) else: raise NotImplementedError(f\"Rollout name: {self.config.rollout.name} is not supported\") return rollout, rollout_sharding_manager SGLangRollout.__init__() 事已至此，再往下看一层 SGLang 具体的初始化：\n调用父类构造函数并设置配置和设备网格。 通过 _initialize_tools() 初始化工具 schemas、map 和解析器，支持 Multi-turn 对话中的工具使用。 初始化 SGLang 推理所需的分布式环境。 通过 _verify_config() 验证模型配置。 通过 _init_inference_engine() 初始化 SGLang 推理引擎。 通过 _init_sampling_params() 初始化生成序列的采样参数。 设置 Tokenizer 和 padding token ID。 SGLangRollout.__init__ 部分源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class SGLangRollout(BaseRollout): def __init__( self, actor_module: str, config: DictConfig, tokenizer, model_hf_config, port=None, trust_remote_code: bool = False, device_mesh: DeviceMesh | None = None, **kwargs, ): \"\"\"Synchronized SGLang rollout engine. Args: actor_module: Huggingface model name or path to the model. The model should be supported by SGLang. config: A DictConfig object containing SGLang-specific operational parameters and rollout settings. Refer to https://docs.sglang.ai/backend/server_arguments.html tokenizer: The tokenizer instance compatible with the actor_module. model_hf_config: The Hugging Face model's configuration (e.g., `transformers.PretrainedConfig`). It provides architectural details and hyperparameters like `max_position_embeddings`, used by SGLang for correct model initialization. This is the model's inherent design, not SGLang's runtime behavior. port: Optional port for multi-node initialization when nnodes \u003e 1. trust_remote_code: Whether or not to allow for custom models defined on the Hub in their own modeling files. device_mesh: Optional `DeviceMesh` object for distributed setup. **kwargs: Additional keyword arguments, primarily `train_tp` for Megatron Backend integration to initialize hybrid engine process groups. \"\"\" super().__init__() self.config = config self._device_mesh_cpu = device_mesh os.environ.setdefault(\"SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK\", \"true\") ( self._tool_schemas, self._tool_map, self._tool_call_parser_type, self._sgl_tools, self._function_call_parser, ) = self._initialize_tools(config, tokenizer) self.interaction: dict[str, BaseInteraction] = self._intitalize_interaction(config) # If turn on `free_cache_engine`, SGLang engine's KV cache # will be freed after each `generate_sequences` call. assert not (not config.enforce_eager and config.free_cache_engine), \"disable CUDA graph (enforce_eager = False) if free cache engine\" logger.info(f\"tool_schemas: {self._tool_schemas}, tool_map: {self._tool_map}, tool_call_parser_type: {self._tool_call_parser_type}, sgl_tools: {self._sgl_tools}, function_call_parser: {self._function_call_parser}\") self._init_distributed_env(device_mesh_cpu=device_mesh, **kwargs) self._verify_config(model_hf_config=model_hf_config) # initialize the inference engine self._init_inference_engine(trust_remote_code, actor_module, port) self._init_sampling_params(**kwargs) self.tokenizer = tokenizer self.pad_token_id = tokenizer.pad_token_id SGLangRollout.AsyncEngine 关于 SGLangRollout 调用 tool 的部分，我们在下文的训练循环中再展开，这里先讨论完 SGLang 的初始化。为了调用 SGLang engine 的接口，verl 进行了一层封装，实现了我们对 SGLang 除开 rollout 之外的所有接口：\nrelease and resume memory occupation：在训练时释放掉显存占用并在训练后恢复。 update weights from tensor：训练结束后更新模型权重。 flush cache：模型参数更新后刷新 KV cache，因为之前的 KV cache 已经失效了。 这里涉及到了非常深入的内存管理问题，读者对 SGLang engine 在 verl 里的显存管理感兴趣，欢迎阅读标哥的博客 optimizing Memory Usage in verl，写的非常深入浅出。\nSGLangRollout 何时需要 flush cache 这一部分内容需要单独拎出来讲讲。SGLang engine 的 release 和 resume 需要保留 CUDA Graph，否则 rollout 效率会大幅降低。因此，我们基于 tom 的 torch_memory_saver 实现了独立的显存管理。简单来说，我们有：\npause；保留 mem savor 作用域内指定 tensor 的 virtual address，但是将其 physical memory 释放回显存管理器。 resume；将先前 pause 的 tensor 重新申请一块 physical memory，并将其 virtual address 映射到新的 physical memory。 注意，整个 pause 和 resume 的过程中，tensor 的 virtual address 不会发生变化，只是这块 virtual address 映射到的 physical memory 改变了。因此，CUDA Graph 并没有失效，不变的 virtual address 让计算流仍旧可以正常执行。\nverl 内的 release_memory_occupation 和 resume_memory_occupation 就是基于 pause 和 resume 实现的。听上去是个完美的故事，我们甚至实现了 mutli-stage 的显存管理，能够独立 release 和 resume kv cache 和 model weights。\n不过，对于 kv cache 而言，在 kv cache 被 release 掉之后，实际上 kv cache 的 tensor 仍旧保留，只是其 virtual address 映射到的 physical memory 被释放了。与此同时，radix tree 仍旧索引着整个 kv cache。当 kv cache 被 resume 之后，一方面之前物理内存上之前的 kv cache 已经不复存在了，另一方面模型的参数也被更新。出于这两点，我们一定要使用 flush cache 接口来刷新 kv cache 的索引（radix tree）。\n这里又有个非常有趣的设计。乍一想 kv cache 的管理这么麻烦，还要 flush，为什么不直接 delete kv cache 以及 delete model weights 再重新初始化呢？显然，这样没法利用已有的 cuda graph，非常消耗时间。保留 virtual address 不变但是更换 physical memory 的方案，让 verl 能够持续利用已建好的 cuda graph。\n最后一个问题，一共要几次 flush cache 呢？我个人理解，在一整个 training engine 被 pause，resume 然后 update weights 的过程中，必须要有一次 flush cache 来刷新 kv cache 的索引，只是 verl 当中为了保险，刷新了很多次罢了。\nSGLangRollout.AsyncEngine 源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class AsyncEngine(sglang.srt.entrypoints.engine.Engine): def __init__(self, **kwargs): super().__init__(**kwargs) # default to use dummy load format, which need to reload weights in first time self._need_reload = True async def release_memory_occupation(self): \"\"\"Release GPU occupation temporarily.\"\"\" obj = ReleaseMemoryOccupationReqInput() return await self.tokenizer_manager.release_memory_occupation(obj, None) async def resume_memory_occupation(self): return await self.tokenizer_manager.resume_memory_occupation(obj, None) async def update_weights_from_tensor( self, named_tensors: List[Tuple[str, torch.Tensor]], # noqa: UP006 load_format: Optional[str] = None, flush_cache: bool = True, ): \"\"\"Update weights from distributed source. If there are going to be more updates, set `flush_cache` to be false to avoid duplicated cache cleaning operation.\"\"\" obj = UpdateWeightsFromTensorReqInput( serialized_named_tensors=[MultiprocessingSerializer.serialize(named_tensors) for _ in range(self.server_args.tp_size)], load_format=load_format, flush_cache=flush_cache, ) return await self.tokenizer_manager.update_weights_from_tensor(obj, None) async def flush_cache(self): return await self.tokenizer_manager.flush_cache() SGLangRollout._init_inference_engine() SGLangRollout._init_inference_engine() 初始化了封装的 AsyncEngine。\nSGLangRollout._init_inference_engine 源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def _init_inference_engine(self, trust_remote_code, actor_module, port): # initialize the inference engine nnodes = -(-self._tp_size // len(self.visible_devices_set)) if nnodes \u003e 1: ip = get_ip() port = get_open_port() if port is None else port [ip, port] = broadcast_pyobj( [ip, port], rank=self._rank, dist_group=self._device_mesh_cpu.get_group(\"tp\"), src=self._device_mesh_cpu[\"tp\"].mesh[0].item(), force_cpu_device=False, ) dist_init_addr = f\"[{ip}]:{port}\" if is_ipv6(ip) else f\"{ip}:{port}\" else: dist_init_addr = None load_format = \"dummy\" if self.config.load_format.startswith(\"dummy\") else self.config.load_format tp_size_per_node = self._tp_size // nnodes node_rank = self._tp_rank // tp_size_per_node first_rank_in_node = self._tp_rank % tp_size_per_node == 0 if first_rank_in_node: rank = dist.get_rank() os.environ[\"SGLANG_BLOCK_NONZERO_RANK_CHILDREN\"] = \"0\" self._engine = AsyncEngine( model_path=actor_module, dtype=self.config.dtype, mem_fraction_static=self.config.gpu_memory_utilization, enable_memory_saver=True, base_gpu_id=0, gpu_id_step=1, tp_size=self._tp_size, node_rank=node_rank, load_format=load_format, dist_init_addr=dist_init_addr, nnodes=nnodes, trust_remote_code=trust_remote_code, # NOTE(linjunrong): add rank to prevent SGLang generate same port inside PortArgs.init_new # when random.seed is being set during training port=30000 + rank, # NOTE(Chenyang): if you want to debug the SGLang engine output # please set the following parameters # Otherwise, it will make the engine run too slow # log_level=\"INFO\", # log_requests=True, # log_requests_level=2, # max_running_requests=1, ) else: self._engine = None self.sharding_manager = None self.is_sleep = True 这里最值得一提的是，SGLang engine 并没有严格实现 verl 所希望的 SPMD 模式（每个 GPU 上的进程完全一样），而是采用了 mock 的 SPMD。举例来说，假设 tp size = 4，按照 verl 的设计，应该要 4 张 GPU 上每个都运行一个相同的 SGLang engine。实际上的实现是在 GPU 0 上启动一个进程占据全部 GPU，而 GPU 1 2 3 上仅仅保留一个空进程 None。虽然 verl team 起初设定中认为严格的 SPMD 意义巨大，但实际使用中，我们认为 mock 的 SPMD 已经足够满足性能需求。\n【TODO】 这么描述可能不严谨。\nTaskRunner.run() 往下走了这么多层，我们终于能够继续回到 TaskRunner 类。😭\n【TODO】上文其实主要是 Actor Rollout，还没有具体说 Actor 的 training forward and backward。以及 Reference，reward 和 critic 的 training forward and backward。\n加载、解析和验证训练任务的配置（使用 OmegaConf），确保所有参数的正确性和一致性。 将模型文件从远程路径复制到本地，确保所有 Worker 都可以访问。 组件初始化： 初始化 Tokenizer 和 Processor，用于文本和多模态数据的处理。 根据配置中指定的 Actor 策略（如 fsdp 或 megatron），动态选择相应的 Worker 类（例如 ActorRolloutRefWorker 和 CriticWorker），并确定使用的 RayWorkerGroup 类型。 定义 Ray 资源池的规格和角色到资源池的映射，用于 GPU 资源的分配和管理。 加载用于训练和验证的奖励模型。 创建训练和验证数据集，以及训练数据采样器。 创建 RayPPOTrainer 实例，它是管理所有计算资源和训练流程的中央协调器。 调用 RayPPOTrainer 的 init_workers() 方法，将配置的 Worker 类实例化到 Ray 集群的 GPU 上，为实际计算做准备。 调用 RayPPOTrainer 的 fit() 方法，启动 PPO 训练循环。 TaskRunner.run 源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @ray.remote(num_cpus=1) class TaskRunner: def run(self, config): from pprint import pprint from omegaconf import OmegaConf from verl.utils.fs import copy_to_local import socket import os print(f\"TaskRunner hostname: {socket.gethostname()}, PID: {os.getpid()}\") pprint(OmegaConf.to_container(config, resolve=True)) OmegaConf.resolve(config) # 模型下载 local_path = copy_to_local(config.actor_rollout_ref.model.path, use_shm=config.actor_rollout_ref.model.get(\"use_shm\", False)) # Tokenizer 和 Processor 初始化 from verl.utils import hf_processor, hf_tokenizer trust_remote_code = config.data.get(\"trust_remote_code\", False) tokenizer = hf_tokenizer(local_path, trust_remote_code=trust_remote_code) processor = hf_processor(local_path, trust_remote_code=trust_remote_code, use_fast=True) # Worker 类型选择 if config.actor_rollout_ref.actor.strategy in [\"fsdp\", \"fsdp2\"]: from verl.single_controller.ray import RayWorkerGroup from verl.workers.fsdp_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == \"async\" else ActorRolloutRefWorker ray_worker_group_cls = RayWorkerGroup elif config.actor_rollout_ref.actor.strategy == \"megatron\": assert config.actor_rollout_ref.actor.strategy == config.critic.strategy from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup from verl.workers.megatron_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == \"async\" else ActorRolloutRefWorker ray_worker_group_cls = NVMegatronRayWorkerGroup else: raise NotImplementedError from verl.trainer.ppo.ray_trainer import ResourcePoolManager, Role # 角色到 Worker 类的映射 role_worker_mapping = { Role.ActorRollout: ray.remote(actor_rollout_cls), Role.Critic: ray.remote(CriticWorker), } # 资源池规格和角色映射 global_pool_id = \"global_pool\" resource_pool_spec = { global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes, } mapping = { Role.ActorRollout: global_pool_id, Role.Critic: global_pool_id, } # Reward Model Worker 的初始化 if config.reward_model.enable: if config.reward_model.strategy in [\"fsdp\", \"fsdp2\"]: from verl.workers.fsdp_workers import RewardModelWorker elif config.reward_model.strategy == \"megatron\": from verl.workers.megatron_workers import RewardModelWorker else: raise NotImplementedError role_worker_mapping[Role.RewardModel] = ray.remote(RewardModelWorker) mapping[Role.RewardModel] = global_pool_id # Reference Policy Worker 的初始化 if config.algorithm.use_kl_in_reward or config.actor_rollout_ref.actor.use_kl_loss: role_worker_mapping[Role.RefPolicy] = ray.remote(ActorRolloutRefWorker) mapping[Role.RefPolicy] = global_pool_id # 加载奖励管理器 reward_fn = load_reward_manager(config, tokenizer, num_examine=0, **config.reward_model.get(\"reward_kwargs\", {})) val_reward_fn = load_reward_manager(config, tokenizer, num_examine=1, **config.reward_model.get(\"reward_kwargs\", {})) resource_pool_manager = ResourcePoolManager(resource_pool_spec=resource_pool_spec, mapping=mapping) from verl.utils.dataset.rl_dataset import collate_fn, create_rl_dataset, create_rl_sampler # 创建训练和验证数据集 train_dataset = create_rl_dataset(config.data.train_files, config.data, tokenizer, processor) val_dataset = create_rl_dataset(config.data.val_files, config.data, tokenizer, processor) train_sampler = create_rl_sampler(config.data, train_dataset) # 初始化 PPO 训练器 trainer = RayPPOTrainer( config=config, tokenizer=tokenizer, processor=processor, role_worker_mapping=role_worker_mapping, resource_pool_manager=resource_pool_manager, ray_worker_group_cls=ray_worker_group_cls, reward_fn=reward_fn, val_reward_fn=val_reward_fn, train_dataset=train_dataset, val_dataset=val_dataset, collate_fn=collate_fn, train_sampler=train_sampler, device_name=config.trainer.device, ) # 初始化训练器的 Workers trainer.init_workers() # 启动训练过程 trainer.fit() RayPPOTrainer.__init__() 保存传入的配置对象、tokenizer、processor、角色到 Worker 的映射、资源池管理器以及 WorkerGroup 类。 根据配置启用或禁用 Critic、Reference Policy、Reward Model 和 Hybrid Engine 等功能组件。 调用 _validate_config() 方法验证配置的合理性。 存储训练和验证数据集、collate 函数和训练数据采样器。 RayPPOTrainer 源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class RayPPOTrainer: # TODO: support each role have individual ray_worker_group_cls, # i.e., support different backend of different role def __init__( self, config, tokenizer, role_worker_mapping: dict[Role, WorkerType], resource_pool_manager: ResourcePoolManager, ray_worker_group_cls: RayWorkerGroup = RayWorkerGroup, processor=None, reward_fn=None, val_reward_fn=None, train_dataset: Optional[Dataset] = None, val_dataset: Optional[Dataset] = None, collate_fn=None, train_sampler: Optional[Sampler] = None, device_name=\"cuda\", ): \"\"\" Initialize distributed PPO trainer with Ray backend. Note that this trainer runs on the driver process on a single CPU/GPU node. Args: config: Configuration object containing training parameters. tokenizer: Tokenizer used for encoding and decoding text. role_worker_mapping (dict[Role, WorkerType]): Mapping from roles to worker classes. resource_pool_manager (ResourcePoolManager): Manager for Ray resource pools. ray_worker_group_cls (RayWorkerGroup, optional): Class for Ray worker groups. Defaults to RayWorkerGroup. processor: Optional data processor, used for multimodal data. reward_fn: Function for computing rewards during training. val_reward_fn: Function for computing rewards during validation. train_dataset (Optional[Dataset], optional): Training dataset. Defaults to None. val_dataset (Optional[Dataset], optional): Validation dataset. Defaults to None. collate_fn: Function to collate data samples into batches. train_sampler (Optional[Sampler], optional): Sampler for the training dataset. Defaults to None. device_name (str, optional): Device name for training (e.g., \"cuda\", \"cpu\"). Defaults to \"cuda\". \"\"\" # Store the tokenizer for text processing self.tokenizer = tokenizer self.processor = processor self.config = config self.reward_fn = reward_fn self.val_reward_fn = val_reward_fn self.hybrid_engine = config.actor_rollout_ref.hybrid_engine assert self.hybrid_engine, \"Currently, only support hybrid engine\" if self.hybrid_engine: assert Role.ActorRollout in role_worker_mapping, f\"{role_worker_mapping.keys()=}\" self.role_worker_mapping = role_worker_mapping self.resource_pool_manager = resource_pool_manager self.use_reference_policy = Role.RefPolicy in role_worker_mapping self.use_rm = Role.RewardModel in role_worker_mapping self.ray_worker_group_cls = ray_worker_group_cls self.device_name = device_name self.validation_generations_logger = ValidationGenerationsLogger() # if ref_in_actor is True, the reference policy will be actor without lora applied self.ref_in_actor = config.actor_rollout_ref.model.get(\"lora_rank\", 0) \u003e 0 # define in-reward KL control # kl loss control currently not suppoorted if config.algorithm.use_kl_in_reward: self.kl_ctrl_in_reward = core_algos.get_kl_controller(config.algorithm.kl_ctrl) if self.config.algorithm.adv_estimator == AdvantageEstimator.GAE: self.use_critic = True elif self.config.algorithm.adv_estimator in [ AdvantageEstimator.GRPO, AdvantageEstimator.GRPO_PASSK, AdvantageEstimator.REINFORCE_PLUS_PLUS, AdvantageEstimator.REMAX, AdvantageEstimator.RLOO, AdvantageEstimator.OPO, AdvantageEstimator.REINFORCE_PLUS_PLUS_BASELINE, ]: self.use_critic = False else: raise NotImplementedError self._validate_config() self._create_dataloader(train_dataset, val_dataset, collate_fn, train_sampler) RayPPOTrainer.init_workers() init_workers() 函数负责在 Ray 集群上实例化和初始化 ActorRollout、Critic、Reference Policy 和 Reward Model Workers。\n创建资源池：通过 ResourcePoolManager 创建 Ray 资源池。 初始化资源池到类的映射：为每个资源池创建一个字典，用于存储不同角色 Worker 的 RayClassWithInitArgs 包装器。RayClassWithInitArgs 用于延迟初始化 Worker，存储了 Worker 的类和初始化参数。 创建不同角色的 Worker 的 RayClassWithInitArgs 实例：根据配置启用情况，为 ActorRollout、Critic、Reference Policy 和 Reward Model 创建对应的 RayClassWithInitArgs 实例。 初始化 WorkerGroup：遍历所有资源池，将同一资源池中的多个 Worker 类通过 create_colocated_worker_cls 组合成一个共置类，然后实例化 RayWorkerGroup。RayWorkerGroup 负责在多个 GPU 上启动多个 Worker 实例。最后调用 spawn() 方法在 Ray 中实际创建 Worker 实例。 初始化各个 Worker：根据角色从创建的 WorkerGroup 字典中获取对应的 WorkerGroup，并调用其 init_model() 方法，按照依赖关系依次初始化不同的 Worker 模块。ActorRollout Worker 通常最后初始化以优化内存使用。 RayPPOTrainer.init_workers 源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def init_workers(self): \"\"\"Initialize distributed training workers using Ray backend. Creates: 1. Ray resource pools from configuration 2. Worker groups for each role (actor, critic, etc.) \"\"\" self.resource_pool_manager.create_resource_pool() self.resource_pool_to_cls = {pool: {} for pool in self.resource_pool_manager.resource_pool_dict.values()} # create actor and rollout if self.hybrid_engine: resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout) actor_rollout_cls = RayClassWithInitArgs( cls=self.role_worker_mapping[Role.ActorRollout], config=self.config.actor_rollout_ref, role=\"actor_rollout\", ) self.resource_pool_to_cls[resource_pool][\"actor_rollout\"] = actor_rollout_cls else: raise NotImplementedError # create critic if self.use_critic: resource_pool = self.resource_pool_manager.get_resource_pool(Role.Critic) critic_cls = RayClassWithInitArgs(cls=self.role_worker_mapping[Role.Critic], config=self.config.critic) self.resource_pool_to_cls[resource_pool][\"critic\"] = critic_cls # create reference policy if needed if self.use_reference_policy: resource_pool = self.resource_pool_manager.get_resource_pool(Role.RefPolicy) ref_policy_cls = RayClassWithInitArgs(self.role_worker_mapping[Role.RefPolicy], config=self.config.actor_rollout_ref, role=\"ref\") self.resource_pool_to_cls[resource_pool][\"ref\"] = ref_policy_cls # create a reward model if reward_fn is None if self.use_rm: # we create a RM here resource_pool = self.resource_pool_manager.get_resource_pool(Role.RewardModel) rm_cls = RayClassWithInitArgs(self.role_worker_mapping[Role.RewardModel], config=self.config.reward_model) self.resource_pool_to_cls[resource_pool][\"rm\"] = rm_cls # initialize WorkerGroup # NOTE: if you want to use a different resource pool for each role, which can support different parallel size, # you should not use `create_colocated_worker_cls`. # Instead, directly pass different resource pool to different worker groups. # See https://github.com/volcengine/verl/blob/master/examples/ray/tutorial.ipynb for more information. all_wg = {} wg_kwargs = {} # Setting up kwargs for RayWorkerGroup if OmegaConf.select(self.config.trainer, \"ray_wait_register_center_timeout\") is not None: wg_kwargs[\"ray_wait_register_center_timeout\"] = self.config.trainer.ray_wait_register_center_timeout if OmegaConf.select(self.config.trainer, \"profile_steps\") is not None: wg_kwargs[\"profile_steps\"] = OmegaConf.select(self.config.trainer, \"profile_steps\") assert OmegaConf.select(self.config.trainer, \"worker_nsight_options\") is not None, \"worker_nsight_options must be set when profile_steps is set\" wg_kwargs[\"worker_nsight_options\"] = OmegaConf.to_container(OmegaConf.select(self.config.trainer, \"worker_nsight_options\")) for resource_pool, class_dict in self.resource_pool_to_cls.items(): worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict) wg_dict = self.ray_worker_group_cls(resource_pool=resource_pool, ray_cls_with_init=worker_dict_cls, device_name=self.device_name, **wg_kwargs) spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys()) all_wg.update(spawn_wg) if self.use_critic: self.critic_wg = all_wg[\"critic\"] self.critic_wg.init_model() if self.use_reference_policy and not self.ref_in_actor: self.ref_policy_wg = all_wg[\"ref\"] self.ref_policy_wg.init_model() if self.use_rm: self.rm_wg = all_wg[\"rm\"] self.rm_wg.init_model() # we should create rollout at the end so that vllm can have a better estimation of kv cache memory self.actor_rollout_wg = all_wg[\"actor_rollout\"] self.actor_rollout_wg.init_model() # create async rollout manager and request scheduler self.async_rollout_mode = False if self.config.actor_rollout_ref.rollout.mode == \"async\": from verl.workers.rollout.async_server import AsyncLLMServerManager self.async_rollout_mode = True self.async_rollout_manager = AsyncLLMServerManager( config=self.config, worker_group=self.actor_rollout_wg, ) ","wordCount":"5605","inLanguage":"en","image":"https://pillumina.github.io/imgs/icon_head.png","datePublished":"2025-08-03T15:30:12+08:00","dateModified":"2025-08-03T15:30:12+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://pillumina.github.io/posts/aiinfra/07-verl-multiturn-1/"},"publisher":{"@type":"Organization","name":"CctoctoFX","logo":{"@type":"ImageObject","url":"https://pillumina.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://pillumina.github.io/ accesskey=h title="CctoctoFX (Alt + H)"><img src=https://pillumina.github.io/apple-touch-icon.png alt aria-label=logo height=30>CctoctoFX</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://pillumina.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://pillumina.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://pillumina.github.io/posts/aiinfra/ title="AI Infra"><span>AI Infra</span></a></li><li><a href=https://pillumina.github.io/posts/llmtheory/ title=Thoery><span>Thoery</span></a></li><li><a href=https://pillumina.github.io/posts/programming/ title=Programming><span>Programming</span></a></li><li><a href=https://pillumina.github.io/social/ title=Social><span>Social</span></a></li><li><a href=https://pillumina.github.io/open_courses/ title=Study><span>Study</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://pillumina.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/aiinfra/>AI Infra</a></div><h1 class="post-title entry-hint-parent">[VeRL] Multi-Turn RL训练源码走读（1）</h1><div class=post-meta><span title='2025-08-03 15:30:12 +0800 CST'>August 3, 2025</span>&nbsp;·&nbsp;27 min&nbsp;·&nbsp;5605 words&nbsp;·&nbsp;Me</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#数据预处理><strong>数据预处理</strong></a></li><li><a href=#启动训练>启动训练</a></li><li><a href=#脚本配置>脚本配置</a></li><li><a href=#训练主入口与初始化>训练主入口与初始化</a><ul><li><a href=#ray-actorray-task-和-ray-worker>Ray Actor，Ray Task 和 Ray Worker</a></li><li><a href=#run_ppo-和-taskrunnerrun><code>run_ppo()</code> 和 <code>TaskRunner.run()</code></a></li><li><a href=#actorrolloutrefworker-和-rayworkergroup-的相互关系>ActorRolloutRefWorker 和 RayWorkerGroup 的相互关系</a></li><li><a href=#actorrolloutrefworker__init__><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/workers/fsdp_workers.py#L101><code>ActorRolloutRefWorker.__init__()</code></a></a></li><li><a href=#actorrolloutrefworker_build_model_optimizer><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/fsdp_workers.py#L177><code>ActorRolloutRefWorker._build_model_optimizer()</code></a></a></li><li><a href=#actorrolloutrefworker_build_rollout><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/fsdp_workers.py#L394><code>ActorRolloutRefWorker._build_rollout()</code></a></a></li><li><a href=#sglangrollout__init__><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L208><code>SGLangRollout.__init__()</code></a></a></li><li><a href=#sglangrolloutasyncengine><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L124><code>SGLangRollout.AsyncEngine</code></a></a></li><li><a href=#sglangrollout_init_inference_engine><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L325><code>SGLangRollout._init_inference_engine()</code></a></a></li><li><a href=#taskrunnerrun><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L64><code>TaskRunner.run()</code></a></a></li><li><a href=#rayppotrainer__init__><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L277><code>RayPPOTrainer.__init__()</code></a></a></li><li><a href=#rayppotrainerinit_workers><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L715><code>RayPPOTrainer.init_workers()</code></a></a></li></ul></li></ul></nav></div></details></div><div class=post-content><blockquote><p>该part主要聚焦相关模块初始化部分</p></blockquote><p>还是以 verl 出发，分析其 end to end mutli-turn RL 训练的全过程。整体上，我希望覆盖所有重要的 class 以及函数，更细粒度的代码不再展开。</p><p>为了前后内容的一致性，基于 <a href=https://github.com/volcengine/verl/commit/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39>76f63cffa5</a> 的 commit 进行分析。</p><p>虽然本文以分析 verl 的代码为主，写完之后我才意识到，系统设计问题是非常通用的。诸如“log probs 重计算”，“Rollout Engine 显存管理”等等系统设计，是各大 RL 框架都需要考虑的核心问题。</p><p>此外因为最近在学习SGLang的实现，本文的推理后端选择的是SGLang展开分析。</p><hr><p>整个训练的示意图如下，我们会具体展开每个部分。</p><pre class=mermaid>
  flowchart LR
subgraph W2[&#34;Initialize&#34;]
WP[Process Data] --&gt; A
direction TB D1[Data Prepare] --&gt; A
A[TaskRunner] --&gt; B1[RayPPOTrainer]
B1 --&gt; Workers

    subgraph Workers[&#34;Workers&#34;]
        direction TB
                WA[ActorRolloutWorker] --&gt; WD[FSDP Engine]
        WB[CriticWorker] --&gt; WD
        WC[RewardModelWorker] --&gt; WD
        WD --&gt; WE[SGLang Engine]
    end
    
    Workers --&gt; C1[Hybrid Engine]
end

subgraph W3[&#34;Train Loop&#34;]
    direction TB
    E[DataLoader] --&gt; RolloutBox
    
    subgraph RolloutBox[&#34;Rollout&#34;]
        F1[Prepare Data] --&gt; F2[SGLang Async Rollout]
        F2 --&gt; F3[Multi-turn Chat Process]
    end
    
    RolloutBox --&gt; ExpBox
    
    subgraph ExpBox[&#34;Make Experience&#34;]
        G1[Recompute Log Probs] --&gt; G2[Compute Reward]
        G2 --&gt; G3[Compute Advantage]
    end
    
    ExpBox --&gt; UpdateBox
    
    subgraph UpdateBox[&#34;Train The Model&#34;]
        H1[Load FSDP Model Weight] --&gt; H2[Compute Gradient]
        H2 --&gt; H3[Weights Update]
        H3 --&gt; H4[Sync Weights]
    end
    
    UpdateBox --&gt; E
end

W2 --&gt; W3
</pre><h2 id=数据预处理><strong>数据预处理</strong><a hidden class=anchor aria-hidden=true href=#数据预处理>#</a></h2><p>以 <a href=https://huggingface.co/datasets/openai/gsm8k>GSM8K</a> 为例，预处理脚本是 <code>examples/data_preprocess/gsm8k_multiturn_w_tool.py</code>。整个脚本只做了经典的 huggingface datasets mapping，核心逻辑如下：</p><ol><li>加载 openai/gsm8k 原始数据集（train/test）。</li><li>对每条原始数据，生成带有工具调用要求的 prompt（比如在 user turn 强调模型可以调用 <code>calc_gsm8k_reward</code> 工具，每个qa至少调用一次）。</li><li>同样对于每条原始数据，解析答案；将 ground truth 写入 extra_info 字段。</li><li>存储为 parquet 文件，分别保留为 train.parquet 和 test.parquet，默认路径为 <code>~/data/gsm8k/</code>。</li></ol><h2 id=启动训练>启动训练<a hidden class=anchor aria-hidden=true href=#启动训练>#</a></h2><p>一个典型的启动命令如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># now 用于生成实验启动的时间尾缀，避免重复启动实验时覆盖已有 wandb log</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>function</span> now<span class=o>()</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    date <span class=s1>&#39;+%Y-%m-%d-%H-%M&#39;</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>CUDA_VISIBLE_DEVICES</span><span class=o>=</span>0,1,2,3,4,5,6,7
</span></span><span class=line><span class=cl>nohup bash examples/sglang_multiturn/run_qwen2.5-3b_gsm8k_multiturn.sh <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    trainer.experiment_name<span class=o>=</span>qwen2.5-3b_rm-gsm8k-sgl-multiturn-<span class=nv>$now</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    &gt; logs/gsm8k-<span class=nv>$now</span>.log 2&gt;<span class=p>&amp;</span><span class=m>1</span> <span class=p>&amp;</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=脚本配置>脚本配置<a hidden class=anchor aria-hidden=true href=#脚本配置>#</a></h2><p>verl 的各项参数实属复杂，我们会单独编写文档来分享对 verl 各类参数的理解。在这篇文档中，我们想要格外强调的是 verl 各类 config 的覆盖关系。verl 的配置文件利用 hydra 进行了<strong>分层覆盖</strong>的设计模式。</p><details><summary>Hydra 简介</summary><p><a href=https://github.com/facebookresearch/hydra><strong>Hydra</strong></a> 是一个由 Facebook Research 开发的 Python 框架，旨在<strong>优雅地配置复杂的应用程序</strong>。它特别适用于需要管理大量参数和进行多组实验的场景，例如机器学习项目。Hydra 的核心特点在于其<strong>动态、分层和可组合的配置管理能力</strong>。Hydra 的核心优势：</p><ul><li><strong>分层配置 (Hierarchical Configuration)</strong>：可以将配置分解成多个小型、模块化的 YAML 文件，并以目录结构进行组织。这使得配置更加清晰、易于管理和复用。</li><li><strong>配置组合 (Configuration Composition)</strong>：Hydra 能够将这些独立的配置模块动态地组合起来，形成一个完整的配置对象。你可以通过在主配置文件中指定 <code>defaults</code> 列表来选择和组合不同的配置组件。</li><li><strong>命令行覆盖 (Command-line Overrides)</strong>：这是 Hydra 最强大的功能之一。你可以在运行应用程序时，直接通过命令行参数来覆盖配置中的任何值。这使得进行实验和快速迭代变得非常方便，无需修改配置文件本身。</li><li><strong>多运行模式 (Multi-run)</strong>：Hydra 允许你通过一个命令运行多个具有不同配置的实验。这对于超参数搜索和模型比较非常有用。</li><li><strong>动态工作目录 (Dynamic Working Directory)</strong>：每次运行应用程序时，Hydra 都会自动创建一个独立的工作目录，并将当前运行的配置和输出保存到该目录中，确保实验的可复现性。</li><li><strong>对象实例化 (Object Instantiation)</strong>：Hydra 可以直接从配置中实例化 Python 对象（类或函数），这大大简化了代码，使配置更具声明性。</li></ul><p>Hydra 实现分层覆盖的主要机制是<strong>组合 (Composition)</strong> 和 <strong>命令行覆盖 (Command-line Overrides)</strong>。</p><ol><li><strong>分层配置的组织</strong>：</li></ol><p>通常会创建一个 <code>conf</code> 目录，并在其中组织配置。例如：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=l>.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=l>├── my_app.py</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=l>└── conf</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=l>├── config.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=l>├── model</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=l>│   ├── cnn.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=l>│   └── rnn.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=l>└── dataset</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=l>├── cifar10.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=l>└── imagenet.yaml</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p><code>config.yaml</code> 是你的主配置文件。在 <code>model</code> 目录下，你可以定义不同的模型配置（如 <code>cnn.yaml</code>、<code>rnn.yaml</code>），在 <code>dataset</code> 目录下定义不同的数据集配置（如 <code>cifar10.yaml</code>、<code>imagenet.yaml</code>）。</p><ol start=2><li><strong><code>defaults</code> 列表进行组合</strong>：</li></ol><p>在 <code>config.yaml</code> 中，你可以使用特殊的 <code>defaults</code> 列表来指定默认加载哪些配置组件。</p><p><strong><code>conf/config.yaml</code> 示例：</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>defaults</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>model</span><span class=p>:</span><span class=w> </span><span class=l>cnn      </span><span class=w> </span><span class=c># 默认加载 conf/model/cnn.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>dataset</span><span class=p>:</span><span class=w> </span><span class=l>cifar10</span><span class=w> </span><span class=c># 默认加载 conf/dataset/cifar10.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>_self_         </span><span class=w> </span><span class=c># 确保当前文件中的其他配置项也被加载</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># 其他应用级别的默认配置</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>learning_rate</span><span class=p>:</span><span class=w> </span><span class=m>0.001</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>epochs</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>当 Hydra 加载 <code>config.yaml</code> 时，它会根据 <code>defaults</code> 列表中的指示，自动将 <code>conf/model/cnn.yaml</code> 和 <code>conf/dataset/cifar10.yaml</code> 的内容合并到最终的配置对象中。</p><ol start=3><li><strong>命令行覆盖</strong>：</li></ol><p>这是实现灵活覆盖的关键。你可以通过命令行参数来覆盖任何已加载的配置值，包括在 <code>defaults</code> 列表中指定的组件或其内部的任何参数。</p><ul><li><strong>覆盖整个配置组</strong>：<br>要切换模型从 <code>cnn</code> 到 <code>rnn</code>，你可以在命令行中这样运行：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python my_app.py <span class=nv>model</span><span class=o>=</span>rnn
</span></span></code></pre></td></tr></table></div></div><p>这将指示 Hydra 加载 <code>conf/model/rnn.yaml</code>，并用它来替换默认的 <code>cnn</code> 配置。</p><ul><li><strong>覆盖特定参数</strong>：<br>你可以深入到配置的任何层级来覆盖特定的参数。例如，如果你想修改学习率或数据集的某个参数：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python my_app.py <span class=nv>learning_rate</span><span class=o>=</span>0.01 dataset.batch_size<span class=o>=</span><span class=m>64</span>
</span></span></code></pre></td></tr></table></div></div><p>这里，<code>learning_rate</code> 直接覆盖了 <code>config.yaml</code> 中的值，而 <code>dataset.batch_size</code> 则覆盖了 <code>conf/dataset/cifar10.yaml</code>（或者你通过 <code>dataset=imagenet</code> 指定的其他数据集配置文件）中的 <code>batch_size</code> 参数。</p><ul><li><strong>添加新参数 (使用 <code>+</code>)</strong>：<br>如果你想添加一个在默认配置中不存在的新参数，可以使用 <code>+</code> 前缀：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python my_app.py +optimizer.name<span class=o>=</span>AdamW
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>动态覆盖 (使用 <code>++</code>)</strong>：<br>如果你希望修改一个已有字段，或者在原配置中没有该字段时自动创建它，可以使用 ++。这种方式适用于需要动态添加或覆盖配置项的场景，确保字段总是被设置为你指定的值，无论它是否已存在。</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python my_app.py ++model.num_layers<span class=o>=</span><span class=m>10</span>
</span></span></code></pre></td></tr></table></div></div><p>Hydra 内部使用 <a href="https://www.google.com/search?q=https://omegaconf.readthedocs.io/en/2.3_latest/">OmegaConf</a> 库来处理这些配置对象，它提供了强大的合并和解析功能，使得分层覆盖和值插值（例如，引用其他配置值或环境变量）变得非常容易。</p></details><p>回到 verl multi turn，在我们启动的 <code>run_qwen2.5-3b_gsm8k_multiturn.sh</code> 中，设置了：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>PROJECT_DIR</span><span class=o>=</span><span class=s2>&#34;</span><span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=nv>CONFIG_PATH</span><span class=o>=</span><span class=s2>&#34;</span><span class=nv>$PROJECT_DIR</span><span class=s2>/examples/sglang_multiturn/config&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>python3 -m verl.trainer.main_ppo <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --config-path<span class=o>=</span><span class=s2>&#34;</span><span class=nv>$CONFIG_PATH</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --config-name<span class=o>=</span><span class=s1>&#39;gsm8k_multiturn_grpo&#39;</span> <span class=se>\
</span></span></span></code></pre></td></tr></table></div></div><p>这意味着这次任务的默认 config 是 <code>CONFIG_PATH</code> 下的 <code>gsm8k_multiturn_grpo.yaml</code>，且接下来的参数会覆盖 <code>gsm8k_multiturn_grpo.yaml</code> 中的默认值。更进一步，我们来观察 <code>gsm8k_multiturn_grpo.yaml</code> 的内容：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>hydra</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>searchpath</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>file://verl/trainer/config</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>defaults</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>ppo_trainer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>_self_</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>data</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>max_prompt_length</span><span class=p>:</span><span class=w> </span><span class=m>1024</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>max_response_length</span><span class=p>:</span><span class=w> </span><span class=m>1024</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>train_batch_size</span><span class=p>:</span><span class=w> </span><span class=m>256</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>return_raw_chat</span><span class=p>:</span><span class=w> </span><span class=kc>True</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>actor_rollout_ref</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>hybrid_engine</span><span class=p>:</span><span class=w> </span><span class=kc>True</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>rollout</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>sglang</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>multi_turn</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>enable</span><span class=p>:</span><span class=w> </span><span class=kc>True</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>max_turns</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=c># tool_config_path: &#34;./config/tool_config/gsm8k_tool_config.yaml&#34;</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>这里 hydra 语法，会去 <code>verl/trainer/config</code> 目录下寻找 <code>ppo_trainer.yaml</code> 作为基础配置，并且覆盖。因此，启动 <code>run_qwen2.5-3b_gsm8k_multiturn.sh</code> 时，先加载 <code>gsm8k_multiturn_grpo.yaml</code> 作为基础配置并覆盖，然后加载 <code>ppo_trainer.yaml</code> 并覆盖。最终合并这三级配置，得到最终的 config。</p><p>最后，注意到在 <code>run_qwen2.5-3b_gsm8k_multiturn.sh</code> 的最后，我们，我们设置了 <code>actor_rollout_ref.rollout.multi_turn.tool_config_path="$PROJECT_DIR/examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml"</code>，这里指定 multi_turn 的 tool_config_path 为 <code>examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml</code>。这一文件仅仅配置了 gsm8k 的 tool 调用，并不会覆盖之前训练的 config。</p><h2 id=训练主入口与初始化>训练主入口与初始化<a hidden class=anchor aria-hidden=true href=#训练主入口与初始化>#</a></h2><h3 id=ray-actorray-task-和-ray-worker>Ray Actor，Ray Task 和 Ray Worker<a hidden class=anchor aria-hidden=true href=#ray-actorray-task-和-ray-worker>#</a></h3><p>在介绍 verl 的训练主入口之前，我们先介绍 Ray 的一些核心概念。Ray 是一个统一计算框架，旨在实现简单地从单机到大型分布式集群的扩展，提供构建和运行分布式应用的底层基础设施和一组核心原语。Ray 通过以下功能实现这一目标：</p><ol><li><strong>统一 API</strong>：Ray 提供了一套简单易用的 Python API，将普通函数转换为分布式任务，将 Python 类转换为分布式服务，也即 Ray Actor。Ray Actor 内部持久存储的数据称为状态，可以在 Actor 的整个生命周期内被多次访问、修改和维护，而不会在每次方法调用结束后消失。</li><li><strong>弹性伸缩</strong>：Ray 可以将应用从单个机器无缝扩展到拥有数千个节点的集群，并能根据需求自动扩缩容。</li><li><strong>容错性</strong>：Ray 内置了容错机制，可以处理节点故障和任务失败，确保应用的健壮性。</li><li><strong>性能优化</strong>：Ray 优化了分布式任务调度、内存管理和数据传输，以实现高效的并行计算。</li></ol><p>Ray Task 和 Ray Actor 都是用于分布式计算的核心原语，但它们各自服务于不同的目的，主要区别在于<strong>是否维护状态</strong>。</p><p>Ray Task 是 Ray 中最基本的计算单元，代表一个无状态的远程函数。Ray Task 的每次执行都是独立的，不保留之前的任何信息。就像调用一个普通函数，执行完后就清除内部状态。我们调用一个 Ray Task 后，会立即返回得到一个 Ray ObjectRef，而不是实际的结果。主程序可以继续执行其他操作，而 Ray Task 则在后台并行运行。我们需要使用 <code>ray.get()</code> 来获取 Task 的实际结果。 Ray Task 非常适合并行执行大量独立、一次性的计算任务，譬如数据批处理、独立的模型推理等场景。</p><p>Ray Actor 是一种特殊的 Ray Task，正如前文所述，它是一个持续运行的、有自己的状态和方法的远程对象。当我们创建一个 Ray Actor 后，Ray 会在集群中的某个 <strong>Ray Worker</strong> 上启动一个专门的进程来托管这个对象。该进程会一直运行，直到被销毁。Actor 可以维护内部变量，并且这些变量在 Actor 的生命周期内是持久存在的。每次调用 Actor 的方法，都可以访问和修改这些状态。这与普通的 Ray Task 不同，普通 Task 执行完会清除内部状态。Ray Actor 支持并发请求，Ray 会负责将这些请求序列化执行，保证 Actor 内部状态的一致性和线程安全。我们可以通过 <code>@ray.remote</code> 装饰器将一个 Python 类转换为一个 Ray Actor 类，然后通过 <code>.remote()</code> 方法实例化一个远程 Actor。</p><p>最后，Ray Worker 是 Ray 集群中真正执行代码的工作单元。一个 Ray 集群通常由一个 Head Node 和多个 Worker Nodes 组成。每个节点上都会运行一个或多个 Ray Worker 进程。无论是普通的 Ray Task 还是 Ray Actor 的方法，最终都是由 Ray Worker 进程来执行的。每个 Ray Worker 都会被分配一定的计算资源（如 CPU、GPU）。当你提交一个 Ray Task 或创建一个 Ray Actor 时，Ray 的调度器会找到一个有足够资源的 Worker 来运行它。Worker 进程之间以及 Worker 进程与头节点之间会进行通信，以协调任务执行、传输数据和管理状态。一个 Ray Worker 通常就是一个独立的 Python 进程。对于普通的 Ray Task，Ray Worker 相当于函数解释器，执行完任务后可能会被复用去执行其他任务。而对于 Ray Actor，Ray 会启动一个专门的 Worker 进程来托管这个 Actor，这个 Worker 进程的生命周期与 Actor 的生命周期绑定。</p><h3 id=run_ppo-和-taskrunnerrun><code>run_ppo()</code> 和 <code>TaskRunner.run()</code><a hidden class=anchor aria-hidden=true href=#run_ppo-和-taskrunnerrun>#</a></h3><p>有了 ray 的概念，我们回到整个 RL 训练流程的起点：<code>verl.trainer.main_ppo.py</code> 中的 <a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L35><code>run_ppo()</code></a>，它负责初始化 Ray 集群，配置 CPU 资源和运行时环境变量，并创建远程 TaskRunner 实例。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>run_ppo</span><span class=p>(</span><span class=n>config</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># 初始化 Ray 集群，配置 CPU 资源和运行时环境变量</span>
</span></span><span class=line><span class=cl>    <span class=n>ray</span><span class=o>.</span><span class=n>init</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>runtime_env</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;env_vars&#34;</span><span class=p>:</span> <span class=p>{</span><span class=o>...</span><span class=p>}},</span>
</span></span><span class=line><span class=cl>        <span class=n>num_cpus</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>ray_init</span><span class=o>.</span><span class=n>num_cpus</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 创建远程 TaskRunner 实例</span>
</span></span><span class=line><span class=cl>    <span class=c1># TaskRunner 是 Ray 中的一个远程 actor，它将在 Ray 集群上异步执行主要的训练任务</span>
</span></span><span class=line><span class=cl>    <span class=n>runner</span> <span class=o>=</span> <span class=n>TaskRunner</span><span class=o>.</span><span class=n>remote</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=c1># 异步执行远程任务 runner.run()，并等待其完成</span>
</span></span><span class=line><span class=cl>    <span class=c1># 通过 ray.get() 阻塞直到远程任务执行完毕，确保整个初始化流程的顺序性</span>
</span></span><span class=line><span class=cl>    <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>runner</span><span class=o>.</span><span class=n>run</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>config</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=actorrolloutrefworker-和-rayworkergroup-的相互关系>ActorRolloutRefWorker 和 RayWorkerGroup 的相互关系<a hidden class=anchor aria-hidden=true href=#actorrolloutrefworker-和-rayworkergroup-的相互关系>#</a></h3><p><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L64>TaskRunner</a> 是 verl 中实现 PPO/GRPO 训练的核心组件，它通过将整个 RL 训练流程封装在一个独立的 Ray Actor 中，实现了任务的封装、资源隔离和分布式协调。为了解释清楚 <code>TaskRunner</code>，我们将 verl 当中最让人费解且最复杂的 <code>ActorRolloutRefWorker</code> 和 <code>RayWorkerGroup</code> 这两个类提前解释清楚。</p><p>我们先不讨论这两个类及其基类的具体意义，先讨论清楚其实例对象的创建过程。我们注意到这段 <code>TaskRunner</code> 的初始化中引入 <code>ActorRolloutRefWorker</code> 和 <code>RayWorkerGroup</code> 的相关代码：</p><details><summary>TaskRunner 中引入 ActorRolloutRefWorker</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Define worker classes based on the actor strategy.</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;fsdp&#34;</span><span class=p>,</span> <span class=s2>&#34;fsdp2&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>config</span><span class=o>.</span><span class=n>critic</span><span class=o>.</span><span class=n>strategy</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;fsdp&#34;</span><span class=p>,</span> <span class=s2>&#34;fsdp2&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>verl.single_controller.ray</span> <span class=kn>import</span> <span class=n>RayWorkerGroup</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>verl.workers.fsdp_workers</span> <span class=kn>import</span> <span class=n>ActorRolloutRefWorker</span><span class=p>,</span> <span class=n>AsyncActorRolloutRefWorker</span><span class=p>,</span> <span class=n>CriticWorker</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>actor_rollout_cls</span> <span class=o>=</span> <span class=n>AsyncActorRolloutRefWorker</span> <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>mode</span> <span class=o>==</span> <span class=s2>&#34;async&#34;</span> <span class=k>else</span> <span class=n>ActorRolloutRefWorker</span>
</span></span><span class=line><span class=cl>    <span class=n>ray_worker_group_cls</span> <span class=o>=</span> <span class=n>RayWorkerGroup</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>elif</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span> <span class=o>==</span> <span class=s2>&#34;megatron&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span> <span class=o>==</span> <span class=n>config</span><span class=o>.</span><span class=n>critic</span><span class=o>.</span><span class=n>strategy</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>verl.single_controller.ray.megatron</span> <span class=kn>import</span> <span class=n>NVMegatronRayWorkerGroup</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>verl.workers.megatron_workers</span> <span class=kn>import</span> <span class=n>ActorRolloutRefWorker</span><span class=p>,</span> <span class=n>AsyncActorRolloutRefWorker</span><span class=p>,</span> <span class=n>CriticWorker</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>actor_rollout_cls</span> <span class=o>=</span> <span class=n>AsyncActorRolloutRefWorker</span> <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>mode</span> <span class=o>==</span> <span class=s2>&#34;async&#34;</span> <span class=k>else</span> <span class=n>ActorRolloutRefWorker</span>
</span></span><span class=line><span class=cl>    <span class=n>ray_worker_group_cls</span> <span class=o>=</span> <span class=n>NVMegatronRayWorkerGroup</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>raise</span> <span class=ne>NotImplementedError</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>verl.trainer.ppo.ray_trainer</span> <span class=kn>import</span> <span class=n>ResourcePoolManager</span><span class=p>,</span> <span class=n>Role</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Map roles to their corresponding remote worker classes.</span>
</span></span><span class=line><span class=cl><span class=n>role_worker_mapping</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>:</span> <span class=n>ray</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>actor_rollout_cls</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>Role</span><span class=o>.</span><span class=n>Critic</span><span class=p>:</span> <span class=n>ray</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>CriticWorker</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define the resource pool specification.</span>
</span></span><span class=line><span class=cl><span class=c1># Map roles to the resource pool.</span>
</span></span><span class=line><span class=cl><span class=n>global_pool_id</span> <span class=o>=</span> <span class=s2>&#34;global_pool&#34;</span>
</span></span><span class=line><span class=cl><span class=n>resource_pool_spec</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>global_pool_id</span><span class=p>:</span> <span class=p>[</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=o>.</span><span class=n>n_gpus_per_node</span><span class=p>]</span> <span class=o>*</span> <span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=o>.</span><span class=n>nnodes</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>mapping</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>:</span> <span class=n>global_pool_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>Role</span><span class=o>.</span><span class=n>Critic</span><span class=p>:</span> <span class=n>global_pool_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div></details><p>可以观察到，在 <code>TaskRunner</code> 的初始化中，会根据各类配置引入对应的 <code>ActorRolloutRefWorker / AsyncActorRolloutRefWorker</code> 类以及 <code>RayWorkerGroup / NVMegatronRayWorkerGroup</code> 类。对于 SGLang 而言，不存在 <code>AsyncActorRolloutRefWorker</code>。<code>ActorRolloutRefWorker</code> 类直接通过 <code>ray.remote(ActorRolloutRefWorker)</code> 创建一个远程的 Ray Actor，将其包装成一个 Ray Actor 类。此时还还没有创建任何实例，也没有分配资源。那么，<code>ActorRolloutRefWorker</code> 类到底在哪儿实例化并分配资源的呢？</p><p>实际上，在 <code>main_ppo.py</code> 的 <a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L172>172 行</a>，构造了 <code>RayPPOTrainer</code> 类，随后调用了 <code>RayPPOTrainer.init_workers()</code> 方法，我们进一步查看 <code>RayPPOTrainer.init_workers()</code> 方法的<a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L715>相关代码</a>，我们观察到，每一个 RL worker 类（比如 ActorRolloutRefWorker）都会创造一个 work group（verl 中的各种 wg 变量），随后调用每个 worker group 的 <code>init_model()</code> 方法，而这些 worker group 实际上都是 <code>RayWorkerGroup</code> 的实例。<code>RayWorkerGroup</code> 的核心作用是资源调度的核心中间层，统一了各种 RL worker（比如 ActorRolloutRefWorker、CriticWorker）的接口，进行统一管理：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># RayWorkerGroup 实例，指定资源池 并规定角色和对应的类</span>
</span></span><span class=line><span class=cl><span class=n>wg_dict</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ray_worker_group_cls</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>resource_pool</span><span class=o>=</span><span class=n>resource_pool</span><span class=p>,</span>  <span class=c1># 只需要指定资源池</span>
</span></span><span class=line><span class=cl>    <span class=n>ray_cls_with_init</span><span class=o>=</span><span class=n>worker_dict_cls</span><span class=p>,</span>  <span class=c1># 一个包含数个worker的类 （e.g. actor_roll， critic, ref）</span>
</span></span><span class=line><span class=cl>    <span class=n>device_name</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_name</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#通过.spawn()获取角色对Ray Actor实例的映射</span>
</span></span><span class=line><span class=cl><span class=n>wg_dict</span><span class=o>.</span><span class=n>spawn</span><span class=p>(</span><span class=n>prefix_set</span><span class=o>=</span><span class=n>class_dict</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 所有 worker 都通过相同的模式创建，我这里进行简化，实际上的代码比较繁琐</span>
</span></span><span class=line><span class=cl><span class=n>actor_rollout_wg</span> <span class=o>=</span> <span class=n>RayWorkerGroup</span><span class=p>(</span><span class=n>resource_pool</span><span class=p>,</span> <span class=n>actor_rollout_cls</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>critic_wg</span> <span class=o>=</span> <span class=n>RayWorkerGroup</span><span class=p>(</span><span class=n>resource_pool</span><span class=p>,</span> <span class=n>critic_cls</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ref_policy_wg</span> <span class=o>=</span> <span class=n>RayWorkerGroup</span><span class=p>(</span><span class=n>resource_pool</span><span class=p>,</span> <span class=n>ref_policy_cls</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><details><summary>各种 worker group 实际上的初始化</summary><p>这部分代码在 <a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L771><code>ray_trainer.py</code></a> 中：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 1. 为每个角色（例如 actor_rollout、critic、ref）指定用哪个类初始化 worker，并且说明在哪个资源池里分配它们</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>create_resource_pool</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span> <span class=o>=</span> <span class=p>{</span><span class=n>pool</span><span class=p>:</span> <span class=p>{}</span> <span class=k>for</span> <span class=n>pool</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>resource_pool_dict</span><span class=o>.</span><span class=n>values</span><span class=p>()}</span>
</span></span><span class=line><span class=cl><span class=n>resource_pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>get_resource_pool</span><span class=p>(</span><span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>actor_rollout_cls</span> <span class=o>=</span> <span class=n>RayClassWithInitArgs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>cls</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>role</span><span class=o>=</span><span class=s2>&#34;actor_rollout&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=p>[</span><span class=n>resource_pool</span><span class=p>][</span><span class=s2>&#34;actor_rollout&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>actor_rollout_cls</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=c1># 2. 根据资源池和角色，批量创建多个 worker 实例（Ray Actor）并统一管理它们，赋予对应的职责</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>resource_pool</span><span class=p>,</span> <span class=n>class_dict</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>worker_dict_cls</span> <span class=o>=</span> <span class=n>create_colocated_worker_cls</span><span class=p>(</span><span class=n>class_dict</span><span class=o>=</span><span class=n>class_dict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>wg_dict</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ray_worker_group_cls</span><span class=p>(</span><span class=n>resource_pool</span><span class=o>=</span><span class=n>resource_pool</span><span class=p>,</span> <span class=n>ray_cls_with_init</span><span class=o>=</span><span class=n>worker_dict_cls</span><span class=p>,</span> <span class=n>device_name</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_name</span><span class=p>,</span> <span class=o>**</span><span class=n>wg_kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>spawn_wg</span> <span class=o>=</span> <span class=n>wg_dict</span><span class=o>.</span><span class=n>spawn</span><span class=p>(</span><span class=n>prefix_set</span><span class=o>=</span><span class=n>class_dict</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=n>all_wg</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>spawn_wg</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=c1># 3.调用 init_model() 完成模型加载</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_critic</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>critic_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;critic&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>critic_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_reference_policy</span> <span class=ow>and</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>ref_in_actor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>ref_policy_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>ref_policy_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_rm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>rm_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;rm&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>rm_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># we should create rollout at the end so that vllm can have a better estimation of kv cache memory</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>actor_rollout_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;actor_rollout&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>actor_rollout_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># create async rollout manager and request scheduler</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>async_rollout_mode</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>mode</span> <span class=o>==</span> <span class=s2>&#34;async&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>verl.workers.rollout.async_server</span> <span class=kn>import</span> <span class=n>AsyncLLMServerManager</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>async_rollout_mode</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>async_rollout_manager</span> <span class=o>=</span> <span class=n>AsyncLLMServerManager</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>worker_group</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>actor_rollout_wg</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>注意到 <code>ray_worker_group_cls</code> 就是 <code>RayWorkerGroup</code> 类，而 <code>worker_dict_cls</code> 就是 <code>ActorRolloutRefWorker</code> 类，所以我的简化是很合理的。</p></details><p>如此以来，<code>ActorRolloutRefWorker</code> 委托给 <code>RayWorkerGroup</code> 进行初始化。<code>RayWorkerGroup</code> 这个类就是专门用于资源调度的。通过其统一的 <code>_init_with_resource_pool</code> <a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/single_controller/ray/base.py#L313>方法</a>，为每个 GPU 创建一个 worker，最终实例化每种 RL worker 并分配资源。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_init_with_resource_pool</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>resource_pool</span><span class=p>,</span> <span class=n>ray_cls_with_init</span><span class=p>,</span> <span class=o>...</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 从 Ray 申请 Placement Groups</span>
</span></span><span class=line><span class=cl>    <span class=n>pgs</span> <span class=o>=</span> <span class=n>resource_pool</span><span class=o>.</span><span class=n>get_placement_groups</span><span class=p>(</span><span class=n>strategy</span><span class=o>=</span><span class=n>strategy</span><span class=p>,</span> <span class=n>device_name</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 为每个 GPU 创建一个 worker</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>local_rank</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>local_world_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>worker</span> <span class=o>=</span> <span class=n>ray_cls_with_init</span><span class=p>(</span><span class=n>placement_group</span><span class=o>=</span><span class=n>pg</span><span class=p>,</span> <span class=n>placement_group_bundle_idx</span><span class=o>=</span><span class=n>local_rank</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_workers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>worker</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>读到这里，我们基本对 verl 有了一些感觉。注意到，在 verl 当中有两个带有 Worker 的 base class，一个就叫做 <a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/single_controller/base/worker.py#L77><code>Worker</code></a>，另一个叫做 <a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/single_controller/base/worker_group.py#L121><code>WorkerGroup</code></a>。<code>Worker</code> 是 RL 里面的逻辑类（比如 actor 和 critic）,实际管理 RL 的数据流，而 <code>WorkerGroup</code> 只用于分布式系统的资源调度。</p><p>此外，从 <code>actor_rollout_wg</code> 和 <code>ref_policy_wg</code> 的实例化当中，也能看出一些学问。在 <code>ActorRolloutRefWorker</code> 的设计当中，Actor Training，Actor Rollout 和 Reference model 是用同一个 worker class 进行管理的。但是，之后委托给 <code>RayWorkerGroup</code> 创建 worker group 并且调用资源的时候，Actor Training 和 Actor Rollout 是由同一组 <code>RayWorkerGroup</code> 进行资源管理的（这二者本来就要被放在同一个资源组上做 hybird engine），而 Reference Model 是由另一组 <code>RayWorkerGroup</code> 管理资源的。</p><p>最后，我去问了相关开发者，他们也认为把 Actor Rollout，Actor Training 和 Reference Model 放在同一个 worker 里是 bad design 😂，不用纠结这种设计是否有什么高瞻远瞩，完全没有。</p><h3 id=actorrolloutrefworker__init__><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/workers/fsdp_workers.py#L101><code>ActorRolloutRefWorker.__init__()</code></a><a hidden class=anchor aria-hidden=true href=#actorrolloutrefworker__init__>#</a></h3><p>如前文所说，<code>ActorRolloutRefWorker</code> 是 verl 中用于管理 Actor Training，Actor Rollout 和 Reference Model 的 worker class。我们具体来分析其逻辑上实现的功能。注意，本文档只分析 FSDP backend 下的实现，megatron 留作后文。</p><ol><li>调用 Worker 基类的构造函数，并保存配置。</li><li>如果 PyTorch 分布式环境尚未初始化，则进行初始化，包括设置通信后端和进程组。</li><li>为 FSDP 创建设备网格，用于模型参数的分片。</li><li>如果启用 Ulysses 序列并行，则初始化其设备网格。</li><li>根据传入的 <code>role</code> 参数设置 Worker 的具体角色（actor, rollout, ref）。</li><li>根据 Worker 角色配置 profiler，用于性能分析。</li><li>配置 parameter offload 和 optimizer offload。</li><li>为 Actor，Rollout 和 Reference 分别 normalize batch size。</li></ol><p>第 8 步中配置了非常多的 batch size；verl 的 batch size 参数满天飞，虽然我个人认为名字基本是准确的，但是由于名字太像了，一定要做出一些区分。事实上，参数分析我们有单独的文档，我先把一部分内容提前公布了。</p><ol><li><code>data.train_batch_size</code>：在一次完整的 PPO 迭代（从 rollout 到 train）中，从数据集中采样并用于生成 experience 的总样本数量，决定了每次 policy 更新所依据的数据量。</li><li><code>actor_rollout_ref.actor.ppo_mini_batch_size</code>：这个参数的名字其实是准确的，因为 mini batch SGD 就是数据到达了一个 mini batch 就更新一次模型参数。在 verl 中，模型会在数据累积到一个 mini batch 后更新一次参数。</li><li><code>actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu</code>：这里其实是 gradient accumulation 的参数。由于一个 mini batch 的数据量可能仍然太大，无法一次性前向和反向传播，因此需要将其进一步拆分为 micro batch。每个 micro batch 会计算一次梯度并且累计，但是不会立刻更新模型参数。处理完整个 mini batch 后，才用累积的梯度进行一次参数更新。</li></ol><p>此外，在 verl 中，由于 verl 强调 SPMD 策略，可以理解为每个 RL worker 所占据的每个 GPU 上希望进行完全一致的操作，所以 verl 会要求每个 GPU 的 micro batch size 相同。因此，verl 会检查 train batch size / gpu 是否整除 <a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/trainer/ppo/ray_trainer.py#L363>(ref)</a>，如果不整除，则报错。这个设定其实完全没必要；对于 rollout 而言，SGLang 完全不需要发送的请求数量整除 DP 或者 TP size，更何况直接要整除 gpu 数量呢？但是，因为 verl 会用 all gather 从 rollout 的每个 worker 里收集数据，这就要求 rollout 的每个 worker 上分到的数据一致。更进一步，为了 SPMD，又要求 rollout 的每个 gpu 上分到的数据一致。最终，这就导致 verl 的 train batch size 必须整除 gpu 数量；在 GRPO 下是 real train batch size 需要整除 n gpus，等于 train batch size * sampling params 中的 n。</p><p>区分好 mini batch 和 micro batch 后，我也是最近才明白 PPO 中是如何维护 on policy 的。我之前一直以为我们都是在做严格 on policy 的训练，但是一个 train batch size 下有好几个 mini batch，似乎第一个 mini batch 结束之后，目标策略（target policy，被训练的 policy）和行为策略（behavior policy，用于在环境中采样的 policy）就不一致了。一次采样会训练很多个 mini batch，从第一个 mini batch 结束就不是 on policy 了。事实也是如此，我们注意到 PPO 的 loss function：</p><p>$$ L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t) \right] $$</p><p>其中的 $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)}$，这是一个对优势函数的矫正比例，而 $\hat{A}<em>t$ 就是 advantage。对于 LLM 的 PPO 而言，$\pi</em>{\theta_{old}}(a_t | s_t)$ 代表着采样时 behavior policy 在给定 $s_t$ 时，选择 $a_t$ 的概率，而 $\pi_\theta(a_t | s_t)$ 就是 target policy 在训练中的每一步给定 $s_t$ 时，选择 $a_t$ 的概率。对 LLM 而言，<code>s_t</code> 是 prompt 前缀，而 <code>a_t</code> 仅仅是 prompt 后的那一个 token。这一概率其实就是 inference 得到的 log probs；我们将收集得到的 (prompt, action) 分别经过 target policy 和 behaviour policy 得到 log probs，然后二者 log probs 相减再取对数，就是矫正项的值。从而，即便第一个 mini batch 之后 target policy 就已经和 behaviour policy 不一致了，仍然可以通过 log probs 进行矫正，也即 importance sampling。</p><p>这样一来，又有了两个问题：log probs 应该如何得到？实际上每次采样时都是发送给 rollout 固定数量的 requests，如果每个 (prompt, action) 对都会计算一次 loss 的话，岂不是更长的 sequence 会计算更多次？</p><p>对于第一个问题，这又是经典的<a href=https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md#introduction>精度问题</a>。如同我在链接到的文章中所说的，rollout engine 目前只有采样得到的 token 能用，而得到的 log probs 以及 reward 精度都不够，不能用于训练。behaviour policy 和 target policy 为了做 importance sampling 所需的 log probs 都得用 training engine 重算。不过要算起来也不麻烦，在第一个 mini batch 启动前，这时候 target behaviour 是一致的，重算 log probs 并且存下来即可。</p><p>对于第二个问题，的确如此。一条很长的 prompt + answer 序列确实会产生非常多的 (prompt, action) 对，其中每个对都可以看作一个 (state, action) 对。而且理论上每个这样的 (prompt, action) 对都会参与 Loss 的计算。这确实可能导致长序列中的 token 会在 Loss 计算中占据更大的比例，让模型过度关注长序列的优化，而对短序列的优化不足。不过，verl 的 rollout engine 会自动对每个 (prompt, action) 对进行加权，从而让长序列和短序列的 token 在 Loss 计算中占据相同的权重。为了缓解这种情况，有很多相关方法：</p><details><summary>样本加权方法</summary><p>序列级别加权： 一种直接的方法是在计算 Loss 时，给来自不同序列的样本赋予不同的权重。例如，给每个完整序列一个固定的权重（比如 1），然后将这个权重均匀分配给该序列中的每个 (prompt, action) 对。这样，无论序列多长，它对总 Loss 的贡献都相同。如果一个序列有 N 个 token，那么每个 (prompt, action) 对的权重就是 1/N。</p><p>按长度分桶： 在数据收集后，可以根据序列长度对样本进行排序，并尝试将相似长度的序列放入同一个 mini-batch。这有助于提高计算效率，因为可以减少 padding，但对于解决 Loss 贡献不均衡的作用有限。</p><p>固定 Token 数量的批次： 最常见且有效的方法是构建批次时，不固定样本数量，而是固定批次中的总 token 数量。这样，一个 mini-batch 可能包含 4 条长序列，也可能包含 40 条短序列，确保每次更新时处理的总计算量和梯度来源的总 token 数是恒定的，从而缓解长短序列的不均衡问题。</p><p>Loss 归一化：在计算每个 mini-batch 的 Loss 时，可以将其除以该 mini-batch 中实际的 token 数量。这确保了 Loss 值不会仅仅因为批次中包含了更多 token 而增大，从而为不同大小的 mini-batches（如果不是按固定 token 数构建）提供一个公平的比较基础。</p><p>截断：设定一个 max_length 参数，限制模型生成的最大 token 数量。虽然这不直接解决已有长序列的权重问题，但可以防止生成过长的序列，从而限制极端不均衡的发生。</p></details><p>whatever，解释了这么多，顺着理解 verl 的框架进一步学习了 RL 算法和系统，这里其实和 multi-turn 都还没有关系，我们还是回到 <code>ActorRolloutRefWorker</code> 的源码上。</p><details><summary>ActorRolloutRefWorker.__init__ 源码</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>DictConfig</span><span class=p>,</span> <span class=n>role</span><span class=p>:</span> <span class=nb>str</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 初始化 Worker 基类</span>
</span></span><span class=line><span class=cl>        <span class=n>Worker</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 存储配置信息</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>config</span>
</span></span><span class=line><span class=cl>        <span class=kn>import</span> <span class=nn>torch.distributed</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 如果分布式环境尚未初始化，则进行初始化</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>is_initialized</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>rank</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;RANK&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>world_size</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;WORLD_SIZE&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>init_process_group</span><span class=p>(</span><span class=n>backend</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;cpu:gloo,</span><span class=si>{</span><span class=n>get_device_name</span><span class=p>()</span><span class=si>}</span><span class=s2>:</span><span class=si>{</span><span class=n>get_nccl_backend</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>rank</span><span class=o>=</span><span class=n>rank</span><span class=p>,</span> <span class=n>world_size</span><span class=o>=</span><span class=n>world_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 为 FSDP 构建设备网格</span>
</span></span><span class=line><span class=cl>        <span class=n>world_size</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>get_world_size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span> <span class=o>=</span> <span class=n>create_device_mesh</span><span class=p>(</span><span class=n>world_size</span><span class=o>=</span><span class=n>world_size</span><span class=p>,</span> <span class=n>fsdp_size</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>fsdp_config</span><span class=o>.</span><span class=n>fsdp_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 为 Ulysses 序列并行构建设备网格</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_device_mesh</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;ulysses_sequence_parallel_size&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>dp</span> <span class=o>=</span> <span class=n>world_size</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_device_mesh</span> <span class=o>=</span> <span class=n>init_device_mesh</span><span class=p>(</span><span class=n>device_name</span><span class=p>,</span> <span class=n>mesh_shape</span><span class=o>=</span><span class=p>(</span><span class=n>dp</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span><span class=p>),</span> <span class=n>mesh_dim_names</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;dp&#34;</span><span class=p>,</span> <span class=s2>&#34;sp&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 初始化 Ulysses 分片管理器</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sharding_manager</span> <span class=o>=</span> <span class=n>FSDPUlyssesShardingManager</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ulysses_device_mesh</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 获取 LoRA rank 和是否使用 LoRA 的标志</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_lora_rank</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;lora_rank&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_is_lora</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_lora_rank</span> <span class=o>&gt;</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 设置 Worker 角色和相关标志</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>role</span> <span class=o>=</span> <span class=n>role</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>role</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;actor&#34;</span><span class=p>,</span> <span class=s2>&#34;rollout&#34;</span><span class=p>,</span> <span class=s2>&#34;ref&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout_ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_is_actor</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>role</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;actor&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout_ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_is_rollout</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>role</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;rollout&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout_ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_is_ref</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>role</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;ref&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout_ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>profiler_config</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>ProfilerConfig</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=c1># 根据角色获取性能分析配置</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_actor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>profiler_config</span> <span class=o>=</span> <span class=n>omega_conf_to_dataclass</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;profiler&#34;</span><span class=p>,</span> <span class=p>{}),</span> <span class=n>ProfilerConfig</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_rollout</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>profiler_config</span> <span class=o>=</span> <span class=n>omega_conf_to_dataclass</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;profiler&#34;</span><span class=p>,</span> <span class=p>{}),</span> <span class=n>ProfilerConfig</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_ref</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>profiler_config</span> <span class=o>=</span> <span class=n>omega_conf_to_dataclass</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>ref</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;profiler&#34;</span><span class=p>,</span> <span class=p>{}),</span> <span class=n>ProfilerConfig</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 初始化分布式性能分析器</span>
</span></span><span class=line><span class=cl>        <span class=n>DistProfilerExtension</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>DistProfiler</span><span class=p>(</span><span class=n>rank</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>rank</span><span class=p>,</span> <span class=n>config</span><span class=o>=</span><span class=n>profiler_config</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 设置参数和优化器卸载标志</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_param</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_optimizer</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_actor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_param</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>fsdp_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;param_offload&#34;</span><span class=p>,</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_optimizer</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>fsdp_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;optimizer_offload&#34;</span><span class=p>,</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_ref</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_param</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>ref</span><span class=o>.</span><span class=n>fsdp_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;param_offload&#34;</span><span class=p>,</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 规范化 actor 相关配置</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_actor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span> <span class=o>*=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>n</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span> <span class=o>//=</span> <span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span>
</span></span><span class=line><span class=cl>            <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;ppo_mini_batch_size </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span><span class=si>}</span><span class=s2> should be larger than 0 after normalization&#34;</span>
</span></span><span class=line><span class=cl>            <span class=c1># micro bsz</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size</span> <span class=o>//=</span> <span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size_per_gpu</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size_per_gpu</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span> <span class=o>%</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size_per_gpu</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;normalized ppo_mini_batch_size </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span><span class=si>}</span><span class=s2> should be divisible by ppo_micro_batch_size_per_gpu </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size_per_gpu</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>                <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size_per_gpu</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;normalized ppo_mini_batch_size </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span><span class=si>}</span><span class=s2> should be larger than ppo_micro_batch_size_per_gpu </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size_per_gpu</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 规范化 rollout 相关配置</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_rollout</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>log_prob_micro_batch_size</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>log_prob_micro_batch_size</span> <span class=o>//=</span> <span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>log_prob_micro_batch_size_per_gpu</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>log_prob_micro_batch_size</span>
</span></span><span class=line><span class=cl>        <span class=c1># 规范化 ref 相关配置</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_ref</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>ref</span><span class=o>.</span><span class=n>log_prob_micro_batch_size</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>ref</span><span class=o>.</span><span class=n>log_prob_micro_batch_size</span> <span class=o>//=</span> <span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>ref</span><span class=o>.</span><span class=n>log_prob_micro_batch_size_per_gpu</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>ref</span><span class=o>.</span><span class=n>log_prob_micro_batch_size</span>
</span></span></code></pre></td></tr></table></div></div></details><h3 id=actorrolloutrefworker_build_model_optimizer><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/fsdp_workers.py#L177><code>ActorRolloutRefWorker._build_model_optimizer()</code></a><a hidden class=anchor aria-hidden=true href=#actorrolloutrefworker_build_model_optimizer>#</a></h3><p>这部分源码和类写的还是很直白的，不用太多解释：</p><ol><li>初始化 Hugging Face 配置，获取 Generation Config，并设置模型的数据类型（Actor 使用 fp32，Reference 使用 bf16）。</li><li>使用 Hugging Face 的 <code>AutoModelForCausalLM</code> 或 <code>AutoModelForVision2Seq</code> 从预训练模型加载基础模型。</li><li>应用各种优化技术，包括 Liger kernel、融合 kernel、梯度检查点、LoRA 等。</li><li>根据配置选择 FSDP 或 FSDP2 策略，将模型封装到分布式训练框架中，支持参数分片和混合精度训练。</li><li>如果当前 Worker 是 Actor 角色，则初始化 AdamW 优化器和学习率调度器。</li></ol><details><summary>ActorRolloutRefWorker._build_model_optimizer 源码</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span><span class=lnt>116
</span><span class=lnt>117
</span><span class=lnt>118
</span><span class=lnt>119
</span><span class=lnt>120
</span><span class=lnt>121
</span><span class=lnt>122
</span><span class=lnt>123
</span><span class=lnt>124
</span><span class=lnt>125
</span><span class=lnt>126
</span><span class=lnt>127
</span><span class=lnt>128
</span><span class=lnt>129
</span><span class=lnt>130
</span><span class=lnt>131
</span><span class=lnt>132
</span><span class=lnt>133
</span><span class=lnt>134
</span><span class=lnt>135
</span><span class=lnt>136
</span><span class=lnt>137
</span><span class=lnt>138
</span><span class=lnt>139
</span><span class=lnt>140
</span><span class=lnt>141
</span><span class=lnt>142
</span><span class=lnt>143
</span><span class=lnt>144
</span><span class=lnt>145
</span><span class=lnt>146
</span><span class=lnt>147
</span><span class=lnt>148
</span><span class=lnt>149
</span><span class=lnt>150
</span><span class=lnt>151
</span><span class=lnt>152
</span><span class=lnt>153
</span><span class=lnt>154
</span><span class=lnt>155
</span><span class=lnt>156
</span><span class=lnt>157
</span><span class=lnt>158
</span><span class=lnt>159
</span><span class=lnt>160
</span><span class=lnt>161
</span><span class=lnt>162
</span><span class=lnt>163
</span><span class=lnt>164
</span><span class=lnt>165
</span><span class=lnt>166
</span><span class=lnt>167
</span><span class=lnt>168
</span><span class=lnt>169
</span><span class=lnt>170
</span><span class=lnt>171
</span><span class=lnt>172
</span><span class=lnt>173
</span><span class=lnt>174
</span><span class=lnt>175
</span><span class=lnt>176
</span><span class=lnt>177
</span><span class=lnt>178
</span><span class=lnt>179
</span><span class=lnt>180
</span><span class=lnt>181
</span><span class=lnt>182
</span><span class=lnt>183
</span><span class=lnt>184
</span><span class=lnt>185
</span><span class=lnt>186
</span><span class=lnt>187
</span><span class=lnt>188
</span><span class=lnt>189
</span><span class=lnt>190
</span><span class=lnt>191
</span><span class=lnt>192
</span><span class=lnt>193
</span><span class=lnt>194
</span><span class=lnt>195
</span><span class=lnt>196
</span><span class=lnt>197
</span><span class=lnt>198
</span><span class=lnt>199
</span><span class=lnt>200
</span><span class=lnt>201
</span><span class=lnt>202
</span><span class=lnt>203
</span><span class=lnt>204
</span><span class=lnt>205
</span><span class=lnt>206
</span><span class=lnt>207
</span><span class=lnt>208
</span><span class=lnt>209
</span><span class=lnt>210
</span><span class=lnt>211
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_build_model_optimizer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>model_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>fsdp_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>optim_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>override_model_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>use_remove_padding</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>use_fused_kernels</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>enable_gradient_checkpointing</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>use_liger</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>role</span><span class=o>=</span><span class=s2>&#34;actor&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>enable_activation_offload</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>optim</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>torch.distributed.fsdp</span> <span class=kn>import</span> <span class=n>CPUOffload</span><span class=p>,</span> <span class=n>MixedPrecision</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoConfig</span><span class=p>,</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoModelForVision2Seq</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.utils.model</span> <span class=kn>import</span> <span class=n>get_generation_config</span><span class=p>,</span> <span class=n>print_model_size</span><span class=p>,</span> <span class=n>update_model_config</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.utils.torch_dtypes</span> <span class=kn>import</span> <span class=n>PrecisionType</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>role</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;actor&#34;</span><span class=p>,</span> <span class=s2>&#34;ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Before init </span><span class=si>{</span><span class=n>role</span><span class=si>}</span><span class=s2> from HF AutoModel&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>local_path</span> <span class=o>=</span> <span class=n>model_path</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>hf_tokenizer</span><span class=p>(</span><span class=n>local_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>processor</span> <span class=o>=</span> <span class=n>hf_processor</span><span class=p>(</span><span class=n>local_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>torch_dtype</span> <span class=o>=</span> <span class=n>fsdp_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;model_dtype&#34;</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>torch_dtype</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>torch_dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>float32</span> <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_actor</span> <span class=k>else</span> <span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>torch_dtype</span> <span class=o>=</span> <span class=n>PrecisionType</span><span class=o>.</span><span class=n>to_dtype</span><span class=p>(</span><span class=n>torch_dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>actor_model_config</span> <span class=o>=</span> <span class=n>AutoConfig</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>local_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>,</span> <span class=n>attn_implementation</span><span class=o>=</span><span class=s2>&#34;flash_attention_2&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>getattr</span><span class=p>(</span><span class=n>actor_model_config</span><span class=p>,</span> <span class=s2>&#34;model_type&#34;</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span> <span class=o>==</span> <span class=s2>&#34;kimi_vl&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_model_config</span><span class=o>.</span><span class=n>text_config</span><span class=o>.</span><span class=n>topk_method</span> <span class=o>=</span> <span class=s2>&#34;greedy&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>generation_config</span> <span class=o>=</span> <span class=n>get_generation_config</span><span class=p>(</span><span class=n>local_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>override_config_kwargs</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;bos_token_id&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>bos_token_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;eos_token_id&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;pad_token_id&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>override_config_kwargs</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>override_model_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>update_model_config</span><span class=p>(</span><span class=n>actor_model_config</span><span class=p>,</span> <span class=n>override_config_kwargs</span><span class=o>=</span><span class=n>override_config_kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 如果是 rank 0 进程，打印更新后的模型配置</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Model config after override: </span><span class=si>{</span><span class=n>actor_model_config</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>init_context</span> <span class=o>=</span> <span class=n>get_init_weight_context_manager</span><span class=p>(</span><span class=n>use_meta_tensor</span><span class=o>=</span><span class=ow>not</span> <span class=n>actor_model_config</span><span class=o>.</span><span class=n>tie_word_embeddings</span><span class=p>,</span> <span class=n>mesh</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>init_context</span><span class=p>(),</span> <span class=n>warnings</span><span class=o>.</span><span class=n>catch_warnings</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>warnings</span><span class=o>.</span><span class=n>simplefilter</span><span class=p>(</span><span class=s2>&#34;ignore&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=nb>type</span><span class=p>(</span><span class=n>actor_model_config</span><span class=p>)</span> <span class=ow>in</span> <span class=n>AutoModelForVision2Seq</span><span class=o>.</span><span class=n>_model_mapping</span><span class=o>.</span><span class=n>keys</span><span class=p>():</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module_class</span> <span class=o>=</span> <span class=n>AutoModelForVision2Seq</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module_class</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>actor_module</span> <span class=o>=</span> <span class=n>actor_module_class</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>pretrained_model_name_or_path</span><span class=o>=</span><span class=n>local_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch_dtype</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>config</span><span class=o>=</span><span class=n>actor_model_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>use_liger</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=kn>from</span> <span class=nn>liger_kernel.transformers.monkey_patch</span> <span class=kn>import</span> <span class=n>_apply_liger_kernel_to_instance</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>_apply_liger_kernel_to_instance</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>actor_module</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>fused_kernel_options</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;fused_kernel_options&#34;</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>fused_kernels_backend</span> <span class=o>=</span> <span class=n>fused_kernel_options</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;impl_backend&#34;</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span> <span class=k>if</span> <span class=n>fused_kernel_options</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>apply_monkey_patch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>model</span><span class=o>=</span><span class=n>actor_module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>use_remove_padding</span><span class=o>=</span><span class=n>use_remove_padding</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>ulysses_sp_size</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>use_fused_kernels</span><span class=o>=</span><span class=n>use_fused_kernels</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>fused_kernels_backend</span><span class=o>=</span><span class=n>fused_kernels_backend</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>actor_module</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>torch_dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>enable_gradient_checkpointing</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module</span><span class=o>.</span><span class=n>gradient_checkpointing_enable</span><span class=p>(</span><span class=n>gradient_checkpointing_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;use_reentrant&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>})</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_lora</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Applying LoRA to actor module&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module</span><span class=o>.</span><span class=n>enable_input_require_grads</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>lora_config</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;task_type&#34;</span><span class=p>:</span> <span class=n>TaskType</span><span class=o>.</span><span class=n>CAUSAL_LM</span><span class=p>,</span> <span class=s2>&#34;r&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>lora_rank</span><span class=p>,</span> <span class=s2>&#34;lora_alpha&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>lora_alpha</span><span class=p>,</span> <span class=s2>&#34;target_modules&#34;</span><span class=p>:</span> <span class=n>convert_to_regular_types</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>target_modules</span><span class=p>),</span> <span class=s2>&#34;bias&#34;</span><span class=p>:</span> <span class=s2>&#34;none&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module</span> <span class=o>=</span> <span class=n>get_peft_model</span><span class=p>(</span><span class=n>actor_module</span><span class=p>,</span> <span class=n>LoraConfig</span><span class=p>(</span><span class=o>**</span><span class=n>lora_config</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>barrier</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>print_model_size</span><span class=p>(</span><span class=n>actor_module</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;After init </span><span class=si>{</span><span class=n>role</span><span class=si>}</span><span class=s2> from HF AutoModel&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>mixed_precision_config</span> <span class=o>=</span> <span class=n>fsdp_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;mixed_precision&#34;</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>mixed_precision_config</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>param_dtype</span> <span class=o>=</span> <span class=n>PrecisionType</span><span class=o>.</span><span class=n>to_dtype</span><span class=p>(</span><span class=n>mixed_precision_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;param_dtype&#34;</span><span class=p>,</span> <span class=s2>&#34;bf16&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>reduce_dtype</span> <span class=o>=</span> <span class=n>PrecisionType</span><span class=o>.</span><span class=n>to_dtype</span><span class=p>(</span><span class=n>mixed_precision_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;reduce_dtype&#34;</span><span class=p>,</span> <span class=s2>&#34;fp32&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>buffer_dtype</span> <span class=o>=</span> <span class=n>PrecisionType</span><span class=o>.</span><span class=n>to_dtype</span><span class=p>(</span><span class=n>mixed_precision_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;buffer_dtype&#34;</span><span class=p>,</span> <span class=s2>&#34;fp32&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>param_dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span>
</span></span><span class=line><span class=cl>            <span class=n>reduce_dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>float32</span>
</span></span><span class=line><span class=cl>            <span class=n>buffer_dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>float32</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>mixed_precision</span> <span class=o>=</span> <span class=n>MixedPrecision</span><span class=p>(</span><span class=n>param_dtype</span><span class=o>=</span><span class=n>param_dtype</span><span class=p>,</span> <span class=n>reduce_dtype</span><span class=o>=</span><span class=n>reduce_dtype</span><span class=p>,</span> <span class=n>buffer_dtype</span><span class=o>=</span><span class=n>buffer_dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>auto_wrap_policy</span> <span class=o>=</span> <span class=n>get_fsdp_wrap_policy</span><span class=p>(</span><span class=n>module</span><span class=o>=</span><span class=n>actor_module</span><span class=p>,</span> <span class=n>config</span><span class=o>=</span><span class=n>fsdp_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;wrap_policy&#34;</span><span class=p>,</span> <span class=kc>None</span><span class=p>),</span> <span class=n>is_lora</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;lora_rank&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># TODO(zhangchi.usc1992, shengguangming) fix me. Current, auto_wrap_policy causes HFRollout to hang in Gemma</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_rollout</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>name</span> <span class=o>==</span> <span class=s2>&#34;hf&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>auto_wrap_policy</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 如果是 rank 0 进程，打印包装策略</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;wrap_policy: </span><span class=si>{</span><span class=n>auto_wrap_policy</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>fsdp_mesh</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span>
</span></span><span class=line><span class=cl>        <span class=n>sharding_strategy</span> <span class=o>=</span> <span class=n>get_sharding_strategy</span><span class=p>(</span><span class=n>fsdp_mesh</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># TODO: 添加 transformer 策略</span>
</span></span><span class=line><span class=cl>        <span class=c1># 我们强制 reference policy 使用 CPUOffload 来节省内存</span>
</span></span><span class=line><span class=cl>        <span class=c1># 我们强制关闭 actor 的 CPUOffload，因为它在使用 grad accumulation 时会导致不正确的结果</span>
</span></span><span class=line><span class=cl>        <span class=n>cpu_offload</span> <span class=o>=</span> <span class=kc>None</span> <span class=k>if</span> <span class=n>role</span> <span class=o>==</span> <span class=s2>&#34;actor&#34;</span> <span class=k>else</span> <span class=n>CPUOffload</span><span class=p>(</span><span class=n>offload_params</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 根据配置的策略，将模型封装到 FSDP 中</span>
</span></span><span class=line><span class=cl>        <span class=n>fsdp_strategy</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>fsdp_strategy</span> <span class=o>==</span> <span class=s2>&#34;fsdp&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_module_fsdp</span> <span class=o>=</span> <span class=n>FSDP</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>cpu_offload</span><span class=o>=</span><span class=n>cpu_offload</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>param_init_fn</span><span class=o>=</span><span class=n>init_fn</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>use_orig_params</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>auto_wrap_policy</span><span class=o>=</span><span class=n>auto_wrap_policy</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>device_id</span><span class=o>=</span><span class=n>get_device_id</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                <span class=n>sharding_strategy</span><span class=o>=</span><span class=n>sharding_strategy</span><span class=p>,</span>  <span class=c1># zero3</span>
</span></span><span class=line><span class=cl>                <span class=n>mixed_precision</span><span class=o>=</span><span class=n>mixed_precision</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>sync_module_states</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>device_mesh</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>forward_prefetch</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>fsdp_config</span><span class=o>.</span><span class=n>forward_prefetch</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>fsdp_strategy</span> <span class=o>==</span> <span class=s2>&#34;fsdp2&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>assert</span> <span class=n>CPUOffloadPolicy</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>,</span> <span class=s2>&#34;PyTorch version &gt;= 2.4 is required for using fully_shard API (FSDP2)&#34;</span>
</span></span><span class=line><span class=cl>            <span class=n>mp_policy</span> <span class=o>=</span> <span class=n>MixedPrecisionPolicy</span><span class=p>(</span><span class=n>param_dtype</span><span class=o>=</span><span class=n>param_dtype</span><span class=p>,</span> <span class=n>reduce_dtype</span><span class=o>=</span><span class=n>reduce_dtype</span><span class=p>,</span> <span class=n>cast_forward_inputs</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>role</span> <span class=o>==</span> <span class=s2>&#34;actor&#34;</span> <span class=ow>and</span> <span class=n>fsdp_config</span><span class=o>.</span><span class=n>offload_policy</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>cpu_offload</span> <span class=o>=</span> <span class=n>CPUOffloadPolicy</span><span class=p>(</span><span class=n>pin_memory</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_param</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_optimizer</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>cpu_offload</span> <span class=o>=</span> <span class=kc>None</span> <span class=k>if</span> <span class=n>role</span> <span class=o>==</span> <span class=s2>&#34;actor&#34;</span> <span class=k>else</span> <span class=n>CPUOffloadPolicy</span><span class=p>(</span><span class=n>pin_memory</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>fsdp_kwargs</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;mesh&#34;</span><span class=p>:</span> <span class=n>fsdp_mesh</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;mp_policy&#34;</span><span class=p>:</span> <span class=n>mp_policy</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;offload_policy&#34;</span><span class=p>:</span> <span class=n>cpu_offload</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;reshard_after_forward&#34;</span><span class=p>:</span> <span class=n>fsdp_config</span><span class=o>.</span><span class=n>reshard_after_forward</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=n>full_state</span> <span class=o>=</span> <span class=n>actor_module</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>apply_fsdp2</span><span class=p>(</span><span class=n>actor_module</span><span class=p>,</span> <span class=n>fsdp_kwargs</span><span class=p>,</span> <span class=n>fsdp_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>fsdp2_load_full_state_dict</span><span class=p>(</span><span class=n>actor_module</span><span class=p>,</span> <span class=n>full_state</span><span class=p>,</span> <span class=n>fsdp_mesh</span><span class=p>,</span> <span class=n>cpu_offload</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_module_fsdp</span> <span class=o>=</span> <span class=n>actor_module</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>NotImplementedError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;not implement </span><span class=si>{</span><span class=n>fsdp_strategy</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 如果启用了激活卸载，则启用它</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>enable_activation_offload</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>enable_activation_offloading</span><span class=p>(</span><span class=n>actor_module_fsdp</span><span class=p>,</span> <span class=n>fsdp_strategy</span><span class=p>,</span> <span class=n>enable_gradient_checkpointing</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 记录 FSDP 初始化之后的 GPU 内存使用情况</span>
</span></span><span class=line><span class=cl>        <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;After </span><span class=si>{</span><span class=n>role</span><span class=si>}</span><span class=s2> FSDP init&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># TODO: add more optimizer args into config</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>role</span> <span class=o>==</span> <span class=s2>&#34;actor&#34;</span> <span class=ow>and</span> <span class=n>optim_config</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=kn>from</span> <span class=nn>verl.utils.torch_functional</span> <span class=kn>import</span> <span class=n>get_constant_schedule_with_warmup</span><span class=p>,</span> <span class=n>get_cosine_schedule_with_warmup</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>actor_optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module_fsdp</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                <span class=n>lr</span><span class=o>=</span><span class=n>optim_config</span><span class=o>.</span><span class=n>lr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>betas</span><span class=o>=</span><span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;betas&#34;</span><span class=p>,</span> <span class=p>(</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>                <span class=n>weight_decay</span><span class=o>=</span><span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;weight_decay&#34;</span><span class=p>,</span> <span class=mf>1e-2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>total_steps</span> <span class=o>=</span> <span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;total_training_steps&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>num_warmup_steps</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;lr_warmup_steps&#34;</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>warmup_style</span> <span class=o>=</span> <span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;warmup_style&#34;</span><span class=p>,</span> <span class=s2>&#34;constant&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>min_lr_ratio</span> <span class=o>=</span> <span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;min_lr_ratio&#34;</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>num_cycles</span> <span class=o>=</span> <span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;num_cycles&#34;</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>num_warmup_steps</span> <span class=o>&lt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>num_warmup_steps_ratio</span> <span class=o>=</span> <span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;lr_warmup_steps_ratio&#34;</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>num_warmup_steps</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_warmup_steps_ratio</span> <span class=o>*</span> <span class=n>total_steps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Total steps: </span><span class=si>{</span><span class=n>total_steps</span><span class=si>}</span><span class=s2>, num_warmup_steps: </span><span class=si>{</span><span class=n>num_warmup_steps</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>warmup_style</span> <span class=o>==</span> <span class=s2>&#34;constant&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_lr_scheduler</span> <span class=o>=</span> <span class=n>get_constant_schedule_with_warmup</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=n>actor_optimizer</span><span class=p>,</span> <span class=n>num_warmup_steps</span><span class=o>=</span><span class=n>num_warmup_steps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>elif</span> <span class=n>warmup_style</span> <span class=o>==</span> <span class=s2>&#34;cosine&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_lr_scheduler</span> <span class=o>=</span> <span class=n>get_cosine_schedule_with_warmup</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=n>actor_optimizer</span><span class=p>,</span> <span class=n>num_warmup_steps</span><span class=o>=</span><span class=n>num_warmup_steps</span><span class=p>,</span> <span class=n>num_training_steps</span><span class=o>=</span><span class=n>total_steps</span><span class=p>,</span> <span class=n>min_lr_ratio</span><span class=o>=</span><span class=n>min_lr_ratio</span><span class=p>,</span> <span class=n>num_cycles</span><span class=o>=</span><span class=n>num_cycles</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>raise</span> <span class=ne>NotImplementedError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Warmup style </span><span class=si>{</span><span class=n>warmup_style</span><span class=si>}</span><span class=s2> is not supported&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;After </span><span class=si>{</span><span class=n>role</span><span class=si>}</span><span class=s2> optimizer init&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_optimizer</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_lr_scheduler</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>actor_module_fsdp</span><span class=p>,</span> <span class=n>actor_optimizer</span><span class=p>,</span> <span class=n>actor_lr_scheduler</span><span class=p>,</span> <span class=n>actor_model_config</span>
</span></span></code></pre></td></tr></table></div></div></details><p>这里代码很直白。有一个点值得单独拎出来讲一下：仔细观察 <code>actor_module</code> 的 dtype，直觉告诉我，<code>actor_module</code> 的 dtype 应该是 bf16 的，而 gradient 和 optimizer 的 dtype 是 fp32 的。可是 <code>actor_module</code> 的 default dtype 被设为了 fp32，然后从 fp32 load 了模型。实际上这是因为 pytorch 的各种 optimizer 都是直接和 parameter 绑定的，用 bf16 的 parameter 初始化的 optimizer 也是 bf16。所以 model 先 load 了 fp32，然后初始化 optimizer 作为混合精度，最后把 model 转成 bf16。</p><h3 id=actorrolloutrefworker_build_rollout><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/fsdp_workers.py#L394><code>ActorRolloutRefWorker._build_rollout()</code></a><a hidden class=anchor aria-hidden=true href=#actorrolloutrefworker_build_rollout>#</a></h3><p>这是对我而言最清晰的地方，实际上也是最熟悉的。在这里终于引入了 SGLang：</p><ol><li><strong>设备网格创建</strong>：为 Rollout 创建推理张量并行（<code>infer_tp</code>）设备网格。</li><li><strong>SGLang Rollout 构建</strong>：导入并实例化 <code>SGLangRollout</code> 和 <code>FSDPSGLangShardingManager</code>。<code>FSDPSGLangShardingManager</code> 负责在 FSDP 训练格式和 SGLang 推理格式之间转换模型权重。</li></ol><details><summary>ActorRolloutRefWorker._build_rollout 部分源码</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_build_rollout</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>torch.distributed.device_mesh</span> <span class=kn>import</span> <span class=n>init_device_mesh</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>infer_tp</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>tensor_model_parallel_size</span>
</span></span><span class=line><span class=cl>    <span class=n>dp</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>world_size</span> <span class=o>//</span> <span class=n>infer_tp</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>world_size</span> <span class=o>%</span> <span class=n>infer_tp</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;rollout world_size: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>world_size</span><span class=si>}</span><span class=s2> is not divisible by infer_tp: </span><span class=si>{</span><span class=n>infer_tp</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>rollout_device_mesh</span> <span class=o>=</span> <span class=n>init_device_mesh</span><span class=p>(</span><span class=n>device_name</span><span class=p>,</span> <span class=n>mesh_shape</span><span class=o>=</span><span class=p>(</span><span class=n>dp</span><span class=p>,</span> <span class=n>infer_tp</span><span class=p>),</span> <span class=n>mesh_dim_names</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;dp&#34;</span><span class=p>,</span> <span class=s2>&#34;infer_tp&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>rollout_name</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>name</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>rollout_name</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;sglang&#34;</span><span class=p>,</span> <span class=s2>&#34;sglang_async&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>rollout_name</span> <span class=o>==</span> <span class=s2>&#34;sglang_async&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>warnings</span><span class=o>.</span><span class=n>warn</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;&#39;sglang_async&#39; has been deprecated and merged into &#39;sglang&#39;. Please use &#39;sglang&#39; going forward.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=ne>DeprecationWarning</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>stacklevel</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.workers.rollout.sglang_rollout</span> <span class=kn>import</span> <span class=n>SGLangRollout</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.workers.sharding_manager.fsdp_sglang</span> <span class=kn>import</span> <span class=n>FSDPSGLangShardingManager</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>local_path</span> <span class=o>=</span> <span class=n>copy_to_local</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Before building </span><span class=si>{</span><span class=n>rollout_name</span><span class=si>}</span><span class=s2> rollout&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>rollout</span> <span class=o>=</span> <span class=n>SGLangRollout</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_module</span><span class=o>=</span><span class=n>local_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>tokenizer</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>model_hf_config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>actor_model_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;After building </span><span class=si>{</span><span class=n>rollout_name</span><span class=si>}</span><span class=s2> rollout&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>get_world_size</span><span class=p>()</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>load_format</span> <span class=o>=</span> <span class=s2>&#34;dummy_hf&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>rollout_sharding_manager</span> <span class=o>=</span> <span class=n>FSDPSGLangShardingManager</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>module</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>actor_module_fsdp</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>inference_engine</span><span class=o>=</span><span class=n>rollout</span><span class=o>.</span><span class=n>_engine</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>model_config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>actor_model_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>full_params</span><span class=o>=</span><span class=s2>&#34;hf&#34;</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>load_format</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>device_mesh</span><span class=o>=</span><span class=n>rollout_device_mesh</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>offload_param</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_param</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=s2>&#34;After building sharding manager&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=ne>NotImplementedError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Rollout name: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>name</span><span class=si>}</span><span class=s2> is not supported&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>rollout</span><span class=p>,</span> <span class=n>rollout_sharding_manager</span>
</span></span></code></pre></td></tr></table></div></div></details><h3 id=sglangrollout__init__><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L208><code>SGLangRollout.__init__()</code></a><a hidden class=anchor aria-hidden=true href=#sglangrollout__init__>#</a></h3><p>事已至此，再往下看一层 SGLang 具体的初始化：</p><ol><li>调用父类构造函数并设置配置和设备网格。</li><li>通过 <code>_initialize_tools()</code> 初始化工具 schemas、map 和解析器，支持 Multi-turn 对话中的工具使用。</li><li>初始化 SGLang 推理所需的分布式环境。</li><li>通过 <code>_verify_config()</code> 验证模型配置。</li><li>通过 <code>_init_inference_engine()</code> 初始化 SGLang 推理引擎。</li><li>通过 <code>_init_sampling_params()</code> 初始化生成序列的采样参数。</li><li>设置 Tokenizer 和 padding token ID。</li></ol><details><summary>SGLangRollout.__init__ 部分源码</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>SGLangRollout</span><span class=p>(</span><span class=n>BaseRollout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>actor_module</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=p>:</span> <span class=n>DictConfig</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>model_hf_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>port</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>trust_remote_code</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>device_mesh</span><span class=p>:</span> <span class=n>DeviceMesh</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=o>**</span><span class=n>kwargs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Synchronized SGLang rollout engine.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            actor_module: Huggingface model name or path to the model. The
</span></span></span><span class=line><span class=cl><span class=s2>                model should be supported by SGLang.
</span></span></span><span class=line><span class=cl><span class=s2>            config: A DictConfig object containing SGLang-specific operational
</span></span></span><span class=line><span class=cl><span class=s2>                parameters and rollout settings.
</span></span></span><span class=line><span class=cl><span class=s2>                Refer to https://docs.sglang.ai/backend/server_arguments.html
</span></span></span><span class=line><span class=cl><span class=s2>            tokenizer: The tokenizer instance compatible with the actor_module.
</span></span></span><span class=line><span class=cl><span class=s2>            model_hf_config: The Hugging Face model&#39;s configuration (e.g.,
</span></span></span><span class=line><span class=cl><span class=s2>                `transformers.PretrainedConfig`). It provides architectural
</span></span></span><span class=line><span class=cl><span class=s2>                details and hyperparameters like `max_position_embeddings`,
</span></span></span><span class=line><span class=cl><span class=s2>                used by SGLang for correct model initialization. This is
</span></span></span><span class=line><span class=cl><span class=s2>                the model&#39;s inherent design, not SGLang&#39;s runtime behavior.
</span></span></span><span class=line><span class=cl><span class=s2>            port: Optional port for multi-node initialization when nnodes &gt; 1.
</span></span></span><span class=line><span class=cl><span class=s2>            trust_remote_code: Whether or not to allow for custom models
</span></span></span><span class=line><span class=cl><span class=s2>                defined on the Hub in their own modeling files.
</span></span></span><span class=line><span class=cl><span class=s2>            device_mesh: Optional `DeviceMesh` object for distributed setup.
</span></span></span><span class=line><span class=cl><span class=s2>            **kwargs: Additional keyword arguments, primarily `train_tp` for
</span></span></span><span class=line><span class=cl><span class=s2>                Megatron Backend integration to initialize hybrid engine
</span></span></span><span class=line><span class=cl><span class=s2>                process groups.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>config</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_device_mesh_cpu</span> <span class=o>=</span> <span class=n>device_mesh</span>
</span></span><span class=line><span class=cl>        <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>setdefault</span><span class=p>(</span><span class=s2>&#34;SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_tool_schemas</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_tool_map</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_tool_call_parser_type</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_sgl_tools</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_function_call_parser</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_initialize_tools</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>interaction</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>BaseInteraction</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_intitalize_interaction</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># If turn on `free_cache_engine`, SGLang engine&#39;s KV cache</span>
</span></span><span class=line><span class=cl>        <span class=c1># will be freed after each `generate_sequences` call.</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=ow>not</span> <span class=p>(</span><span class=ow>not</span> <span class=n>config</span><span class=o>.</span><span class=n>enforce_eager</span> <span class=ow>and</span> <span class=n>config</span><span class=o>.</span><span class=n>free_cache_engine</span><span class=p>),</span> <span class=s2>&#34;disable CUDA graph (enforce_eager = False) if free cache engine&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;tool_schemas: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_tool_schemas</span><span class=si>}</span><span class=s2>, tool_map: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_tool_map</span><span class=si>}</span><span class=s2>, tool_call_parser_type: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_tool_call_parser_type</span><span class=si>}</span><span class=s2>, sgl_tools: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_sgl_tools</span><span class=si>}</span><span class=s2>, function_call_parser: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_function_call_parser</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_init_distributed_env</span><span class=p>(</span><span class=n>device_mesh_cpu</span><span class=o>=</span><span class=n>device_mesh</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_verify_config</span><span class=p>(</span><span class=n>model_hf_config</span><span class=o>=</span><span class=n>model_hf_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># initialize the inference engine</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_init_inference_engine</span><span class=p>(</span><span class=n>trust_remote_code</span><span class=p>,</span> <span class=n>actor_module</span><span class=p>,</span> <span class=n>port</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_init_sampling_params</span><span class=p>(</span><span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>tokenizer</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pad_token_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token_id</span>
</span></span></code></pre></td></tr></table></div></div></details><h3 id=sglangrolloutasyncengine><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L124><code>SGLangRollout.AsyncEngine</code></a><a hidden class=anchor aria-hidden=true href=#sglangrolloutasyncengine>#</a></h3><p>关于 <code>SGLangRollout</code> 调用 tool 的部分，我们在下文的训练循环中再展开，这里先讨论完 SGLang 的初始化。为了调用 SGLang engine 的接口，verl 进行了一层封装，实现了我们对 SGLang 除开 rollout 之外的所有接口：</p><ol><li>release and resume memory occupation：在训练时释放掉显存占用并在训练后恢复。</li><li>update weights from tensor：训练结束后更新模型权重。</li><li>flush cache：模型参数更新后刷新 KV cache，因为之前的 KV cache 已经失效了。</li></ol><p>这里涉及到了非常深入的内存管理问题，读者对 SGLang engine 在 verl 里的显存管理感兴趣，欢迎阅读标哥的博客 <a href=https://hebiao064.github.io/rl-memory-management>optimizing Memory Usage in verl</a>，写的非常深入浅出。</p><details><summary>SGLangRollout 何时需要 flush cache</summary><p>这一部分内容需要单独拎出来讲讲。SGLang engine 的 release 和 resume 需要保留 CUDA Graph，否则 rollout 效率会大幅降低。因此，我们基于 tom 的 <a href=https://github.com/fzyzcjy/torch_memory_saver>torch_memory_saver</a> 实现了独立的显存管理。简单来说，我们有：</p><ol><li><code>pause</code>；保留 mem savor 作用域内指定 tensor 的 virtual address，但是将其 physical memory 释放回显存管理器。</li><li><code>resume</code>；将先前 <code>pause</code> 的 tensor 重新申请一块 physical memory，并将其 virtual address 映射到新的 physical memory。</li></ol><p>注意，整个 pause 和 resume 的过程中，tensor 的 virtual address 不会发生变化，只是这块 virtual address 映射到的 physical memory 改变了。因此，CUDA Graph 并没有失效，不变的 virtual address 让计算流仍旧可以正常执行。</p><p>verl 内的 <code>release_memory_occupation</code> 和 <code>resume_memory_occupation</code> 就是基于 <code>pause</code> 和 <code>resume</code> 实现的。听上去是个完美的故事，我们甚至实现了 <a href=https://github.com/fzyzcjy/torch_memory_saver/pull/20>mutli-stage 的显存管理</a>，能够独立 release 和 resume kv cache 和 model weights。</p><p>不过，对于 kv cache 而言，在 kv cache 被 release 掉之后，实际上 kv cache 的 tensor 仍旧保留，只是其 virtual address 映射到的 physical memory 被释放了。与此同时，radix tree 仍旧索引着整个 kv cache。当 kv cache 被 resume 之后，一方面之前物理内存上之前的 kv cache 已经不复存在了，另一方面模型的参数也被更新。出于这两点，我们一定要使用 flush cache 接口来刷新 kv cache 的索引（radix tree）。</p><p>这里又有个非常有趣的设计。乍一想 kv cache 的管理这么麻烦，还要 flush，为什么不直接 delete kv cache 以及 delete model weights 再重新初始化呢？显然，这样没法利用已有的 cuda graph，非常消耗时间。保留 virtual address 不变但是更换 physical memory 的方案，让 verl 能够持续利用已建好的 cuda graph。</p><p>最后一个问题，一共要几次 flush cache 呢？我个人理解，在一整个 training engine 被 pause，resume 然后 update weights 的过程中，必须要有一次 flush cache 来刷新 kv cache 的索引，只是 verl 当中为了保险，刷新了很多次罢了。</p></details><details><summary>SGLangRollout.AsyncEngine 源码</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>AsyncEngine</span><span class=p>(</span><span class=n>sglang</span><span class=o>.</span><span class=n>srt</span><span class=o>.</span><span class=n>entrypoints</span><span class=o>.</span><span class=n>engine</span><span class=o>.</span><span class=n>Engine</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># default to use dummy load format, which need to reload weights in first time</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_need_reload</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>async</span> <span class=k>def</span> <span class=nf>release_memory_occupation</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Release GPU occupation temporarily.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>obj</span> <span class=o>=</span> <span class=n>ReleaseMemoryOccupationReqInput</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=k>await</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer_manager</span><span class=o>.</span><span class=n>release_memory_occupation</span><span class=p>(</span><span class=n>obj</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>async</span> <span class=k>def</span> <span class=nf>resume_memory_occupation</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=k>await</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer_manager</span><span class=o>.</span><span class=n>resume_memory_occupation</span><span class=p>(</span><span class=n>obj</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>async</span> <span class=k>def</span> <span class=nf>update_weights_from_tensor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>named_tensors</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]],</span>  <span class=c1># noqa: UP006</span>
</span></span><span class=line><span class=cl>        <span class=n>load_format</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>flush_cache</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Update weights from distributed source. If there are going to be more updates, set `flush_cache` to be false
</span></span></span><span class=line><span class=cl><span class=s2>        to avoid duplicated cache cleaning operation.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>obj</span> <span class=o>=</span> <span class=n>UpdateWeightsFromTensorReqInput</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>serialized_named_tensors</span><span class=o>=</span><span class=p>[</span><span class=n>MultiprocessingSerializer</span><span class=o>.</span><span class=n>serialize</span><span class=p>(</span><span class=n>named_tensors</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>server_args</span><span class=o>.</span><span class=n>tp_size</span><span class=p>)],</span>
</span></span><span class=line><span class=cl>            <span class=n>load_format</span><span class=o>=</span><span class=n>load_format</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>flush_cache</span><span class=o>=</span><span class=n>flush_cache</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=k>await</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer_manager</span><span class=o>.</span><span class=n>update_weights_from_tensor</span><span class=p>(</span><span class=n>obj</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>async</span> <span class=k>def</span> <span class=nf>flush_cache</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=k>await</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer_manager</span><span class=o>.</span><span class=n>flush_cache</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div></details><h3 id=sglangrollout_init_inference_engine><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L325><code>SGLangRollout._init_inference_engine()</code></a><a hidden class=anchor aria-hidden=true href=#sglangrollout_init_inference_engine>#</a></h3><p><code>SGLangRollout._init_inference_engine()</code> 初始化了封装的 <code>AsyncEngine</code>。</p><details><summary>SGLangRollout._init_inference_engine 源码</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_init_inference_engine</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=p>,</span> <span class=n>actor_module</span><span class=p>,</span> <span class=n>port</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># initialize the inference engine</span>
</span></span><span class=line><span class=cl>    <span class=n>nnodes</span> <span class=o>=</span> <span class=o>-</span><span class=p>(</span><span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>_tp_size</span> <span class=o>//</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>visible_devices_set</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>nnodes</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>ip</span> <span class=o>=</span> <span class=n>get_ip</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>port</span> <span class=o>=</span> <span class=n>get_open_port</span><span class=p>()</span> <span class=k>if</span> <span class=n>port</span> <span class=ow>is</span> <span class=kc>None</span> <span class=k>else</span> <span class=n>port</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=n>ip</span><span class=p>,</span> <span class=n>port</span><span class=p>]</span> <span class=o>=</span> <span class=n>broadcast_pyobj</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=n>ip</span><span class=p>,</span> <span class=n>port</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=n>rank</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_rank</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>dist_group</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_device_mesh_cpu</span><span class=o>.</span><span class=n>get_group</span><span class=p>(</span><span class=s2>&#34;tp&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>src</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_device_mesh_cpu</span><span class=p>[</span><span class=s2>&#34;tp&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>mesh</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>force_cpu_device</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>dist_init_addr</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;[</span><span class=si>{</span><span class=n>ip</span><span class=si>}</span><span class=s2>]:</span><span class=si>{</span><span class=n>port</span><span class=si>}</span><span class=s2>&#34;</span> <span class=k>if</span> <span class=n>is_ipv6</span><span class=p>(</span><span class=n>ip</span><span class=p>)</span> <span class=k>else</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>ip</span><span class=si>}</span><span class=s2>:</span><span class=si>{</span><span class=n>port</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dist_init_addr</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>load_format</span> <span class=o>=</span> <span class=s2>&#34;dummy&#34;</span> <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>load_format</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s2>&#34;dummy&#34;</span><span class=p>)</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>load_format</span>
</span></span><span class=line><span class=cl>    <span class=n>tp_size_per_node</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_tp_size</span> <span class=o>//</span> <span class=n>nnodes</span>
</span></span><span class=line><span class=cl>    <span class=n>node_rank</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_tp_rank</span> <span class=o>//</span> <span class=n>tp_size_per_node</span>
</span></span><span class=line><span class=cl>    <span class=n>first_rank_in_node</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_tp_rank</span> <span class=o>%</span> <span class=n>tp_size_per_node</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>first_rank_in_node</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>rank</span> <span class=o>=</span> <span class=n>dist</span><span class=o>.</span><span class=n>get_rank</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;SGLANG_BLOCK_NONZERO_RANK_CHILDREN&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&#34;0&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_engine</span> <span class=o>=</span> <span class=n>AsyncEngine</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>model_path</span><span class=o>=</span><span class=n>actor_module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>dtype</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>dtype</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>mem_fraction_static</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>gpu_memory_utilization</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>enable_memory_saver</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>base_gpu_id</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>gpu_id_step</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>tp_size</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_tp_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>node_rank</span><span class=o>=</span><span class=n>node_rank</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>load_format</span><span class=o>=</span><span class=n>load_format</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>dist_init_addr</span><span class=o>=</span><span class=n>dist_init_addr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>nnodes</span><span class=o>=</span><span class=n>nnodes</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=c1># NOTE(linjunrong): add rank to prevent SGLang generate same port inside PortArgs.init_new</span>
</span></span><span class=line><span class=cl>            <span class=c1># when random.seed is being set during training</span>
</span></span><span class=line><span class=cl>            <span class=n>port</span><span class=o>=</span><span class=mi>30000</span> <span class=o>+</span> <span class=n>rank</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=c1># NOTE(Chenyang): if you want to debug the SGLang engine output</span>
</span></span><span class=line><span class=cl>            <span class=c1># please set the following parameters</span>
</span></span><span class=line><span class=cl>            <span class=c1># Otherwise, it will make the engine run too slow</span>
</span></span><span class=line><span class=cl>            <span class=c1># log_level=&#34;INFO&#34;,</span>
</span></span><span class=line><span class=cl>            <span class=c1># log_requests=True,</span>
</span></span><span class=line><span class=cl>            <span class=c1># log_requests_level=2,</span>
</span></span><span class=line><span class=cl>            <span class=c1># max_running_requests=1,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_engine</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>sharding_manager</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>is_sleep</span> <span class=o>=</span> <span class=kc>True</span>
</span></span></code></pre></td></tr></table></div></div></details><p>这里最值得一提的是，SGLang engine 并没有严格实现 verl 所希望的 SPMD 模式（每个 GPU 上的进程完全一样），而是采用了 mock 的 SPMD。举例来说，假设 tp size = 4，按照 verl 的设计，应该要 4 张 GPU 上每个都运行一个相同的 SGLang engine。实际上的实现是在 GPU 0 上启动一个进程占据全部 GPU，而 GPU 1 2 3 上仅仅保留一个空进程 <code>None</code>。虽然 verl team 起初设定中认为严格的 SPMD 意义巨大，但实际使用中，我们认为 mock 的 SPMD 已经足够满足性能需求。</p><p>【TODO】 这么描述可能不严谨。</p><h3 id=taskrunnerrun><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L64><code>TaskRunner.run()</code></a><a hidden class=anchor aria-hidden=true href=#taskrunnerrun>#</a></h3><p>往下走了这么多层，我们终于能够继续回到 <code>TaskRunner</code> 类。😭</p><p>【TODO】上文其实主要是 Actor Rollout，还没有具体说 Actor 的 training forward and backward。以及 Reference，reward 和 critic 的 training forward and backward。</p><ol><li>加载、解析和验证训练任务的配置（使用 <code>OmegaConf</code>），确保所有参数的正确性和一致性。</li><li>将模型文件从远程路径复制到本地，确保所有 Worker 都可以访问。</li><li>组件初始化：<ul><li>初始化 Tokenizer 和 Processor，用于文本和多模态数据的处理。</li><li>根据配置中指定的 Actor 策略（如 <code>fsdp</code> 或 <code>megatron</code>），动态选择相应的 Worker 类（例如 <code>ActorRolloutRefWorker</code> 和 <code>CriticWorker</code>），并确定使用的 <code>RayWorkerGroup</code> 类型。</li><li>定义 Ray 资源池的规格和角色到资源池的映射，用于 GPU 资源的分配和管理。</li><li>加载用于训练和验证的奖励模型。</li><li>创建训练和验证数据集，以及训练数据采样器。</li></ul></li><li>创建 <code>RayPPOTrainer</code> 实例，它是管理所有计算资源和训练流程的中央协调器。</li><li>调用 <code>RayPPOTrainer</code> 的 <code>init_workers()</code> 方法，将配置的 Worker 类实例化到 Ray 集群的 GPU 上，为实际计算做准备。</li><li>调用 <code>RayPPOTrainer</code> 的 <code>fit()</code> 方法，启动 PPO 训练循环。</li></ol><details><summary>TaskRunner.run 源码</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@ray.remote</span><span class=p>(</span><span class=n>num_cpus</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TaskRunner</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>run</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>pprint</span> <span class=kn>import</span> <span class=n>pprint</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>omegaconf</span> <span class=kn>import</span> <span class=n>OmegaConf</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.utils.fs</span> <span class=kn>import</span> <span class=n>copy_to_local</span>
</span></span><span class=line><span class=cl>        <span class=kn>import</span> <span class=nn>socket</span>
</span></span><span class=line><span class=cl>        <span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;TaskRunner hostname: </span><span class=si>{</span><span class=n>socket</span><span class=o>.</span><span class=n>gethostname</span><span class=p>()</span><span class=si>}</span><span class=s2>, PID: </span><span class=si>{</span><span class=n>os</span><span class=o>.</span><span class=n>getpid</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pprint</span><span class=p>(</span><span class=n>OmegaConf</span><span class=o>.</span><span class=n>to_container</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=n>resolve</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>OmegaConf</span><span class=o>.</span><span class=n>resolve</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 模型下载</span>
</span></span><span class=line><span class=cl>        <span class=n>local_path</span> <span class=o>=</span> <span class=n>copy_to_local</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>path</span><span class=p>,</span> <span class=n>use_shm</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;use_shm&#34;</span><span class=p>,</span> <span class=kc>False</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Tokenizer 和 Processor 初始化</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.utils</span> <span class=kn>import</span> <span class=n>hf_processor</span><span class=p>,</span> <span class=n>hf_tokenizer</span>
</span></span><span class=line><span class=cl>        <span class=n>trust_remote_code</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;trust_remote_code&#34;</span><span class=p>,</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>hf_tokenizer</span><span class=p>(</span><span class=n>local_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>processor</span> <span class=o>=</span> <span class=n>hf_processor</span><span class=p>(</span><span class=n>local_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>,</span> <span class=n>use_fast</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Worker 类型选择</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;fsdp&#34;</span><span class=p>,</span> <span class=s2>&#34;fsdp2&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=kn>from</span> <span class=nn>verl.single_controller.ray</span> <span class=kn>import</span> <span class=n>RayWorkerGroup</span>
</span></span><span class=line><span class=cl>            <span class=kn>from</span> <span class=nn>verl.workers.fsdp_workers</span> <span class=kn>import</span> <span class=n>ActorRolloutRefWorker</span><span class=p>,</span> <span class=n>AsyncActorRolloutRefWorker</span><span class=p>,</span> <span class=n>CriticWorker</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_rollout_cls</span> <span class=o>=</span> <span class=n>AsyncActorRolloutRefWorker</span> <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>mode</span> <span class=o>==</span> <span class=s2>&#34;async&#34;</span> <span class=k>else</span> <span class=n>ActorRolloutRefWorker</span>
</span></span><span class=line><span class=cl>            <span class=n>ray_worker_group_cls</span> <span class=o>=</span> <span class=n>RayWorkerGroup</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span> <span class=o>==</span> <span class=s2>&#34;megatron&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>assert</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span> <span class=o>==</span> <span class=n>config</span><span class=o>.</span><span class=n>critic</span><span class=o>.</span><span class=n>strategy</span>
</span></span><span class=line><span class=cl>            <span class=kn>from</span> <span class=nn>verl.single_controller.ray.megatron</span> <span class=kn>import</span> <span class=n>NVMegatronRayWorkerGroup</span>
</span></span><span class=line><span class=cl>            <span class=kn>from</span> <span class=nn>verl.workers.megatron_workers</span> <span class=kn>import</span> <span class=n>ActorRolloutRefWorker</span><span class=p>,</span> <span class=n>AsyncActorRolloutRefWorker</span><span class=p>,</span> <span class=n>CriticWorker</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_rollout_cls</span> <span class=o>=</span> <span class=n>AsyncActorRolloutRefWorker</span> <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>mode</span> <span class=o>==</span> <span class=s2>&#34;async&#34;</span> <span class=k>else</span> <span class=n>ActorRolloutRefWorker</span>
</span></span><span class=line><span class=cl>            <span class=n>ray_worker_group_cls</span> <span class=o>=</span> <span class=n>NVMegatronRayWorkerGroup</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>NotImplementedError</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.trainer.ppo.ray_trainer</span> <span class=kn>import</span> <span class=n>ResourcePoolManager</span><span class=p>,</span> <span class=n>Role</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 角色到 Worker 类的映射</span>
</span></span><span class=line><span class=cl>        <span class=n>role_worker_mapping</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>:</span> <span class=n>ray</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>actor_rollout_cls</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>Role</span><span class=o>.</span><span class=n>Critic</span><span class=p>:</span> <span class=n>ray</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>CriticWorker</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 资源池规格和角色映射</span>
</span></span><span class=line><span class=cl>        <span class=n>global_pool_id</span> <span class=o>=</span> <span class=s2>&#34;global_pool&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>resource_pool_spec</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>global_pool_id</span><span class=p>:</span> <span class=p>[</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=o>.</span><span class=n>n_gpus_per_node</span><span class=p>]</span> <span class=o>*</span> <span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=o>.</span><span class=n>nnodes</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>mapping</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>:</span> <span class=n>global_pool_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>Role</span><span class=o>.</span><span class=n>Critic</span><span class=p>:</span> <span class=n>global_pool_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Reward Model Worker 的初始化</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>reward_model</span><span class=o>.</span><span class=n>enable</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>reward_model</span><span class=o>.</span><span class=n>strategy</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;fsdp&#34;</span><span class=p>,</span> <span class=s2>&#34;fsdp2&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>                <span class=kn>from</span> <span class=nn>verl.workers.fsdp_workers</span> <span class=kn>import</span> <span class=n>RewardModelWorker</span>
</span></span><span class=line><span class=cl>            <span class=k>elif</span> <span class=n>config</span><span class=o>.</span><span class=n>reward_model</span><span class=o>.</span><span class=n>strategy</span> <span class=o>==</span> <span class=s2>&#34;megatron&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=kn>from</span> <span class=nn>verl.workers.megatron_workers</span> <span class=kn>import</span> <span class=n>RewardModelWorker</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>raise</span> <span class=ne>NotImplementedError</span>
</span></span><span class=line><span class=cl>            <span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>RewardModel</span><span class=p>]</span> <span class=o>=</span> <span class=n>ray</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>RewardModelWorker</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>RewardModel</span><span class=p>]</span> <span class=o>=</span> <span class=n>global_pool_id</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Reference Policy Worker 的初始化</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>algorithm</span><span class=o>.</span><span class=n>use_kl_in_reward</span> <span class=ow>or</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>use_kl_loss</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>RefPolicy</span><span class=p>]</span> <span class=o>=</span> <span class=n>ray</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>ActorRolloutRefWorker</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>RefPolicy</span><span class=p>]</span> <span class=o>=</span> <span class=n>global_pool_id</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 加载奖励管理器</span>
</span></span><span class=line><span class=cl>        <span class=n>reward_fn</span> <span class=o>=</span> <span class=n>load_reward_manager</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>num_examine</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=o>**</span><span class=n>config</span><span class=o>.</span><span class=n>reward_model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;reward_kwargs&#34;</span><span class=p>,</span> <span class=p>{}))</span>
</span></span><span class=line><span class=cl>        <span class=n>val_reward_fn</span> <span class=o>=</span> <span class=n>load_reward_manager</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>num_examine</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=o>**</span><span class=n>config</span><span class=o>.</span><span class=n>reward_model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;reward_kwargs&#34;</span><span class=p>,</span> <span class=p>{}))</span>
</span></span><span class=line><span class=cl>        <span class=n>resource_pool_manager</span> <span class=o>=</span> <span class=n>ResourcePoolManager</span><span class=p>(</span><span class=n>resource_pool_spec</span><span class=o>=</span><span class=n>resource_pool_spec</span><span class=p>,</span> <span class=n>mapping</span><span class=o>=</span><span class=n>mapping</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.utils.dataset.rl_dataset</span> <span class=kn>import</span> <span class=n>collate_fn</span><span class=p>,</span> <span class=n>create_rl_dataset</span><span class=p>,</span> <span class=n>create_rl_sampler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 创建训练和验证数据集</span>
</span></span><span class=line><span class=cl>        <span class=n>train_dataset</span> <span class=o>=</span> <span class=n>create_rl_dataset</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>train_files</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>processor</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>val_dataset</span> <span class=o>=</span> <span class=n>create_rl_dataset</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>val_files</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>processor</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>train_sampler</span> <span class=o>=</span> <span class=n>create_rl_sampler</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>train_dataset</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 初始化 PPO 训练器</span>
</span></span><span class=line><span class=cl>        <span class=n>trainer</span> <span class=o>=</span> <span class=n>RayPPOTrainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>config</span><span class=o>=</span><span class=n>config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>processor</span><span class=o>=</span><span class=n>processor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>role_worker_mapping</span><span class=o>=</span><span class=n>role_worker_mapping</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>resource_pool_manager</span><span class=o>=</span><span class=n>resource_pool_manager</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>ray_worker_group_cls</span><span class=o>=</span><span class=n>ray_worker_group_cls</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>reward_fn</span><span class=o>=</span><span class=n>reward_fn</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>val_reward_fn</span><span class=o>=</span><span class=n>val_reward_fn</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>train_dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>val_dataset</span><span class=o>=</span><span class=n>val_dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>collate_fn</span><span class=o>=</span><span class=n>collate_fn</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>train_sampler</span><span class=o>=</span><span class=n>train_sampler</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>device_name</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=o>.</span><span class=n>device</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 初始化训练器的 Workers</span>
</span></span><span class=line><span class=cl>        <span class=n>trainer</span><span class=o>.</span><span class=n>init_workers</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># 启动训练过程</span>
</span></span><span class=line><span class=cl>        <span class=n>trainer</span><span class=o>.</span><span class=n>fit</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div></details><h3 id=rayppotrainer__init__><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L277><code>RayPPOTrainer.__init__()</code></a><a hidden class=anchor aria-hidden=true href=#rayppotrainer__init__>#</a></h3><ol><li>保存传入的配置对象、tokenizer、processor、角色到 Worker 的映射、资源池管理器以及 WorkerGroup 类。</li><li>根据配置启用或禁用 Critic、Reference Policy、Reward Model 和 Hybrid Engine 等功能组件。</li><li>调用 <code>_validate_config()</code> 方法验证配置的合理性。</li><li>存储训练和验证数据集、collate 函数和训练数据采样器。</li></ol><details><summary>RayPPOTrainer 源码</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>RayPPOTrainer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># TODO: support each role have individual ray_worker_group_cls,</span>
</span></span><span class=line><span class=cl>    <span class=c1># i.e., support different backend of different role</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>role_worker_mapping</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=n>Role</span><span class=p>,</span> <span class=n>WorkerType</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>resource_pool_manager</span><span class=p>:</span> <span class=n>ResourcePoolManager</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>ray_worker_group_cls</span><span class=p>:</span> <span class=n>RayWorkerGroup</span> <span class=o>=</span> <span class=n>RayWorkerGroup</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>processor</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>reward_fn</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>val_reward_fn</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>train_dataset</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Dataset</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>val_dataset</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Dataset</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>collate_fn</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>train_sampler</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Sampler</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>device_name</span><span class=o>=</span><span class=s2>&#34;cuda&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Initialize distributed PPO trainer with Ray backend.
</span></span></span><span class=line><span class=cl><span class=s2>        Note that this trainer runs on the driver process on a single CPU/GPU node.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            config: Configuration object containing training parameters.
</span></span></span><span class=line><span class=cl><span class=s2>            tokenizer: Tokenizer used for encoding and decoding text.
</span></span></span><span class=line><span class=cl><span class=s2>            role_worker_mapping (dict[Role, WorkerType]): Mapping from roles to worker classes.
</span></span></span><span class=line><span class=cl><span class=s2>            resource_pool_manager (ResourcePoolManager): Manager for Ray resource pools.
</span></span></span><span class=line><span class=cl><span class=s2>            ray_worker_group_cls (RayWorkerGroup, optional): Class for Ray worker groups. Defaults to RayWorkerGroup.
</span></span></span><span class=line><span class=cl><span class=s2>            processor: Optional data processor, used for multimodal data.
</span></span></span><span class=line><span class=cl><span class=s2>            reward_fn: Function for computing rewards during training.
</span></span></span><span class=line><span class=cl><span class=s2>            val_reward_fn: Function for computing rewards during validation.
</span></span></span><span class=line><span class=cl><span class=s2>            train_dataset (Optional[Dataset], optional): Training dataset. Defaults to None.
</span></span></span><span class=line><span class=cl><span class=s2>            val_dataset (Optional[Dataset], optional): Validation dataset. Defaults to None.
</span></span></span><span class=line><span class=cl><span class=s2>            collate_fn: Function to collate data samples into batches.
</span></span></span><span class=line><span class=cl><span class=s2>            train_sampler (Optional[Sampler], optional): Sampler for the training dataset. Defaults to None.
</span></span></span><span class=line><span class=cl><span class=s2>            device_name (str, optional): Device name for training (e.g., &#34;cuda&#34;, &#34;cpu&#34;). Defaults to &#34;cuda&#34;.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Store the tokenizer for text processing</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>tokenizer</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>processor</span> <span class=o>=</span> <span class=n>processor</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>config</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>reward_fn</span> <span class=o>=</span> <span class=n>reward_fn</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>val_reward_fn</span> <span class=o>=</span> <span class=n>val_reward_fn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hybrid_engine</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>hybrid_engine</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>hybrid_engine</span><span class=p>,</span> <span class=s2>&#34;Currently, only support hybrid engine&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>hybrid_engine</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>assert</span> <span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span> <span class=ow>in</span> <span class=n>role_worker_mapping</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>role_worker_mapping</span><span class=o>.</span><span class=n>keys</span><span class=p>()</span><span class=si>=}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>role_worker_mapping</span> <span class=o>=</span> <span class=n>role_worker_mapping</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span> <span class=o>=</span> <span class=n>resource_pool_manager</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>use_reference_policy</span> <span class=o>=</span> <span class=n>Role</span><span class=o>.</span><span class=n>RefPolicy</span> <span class=ow>in</span> <span class=n>role_worker_mapping</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>use_rm</span> <span class=o>=</span> <span class=n>Role</span><span class=o>.</span><span class=n>RewardModel</span> <span class=ow>in</span> <span class=n>role_worker_mapping</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ray_worker_group_cls</span> <span class=o>=</span> <span class=n>ray_worker_group_cls</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>device_name</span> <span class=o>=</span> <span class=n>device_name</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>validation_generations_logger</span> <span class=o>=</span> <span class=n>ValidationGenerationsLogger</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># if ref_in_actor is True, the reference policy will be actor without lora applied</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ref_in_actor</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;lora_rank&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># define in-reward KL control</span>
</span></span><span class=line><span class=cl>        <span class=c1># kl loss control currently not suppoorted</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>algorithm</span><span class=o>.</span><span class=n>use_kl_in_reward</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>kl_ctrl_in_reward</span> <span class=o>=</span> <span class=n>core_algos</span><span class=o>.</span><span class=n>get_kl_controller</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>algorithm</span><span class=o>.</span><span class=n>kl_ctrl</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>algorithm</span><span class=o>.</span><span class=n>adv_estimator</span> <span class=o>==</span> <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>GAE</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>use_critic</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>algorithm</span><span class=o>.</span><span class=n>adv_estimator</span> <span class=ow>in</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>GRPO</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>GRPO_PASSK</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>REINFORCE_PLUS_PLUS</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>REMAX</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>RLOO</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>OPO</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>REINFORCE_PLUS_PLUS_BASELINE</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>use_critic</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>NotImplementedError</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_validate_config</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_create_dataloader</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span> <span class=n>val_dataset</span><span class=p>,</span> <span class=n>collate_fn</span><span class=p>,</span> <span class=n>train_sampler</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></details><h3 id=rayppotrainerinit_workers><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L715><code>RayPPOTrainer.init_workers()</code></a><a hidden class=anchor aria-hidden=true href=#rayppotrainerinit_workers>#</a></h3><p><code>init_workers()</code> 函数负责在 Ray 集群上实例化和初始化 ActorRollout、Critic、Reference Policy 和 Reward Model Workers。</p><ol><li><strong>创建资源池</strong>：通过 <code>ResourcePoolManager</code> 创建 Ray 资源池。</li><li><strong>初始化资源池到类的映射</strong>：为每个资源池创建一个字典，用于存储不同角色 Worker 的 <code>RayClassWithInitArgs</code> 包装器。<code>RayClassWithInitArgs</code> 用于延迟初始化 Worker，存储了 Worker 的类和初始化参数。</li><li><strong>创建不同角色的 Worker 的 <code>RayClassWithInitArgs</code> 实例</strong>：根据配置启用情况，为 ActorRollout、Critic、Reference Policy 和 Reward Model 创建对应的 <code>RayClassWithInitArgs</code> 实例。</li><li><strong>初始化 WorkerGroup</strong>：遍历所有资源池，将同一资源池中的多个 Worker 类通过 <code>create_colocated_worker_cls</code> 组合成一个共置类，然后实例化 <code>RayWorkerGroup</code>。<code>RayWorkerGroup</code> 负责在多个 GPU 上启动多个 Worker 实例。最后调用 <code>spawn()</code> 方法在 Ray 中实际创建 Worker 实例。</li><li><strong>初始化各个 Worker</strong>：根据角色从创建的 WorkerGroup 字典中获取对应的 WorkerGroup，并调用其 <code>init_model()</code> 方法，按照依赖关系依次初始化不同的 Worker 模块。ActorRollout Worker 通常最后初始化以优化内存使用。</li></ol><details><summary>RayPPOTrainer.init_workers 源码</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span><span class=lnt>86
</span><span class=lnt>87
</span><span class=lnt>88
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>init_workers</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Initialize distributed training workers using Ray backend.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Creates:
</span></span></span><span class=line><span class=cl><span class=s2>        1. Ray resource pools from configuration
</span></span></span><span class=line><span class=cl><span class=s2>        2. Worker groups for each role (actor, critic, etc.)
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>create_resource_pool</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span> <span class=o>=</span> <span class=p>{</span><span class=n>pool</span><span class=p>:</span> <span class=p>{}</span> <span class=k>for</span> <span class=n>pool</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>resource_pool_dict</span><span class=o>.</span><span class=n>values</span><span class=p>()}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create actor and rollout</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>hybrid_engine</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>resource_pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>get_resource_pool</span><span class=p>(</span><span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_rollout_cls</span> <span class=o>=</span> <span class=n>RayClassWithInitArgs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=bp>cls</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>role</span><span class=o>=</span><span class=s2>&#34;actor_rollout&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=p>[</span><span class=n>resource_pool</span><span class=p>][</span><span class=s2>&#34;actor_rollout&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>actor_rollout_cls</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>NotImplementedError</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create critic</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_critic</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>resource_pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>get_resource_pool</span><span class=p>(</span><span class=n>Role</span><span class=o>.</span><span class=n>Critic</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>critic_cls</span> <span class=o>=</span> <span class=n>RayClassWithInitArgs</span><span class=p>(</span><span class=bp>cls</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>Critic</span><span class=p>],</span> <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>critic</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=p>[</span><span class=n>resource_pool</span><span class=p>][</span><span class=s2>&#34;critic&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>critic_cls</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create reference policy if needed</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_reference_policy</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>resource_pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>get_resource_pool</span><span class=p>(</span><span class=n>Role</span><span class=o>.</span><span class=n>RefPolicy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>ref_policy_cls</span> <span class=o>=</span> <span class=n>RayClassWithInitArgs</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>RefPolicy</span><span class=p>],</span> <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=p>,</span> <span class=n>role</span><span class=o>=</span><span class=s2>&#34;ref&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=p>[</span><span class=n>resource_pool</span><span class=p>][</span><span class=s2>&#34;ref&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>ref_policy_cls</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create a reward model if reward_fn is None</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_rm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># we create a RM here</span>
</span></span><span class=line><span class=cl>            <span class=n>resource_pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>get_resource_pool</span><span class=p>(</span><span class=n>Role</span><span class=o>.</span><span class=n>RewardModel</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>rm_cls</span> <span class=o>=</span> <span class=n>RayClassWithInitArgs</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>RewardModel</span><span class=p>],</span> <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>reward_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=p>[</span><span class=n>resource_pool</span><span class=p>][</span><span class=s2>&#34;rm&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>rm_cls</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># initialize WorkerGroup</span>
</span></span><span class=line><span class=cl>        <span class=c1># NOTE: if you want to use a different resource pool for each role, which can support different parallel size,</span>
</span></span><span class=line><span class=cl>        <span class=c1># you should not use `create_colocated_worker_cls`.</span>
</span></span><span class=line><span class=cl>        <span class=c1># Instead, directly pass different resource pool to different worker groups.</span>
</span></span><span class=line><span class=cl>        <span class=c1># See https://github.com/volcengine/verl/blob/master/examples/ray/tutorial.ipynb for more information.</span>
</span></span><span class=line><span class=cl>        <span class=n>all_wg</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>        <span class=n>wg_kwargs</span> <span class=o>=</span> <span class=p>{}</span>  <span class=c1># Setting up kwargs for RayWorkerGroup</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>OmegaConf</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=p>,</span> <span class=s2>&#34;ray_wait_register_center_timeout&#34;</span><span class=p>)</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>wg_kwargs</span><span class=p>[</span><span class=s2>&#34;ray_wait_register_center_timeout&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=o>.</span><span class=n>ray_wait_register_center_timeout</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>OmegaConf</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=p>,</span> <span class=s2>&#34;profile_steps&#34;</span><span class=p>)</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>wg_kwargs</span><span class=p>[</span><span class=s2>&#34;profile_steps&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>OmegaConf</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=p>,</span> <span class=s2>&#34;profile_steps&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>assert</span> <span class=n>OmegaConf</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=p>,</span> <span class=s2>&#34;worker_nsight_options&#34;</span><span class=p>)</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>,</span> <span class=s2>&#34;worker_nsight_options must be set when profile_steps is set&#34;</span>
</span></span><span class=line><span class=cl>            <span class=n>wg_kwargs</span><span class=p>[</span><span class=s2>&#34;worker_nsight_options&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>OmegaConf</span><span class=o>.</span><span class=n>to_container</span><span class=p>(</span><span class=n>OmegaConf</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=p>,</span> <span class=s2>&#34;worker_nsight_options&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>resource_pool</span><span class=p>,</span> <span class=n>class_dict</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>worker_dict_cls</span> <span class=o>=</span> <span class=n>create_colocated_worker_cls</span><span class=p>(</span><span class=n>class_dict</span><span class=o>=</span><span class=n>class_dict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>wg_dict</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ray_worker_group_cls</span><span class=p>(</span><span class=n>resource_pool</span><span class=o>=</span><span class=n>resource_pool</span><span class=p>,</span> <span class=n>ray_cls_with_init</span><span class=o>=</span><span class=n>worker_dict_cls</span><span class=p>,</span> <span class=n>device_name</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_name</span><span class=p>,</span> <span class=o>**</span><span class=n>wg_kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>spawn_wg</span> <span class=o>=</span> <span class=n>wg_dict</span><span class=o>.</span><span class=n>spawn</span><span class=p>(</span><span class=n>prefix_set</span><span class=o>=</span><span class=n>class_dict</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
</span></span><span class=line><span class=cl>            <span class=n>all_wg</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>spawn_wg</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_critic</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>critic_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;critic&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>critic_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_reference_policy</span> <span class=ow>and</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>ref_in_actor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ref_policy_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ref_policy_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_rm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>rm_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;rm&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>rm_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># we should create rollout at the end so that vllm can have a better estimation of kv cache memory</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>actor_rollout_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;actor_rollout&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>actor_rollout_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create async rollout manager and request scheduler</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>async_rollout_mode</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>mode</span> <span class=o>==</span> <span class=s2>&#34;async&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=kn>from</span> <span class=nn>verl.workers.rollout.async_server</span> <span class=kn>import</span> <span class=n>AsyncLLMServerManager</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>async_rollout_mode</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>async_rollout_manager</span> <span class=o>=</span> <span class=n>AsyncLLMServerManager</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>worker_group</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>actor_rollout_wg</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><details></div><footer class=post-footer><ul class=post-tags><li><a href=https://pillumina.github.io/tags/framework/>Framework</a></li><li><a href=https://pillumina.github.io/tags/verl/>Verl</a></li><li><a href=https://pillumina.github.io/tags/sglang/>Sglang</a></li></ul><nav class=paginav><a class=prev href=https://pillumina.github.io/posts/aiinfra/08-verl-multiturn-2/><span class=title>« Prev</span><br><span>[VeRL] Multi-Turn RL训练源码走读（2）</span>
</a><a class=next href=https://pillumina.github.io/posts/aiinfra/04-aiinfra-thinking/><span class=title>Next »</span><br><span>AI Infra：颠覆性创新，还是经典工程范式的华丽转身？</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RL训练源码走读（1） on x" href="https://x.com/intent/tweet/?text=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f&amp;hashtags=framework%2cverl%2csglang"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RL训练源码走读（1） on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f&amp;title=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89&amp;summary=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89&amp;source=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RL训练源码走读（1） on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f&title=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RL训练源码走读（1） on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RL训练源码走读（1） on whatsapp" href="https://api.whatsapp.com/send?text=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89%20-%20https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RL训练源码走读（1） on telegram" href="https://telegram.me/share/url?text=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RL训练源码走读（1） on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89&u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul><div class=related-posts><div class=related-series><h3>同系列文章</h3><ul><li><a href=/posts/aiinfra/09-verl-agentloop/>[VeRL] AgentLoop源码走读</a>
<span class=meta>2025-08-14
· 15 min read</span></li><li><a href=/posts/aiinfra/05-verl-params/>[VeRL] 参数速览</a>
<span class=meta>2025-08-14
· 6 min read</span></li><li><a href=/posts/aiinfra/08-verl-multiturn-2/>[VeRL] Multi-Turn RL训练源码走读（2）</a>
<span class=meta>2025-08-03
· 27 min read</span></li></ul></div><div class=related-tags><h3>相关文章</h3><ul><li><a href=/posts/aiinfra/09-verl-agentloop/>[VeRL] AgentLoop源码走读</a>
<span class=meta>2025-08-14
· 15 min read
· Tags: framework, verl, sglang</span></li><li><a href=/posts/aiinfra/05-verl-params/>[VeRL] 参数速览</a>
<span class=meta>2025-08-14
· 6 min read
· Tags: framework, verl</span></li><li><a href=/posts/aiinfra/06-sglang-backend/>[SGLang] 后端代码速览</a>
<span class=meta>2025-08-13
· 5 min read
· Tags: inference, sglang</span></li><li><a href=/posts/aiinfra/02-slime/>[RL4LLM] 异步RL框架: Slime</a>
<span class=meta>2025-08-07
· 15 min read
· Tags: framework, LLM, RL</span></li><li><a href=/posts/aiinfra/03-areal/>[RL4LLM] 异步RL框架: Areal</a>
<span class=meta>2025-08-07
· 23 min read
· Tags: framework, LLM, RL</span></li></ul></div></div></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://pillumina.github.io/>CctoctoFX</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div class=reading-progress-bar></div><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelector(".reading-progress-bar");if(!t)return;const n=document.querySelector(".post-single");if(!n)return;function s(){const e=n.getBoundingClientRect(),s=e.height,o=window.innerHeight,i=window.scrollY||window.pageYOffset,a=i/(s-o)*100;t.style.width=`${Math.min(100,Math.max(0,a))}%`}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){s(),e=!1}),e=!0)}),s()}),document.addEventListener("DOMContentLoaded",function(){mediumZoom("article img:not(.nozoom)",{margin:24,background:"var(--theme)",scrollOffset:0})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>