<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰ | CctoctoFX</title><meta name=keywords content="framework,verl,sglang"><meta name=description content='
è¯¥partä¸»è¦èšç„¦ç›¸å…³æ¨¡å—åˆå§‹åŒ–éƒ¨åˆ†
è¿˜æ˜¯ä»¥ verl å‡ºå‘ï¼Œåˆ†æå…¶ end to end mutli-turn RL è®­ç»ƒçš„å…¨è¿‡ç¨‹ã€‚æ•´ä½“ä¸Šï¼Œæˆ‘å¸Œæœ›è¦†ç›–æ‰€æœ‰é‡è¦çš„ class ä»¥åŠå‡½æ•°ï¼Œæ›´ç»†ç²’åº¦çš„ä»£ç ä¸å†å±•å¼€ã€‚
ä¸ºäº†å‰åå†…å®¹çš„ä¸€è‡´æ€§ï¼ŒåŸºäº 76f63cffa5 çš„ commit è¿›è¡Œåˆ†æã€‚
è™½ç„¶æœ¬æ–‡ä»¥åˆ†æ verl çš„ä»£ç ä¸ºä¸»ï¼Œå†™å®Œä¹‹åæˆ‘æ‰æ„è¯†åˆ°ï¼Œç³»ç»Ÿè®¾è®¡é—®é¢˜æ˜¯éå¸¸é€šç”¨çš„ã€‚è¯¸å¦‚â€œlog probs é‡è®¡ç®—â€ï¼Œâ€œRollout Engine æ˜¾å­˜ç®¡ç†â€ç­‰ç­‰ç³»ç»Ÿè®¾è®¡ï¼Œæ˜¯å„å¤§ RL æ¡†æ¶éƒ½éœ€è¦è€ƒè™‘çš„æ ¸å¿ƒé—®é¢˜ã€‚
æ­¤å¤–å› ä¸ºæœ€è¿‘åœ¨å­¦ä¹ SGLangçš„å®ç°ï¼Œæœ¬æ–‡çš„æ¨ç†åç«¯é€‰æ‹©çš„æ˜¯SGLangå±•å¼€åˆ†æã€‚

æ•´ä¸ªè®­ç»ƒçš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼Œæˆ‘ä»¬ä¼šå…·ä½“å±•å¼€æ¯ä¸ªéƒ¨åˆ†ã€‚

  flowchart LR
subgraph W2["Initialize"]
WP[Process Data] --> A
direction TB D1[Data Prepare] --> A
A[TaskRunner] --> B1[RayPPOTrainer]
B1 --> Workers

    subgraph Workers["Workers"]
        direction TB
                WA[ActorRolloutWorker] --> WD[FSDP Engine]
        WB[CriticWorker] --> WD
        WC[RewardModelWorker] --> WD
        WD --> WE[SGLang Engine]
    end
    
    Workers --> C1[Hybrid Engine]
end

subgraph W3["Train Loop"]
    direction TB
    E[DataLoader] --> RolloutBox
    
    subgraph RolloutBox["Rollout"]
        F1[Prepare Data] --> F2[SGLang Async Rollout]
        F2 --> F3[Multi-turn Chat Process]
    end
    
    RolloutBox --> ExpBox
    
    subgraph ExpBox["Make Experience"]
        G1[Recompute Log Probs] --> G2[Compute Reward]
        G2 --> G3[Compute Advantage]
    end
    
    ExpBox --> UpdateBox
    
    subgraph UpdateBox["Train The Model"]
        H1[Load FSDP Model Weight] --> H2[Compute Gradient]
        H2 --> H3[Weights Update]
        H3 --> H4[Sync Weights]
    end
    
    UpdateBox --> E
end

W2 --> W3


æ•°æ®é¢„å¤„ç†
ä»¥ GSM8K ä¸ºä¾‹ï¼Œé¢„å¤„ç†è„šæœ¬æ˜¯Â examples/data_preprocess/gsm8k_multiturn_w_tool.pyã€‚æ•´ä¸ªè„šæœ¬åªåšäº†ç»å…¸çš„ huggingface datasets mappingï¼Œæ ¸å¿ƒé€»è¾‘å¦‚ä¸‹ï¼š'><meta name=author content="Me"><link rel=canonical href=https://pillumina.github.io/posts/aiinfra/07-verl-multiturn-1/><link crossorigin=anonymous href=/assets/css/stylesheet.9d388901283682bb45dd422fcaa0d0a2054a3c8ff47c9cc6b2baab15508b1b90.css integrity="sha256-nTiJASg2grtF3UIvyqDQogVKPI/0fJzGsrqrFVCLG5A=" rel="preload stylesheet" as=style><link rel=icon href=https://pillumina.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://pillumina.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://pillumina.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://pillumina.github.io/apple-touch-icon.png><link rel=mask-icon href=https://pillumina.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://pillumina.github.io/posts/aiinfra/07-verl-multiturn-1/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>(function(){function t(){return document.querySelector(".post-content")||document.querySelector(".post-single")||document.body}function n(e){return/\$\$[\s\S]+?\$\$|\\\(|\\\)|\\\[|\\\]/.test(e)}function s(e){if(window.__mathjaxLoaded)return;window.__mathjaxLoaded=!0,window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code","tt"],ignoreHtmlClass:"no-math"}};var t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js",t.defer=!0,t.onload=function(){window.MathJax&&window.MathJax.typesetPromise&&window.MathJax.typesetPromise([e]).catch(function(e){console.warn("MathJax typeset error",e)})},document.head.appendChild(t)}function e(){try{if(typeof renderMathInElement=="function"){const e=t();renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,strict:!1,trust:!0,ignoredTags:["script","noscript","style","textarea","pre","code","tt"],ignoredClasses:["no-math"],macros:{"\\boldsymbol":"\\mathbf{#1}","\\bm":"\\mathbf{#1}"}}),setTimeout(function(){n(e.innerHTML)&&s(e)},200)}}catch(e){console.warn("KaTeX render error:",e)}}document.addEventListener("DOMContentLoaded",function(){e(),setTimeout(e,200)}),window.addEventListener("load",function(){setTimeout(e,0)})})()</script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>const config={startOnLoad:!0,theme:"neutral",themeVariables:{lineColor:"#0f0f0f"},flowchart:{useMaxWidth:!1,htmlLabels:!0}};mermaid.initialize(config),window.onload=()=>{window.mermaid.init(0[0],document.querySelectorAll(".language-mermaid"))}</script><link rel=stylesheet href=/css/custom.min.bda7229c4269a242639e058fb11a4782f02f8d77071ba16609befee67cc41c49.css integrity="sha256-vacinEJpokJjngWPsRpHgvAvjXcHG6FmCb7+5nzEHEk="><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]"),n=document.querySelectorAll(".toc a");if(t.length===0||n.length===0)return;const s={};t.forEach(e=>{s[e.id]=e.offsetTop});function i(){const t=window.scrollY+100;let e="";for(const[n,o]of Object.entries(s))if(t>=o)e=n;else break;return e}function o(){const e=i();if(n.forEach(e=>{e.classList.remove("active")}),e){const t=document.querySelector(`.toc a[href="#${e}"]`);t&&t.classList.add("active")}}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){o(),e=!1}),e=!0)}),o()})</script><meta property="og:url" content="https://pillumina.github.io/posts/aiinfra/07-verl-multiturn-1/"><meta property="og:site_name" content="CctoctoFX"><meta property="og:title" content="[VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰"><meta property="og:description" content=' è¯¥partä¸»è¦èšç„¦ç›¸å…³æ¨¡å—åˆå§‹åŒ–éƒ¨åˆ†
è¿˜æ˜¯ä»¥ verl å‡ºå‘ï¼Œåˆ†æå…¶ end to end mutli-turn RL è®­ç»ƒçš„å…¨è¿‡ç¨‹ã€‚æ•´ä½“ä¸Šï¼Œæˆ‘å¸Œæœ›è¦†ç›–æ‰€æœ‰é‡è¦çš„ class ä»¥åŠå‡½æ•°ï¼Œæ›´ç»†ç²’åº¦çš„ä»£ç ä¸å†å±•å¼€ã€‚
ä¸ºäº†å‰åå†…å®¹çš„ä¸€è‡´æ€§ï¼ŒåŸºäº 76f63cffa5 çš„ commit è¿›è¡Œåˆ†æã€‚
è™½ç„¶æœ¬æ–‡ä»¥åˆ†æ verl çš„ä»£ç ä¸ºä¸»ï¼Œå†™å®Œä¹‹åæˆ‘æ‰æ„è¯†åˆ°ï¼Œç³»ç»Ÿè®¾è®¡é—®é¢˜æ˜¯éå¸¸é€šç”¨çš„ã€‚è¯¸å¦‚â€œlog probs é‡è®¡ç®—â€ï¼Œâ€œRollout Engine æ˜¾å­˜ç®¡ç†â€ç­‰ç­‰ç³»ç»Ÿè®¾è®¡ï¼Œæ˜¯å„å¤§ RL æ¡†æ¶éƒ½éœ€è¦è€ƒè™‘çš„æ ¸å¿ƒé—®é¢˜ã€‚
æ­¤å¤–å› ä¸ºæœ€è¿‘åœ¨å­¦ä¹ SGLangçš„å®ç°ï¼Œæœ¬æ–‡çš„æ¨ç†åç«¯é€‰æ‹©çš„æ˜¯SGLangå±•å¼€åˆ†æã€‚
æ•´ä¸ªè®­ç»ƒçš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼Œæˆ‘ä»¬ä¼šå…·ä½“å±•å¼€æ¯ä¸ªéƒ¨åˆ†ã€‚
flowchart LR subgraph W2["Initialize"] WP[Process Data] --> A direction TB D1[Data Prepare] --> A A[TaskRunner] --> B1[RayPPOTrainer] B1 --> Workers subgraph Workers["Workers"] direction TB WA[ActorRolloutWorker] --> WD[FSDP Engine] WB[CriticWorker] --> WD WC[RewardModelWorker] --> WD WD --> WE[SGLang Engine] end Workers --> C1[Hybrid Engine] end subgraph W3["Train Loop"] direction TB E[DataLoader] --> RolloutBox subgraph RolloutBox["Rollout"] F1[Prepare Data] --> F2[SGLang Async Rollout] F2 --> F3[Multi-turn Chat Process] end RolloutBox --> ExpBox subgraph ExpBox["Make Experience"] G1[Recompute Log Probs] --> G2[Compute Reward] G2 --> G3[Compute Advantage] end ExpBox --> UpdateBox subgraph UpdateBox["Train The Model"] H1[Load FSDP Model Weight] --> H2[Compute Gradient] H2 --> H3[Weights Update] H3 --> H4[Sync Weights] end UpdateBox --> E end W2 --> W3 æ•°æ®é¢„å¤„ç† ä»¥ GSM8K ä¸ºä¾‹ï¼Œé¢„å¤„ç†è„šæœ¬æ˜¯Â examples/data_preprocess/gsm8k_multiturn_w_tool.pyã€‚æ•´ä¸ªè„šæœ¬åªåšäº†ç»å…¸çš„ huggingface datasets mappingï¼Œæ ¸å¿ƒé€»è¾‘å¦‚ä¸‹ï¼š'><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-03T15:30:12+08:00"><meta property="article:modified_time" content="2025-08-03T15:30:12+08:00"><meta property="article:tag" content="Framework"><meta property="article:tag" content="Verl"><meta property="article:tag" content="Sglang"><meta property="og:image" content="https://pillumina.github.io/imgs/icon_head.png"><meta property="og:see_also" content="https://pillumina.github.io/posts/aiinfra/12-verl-sglang-memory/"><meta property="og:see_also" content="https://pillumina.github.io/posts/aiinfra/10-verl-dataproto/"><meta property="og:see_also" content="https://pillumina.github.io/posts/aiinfra/09-verl-agentloop/"><meta property="og:see_also" content="https://pillumina.github.io/posts/aiinfra/05-verl-params/"><meta property="og:see_also" content="https://pillumina.github.io/posts/aiinfra/08-verl-multiturn-2/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://pillumina.github.io/imgs/icon_head.png"><meta name=twitter:title content="[VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰"><meta name=twitter:description content='
è¯¥partä¸»è¦èšç„¦ç›¸å…³æ¨¡å—åˆå§‹åŒ–éƒ¨åˆ†
è¿˜æ˜¯ä»¥ verl å‡ºå‘ï¼Œåˆ†æå…¶ end to end mutli-turn RL è®­ç»ƒçš„å…¨è¿‡ç¨‹ã€‚æ•´ä½“ä¸Šï¼Œæˆ‘å¸Œæœ›è¦†ç›–æ‰€æœ‰é‡è¦çš„ class ä»¥åŠå‡½æ•°ï¼Œæ›´ç»†ç²’åº¦çš„ä»£ç ä¸å†å±•å¼€ã€‚
ä¸ºäº†å‰åå†…å®¹çš„ä¸€è‡´æ€§ï¼ŒåŸºäº 76f63cffa5 çš„ commit è¿›è¡Œåˆ†æã€‚
è™½ç„¶æœ¬æ–‡ä»¥åˆ†æ verl çš„ä»£ç ä¸ºä¸»ï¼Œå†™å®Œä¹‹åæˆ‘æ‰æ„è¯†åˆ°ï¼Œç³»ç»Ÿè®¾è®¡é—®é¢˜æ˜¯éå¸¸é€šç”¨çš„ã€‚è¯¸å¦‚â€œlog probs é‡è®¡ç®—â€ï¼Œâ€œRollout Engine æ˜¾å­˜ç®¡ç†â€ç­‰ç­‰ç³»ç»Ÿè®¾è®¡ï¼Œæ˜¯å„å¤§ RL æ¡†æ¶éƒ½éœ€è¦è€ƒè™‘çš„æ ¸å¿ƒé—®é¢˜ã€‚
æ­¤å¤–å› ä¸ºæœ€è¿‘åœ¨å­¦ä¹ SGLangçš„å®ç°ï¼Œæœ¬æ–‡çš„æ¨ç†åç«¯é€‰æ‹©çš„æ˜¯SGLangå±•å¼€åˆ†æã€‚

æ•´ä¸ªè®­ç»ƒçš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼Œæˆ‘ä»¬ä¼šå…·ä½“å±•å¼€æ¯ä¸ªéƒ¨åˆ†ã€‚

  flowchart LR
subgraph W2["Initialize"]
WP[Process Data] --> A
direction TB D1[Data Prepare] --> A
A[TaskRunner] --> B1[RayPPOTrainer]
B1 --> Workers

    subgraph Workers["Workers"]
        direction TB
                WA[ActorRolloutWorker] --> WD[FSDP Engine]
        WB[CriticWorker] --> WD
        WC[RewardModelWorker] --> WD
        WD --> WE[SGLang Engine]
    end
    
    Workers --> C1[Hybrid Engine]
end

subgraph W3["Train Loop"]
    direction TB
    E[DataLoader] --> RolloutBox
    
    subgraph RolloutBox["Rollout"]
        F1[Prepare Data] --> F2[SGLang Async Rollout]
        F2 --> F3[Multi-turn Chat Process]
    end
    
    RolloutBox --> ExpBox
    
    subgraph ExpBox["Make Experience"]
        G1[Recompute Log Probs] --> G2[Compute Reward]
        G2 --> G3[Compute Advantage]
    end
    
    ExpBox --> UpdateBox
    
    subgraph UpdateBox["Train The Model"]
        H1[Load FSDP Model Weight] --> H2[Compute Gradient]
        H2 --> H3[Weights Update]
        H3 --> H4[Sync Weights]
    end
    
    UpdateBox --> E
end

W2 --> W3


æ•°æ®é¢„å¤„ç†
ä»¥ GSM8K ä¸ºä¾‹ï¼Œé¢„å¤„ç†è„šæœ¬æ˜¯Â examples/data_preprocess/gsm8k_multiturn_w_tool.pyã€‚æ•´ä¸ªè„šæœ¬åªåšäº†ç»å…¸çš„ huggingface datasets mappingï¼Œæ ¸å¿ƒé€»è¾‘å¦‚ä¸‹ï¼š'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://pillumina.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI Infra","item":"https://pillumina.github.io/posts/aiinfra/"},{"@type":"ListItem","position":3,"name":"[VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰","item":"https://pillumina.github.io/posts/aiinfra/07-verl-multiturn-1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰","name":"[VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰","description":" è¯¥partä¸»è¦èšç„¦ç›¸å…³æ¨¡å—åˆå§‹åŒ–éƒ¨åˆ†\nè¿˜æ˜¯ä»¥ verl å‡ºå‘ï¼Œåˆ†æå…¶ end to end mutli-turn RL è®­ç»ƒçš„å…¨è¿‡ç¨‹ã€‚æ•´ä½“ä¸Šï¼Œæˆ‘å¸Œæœ›è¦†ç›–æ‰€æœ‰é‡è¦çš„ class ä»¥åŠå‡½æ•°ï¼Œæ›´ç»†ç²’åº¦çš„ä»£ç ä¸å†å±•å¼€ã€‚\nä¸ºäº†å‰åå†…å®¹çš„ä¸€è‡´æ€§ï¼ŒåŸºäº 76f63cffa5 çš„ commit è¿›è¡Œåˆ†æã€‚\nè™½ç„¶æœ¬æ–‡ä»¥åˆ†æ verl çš„ä»£ç ä¸ºä¸»ï¼Œå†™å®Œä¹‹åæˆ‘æ‰æ„è¯†åˆ°ï¼Œç³»ç»Ÿè®¾è®¡é—®é¢˜æ˜¯éå¸¸é€šç”¨çš„ã€‚è¯¸å¦‚â€œlog probs é‡è®¡ç®—â€ï¼Œâ€œRollout Engine æ˜¾å­˜ç®¡ç†â€ç­‰ç­‰ç³»ç»Ÿè®¾è®¡ï¼Œæ˜¯å„å¤§ RL æ¡†æ¶éƒ½éœ€è¦è€ƒè™‘çš„æ ¸å¿ƒé—®é¢˜ã€‚\næ­¤å¤–å› ä¸ºæœ€è¿‘åœ¨å­¦ä¹ SGLangçš„å®ç°ï¼Œæœ¬æ–‡çš„æ¨ç†åç«¯é€‰æ‹©çš„æ˜¯SGLangå±•å¼€åˆ†æã€‚\næ•´ä¸ªè®­ç»ƒçš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼Œæˆ‘ä»¬ä¼šå…·ä½“å±•å¼€æ¯ä¸ªéƒ¨åˆ†ã€‚\nflowchart LR subgraph W2[\u0026#34;Initialize\u0026#34;] WP[Process Data] --\u0026gt; A direction TB D1[Data Prepare] --\u0026gt; A A[TaskRunner] --\u0026gt; B1[RayPPOTrainer] B1 --\u0026gt; Workers subgraph Workers[\u0026#34;Workers\u0026#34;] direction TB WA[ActorRolloutWorker] --\u0026gt; WD[FSDP Engine] WB[CriticWorker] --\u0026gt; WD WC[RewardModelWorker] --\u0026gt; WD WD --\u0026gt; WE[SGLang Engine] end Workers --\u0026gt; C1[Hybrid Engine] end subgraph W3[\u0026#34;Train Loop\u0026#34;] direction TB E[DataLoader] --\u0026gt; RolloutBox subgraph RolloutBox[\u0026#34;Rollout\u0026#34;] F1[Prepare Data] --\u0026gt; F2[SGLang Async Rollout] F2 --\u0026gt; F3[Multi-turn Chat Process] end RolloutBox --\u0026gt; ExpBox subgraph ExpBox[\u0026#34;Make Experience\u0026#34;] G1[Recompute Log Probs] --\u0026gt; G2[Compute Reward] G2 --\u0026gt; G3[Compute Advantage] end ExpBox --\u0026gt; UpdateBox subgraph UpdateBox[\u0026#34;Train The Model\u0026#34;] H1[Load FSDP Model Weight] --\u0026gt; H2[Compute Gradient] H2 --\u0026gt; H3[Weights Update] H3 --\u0026gt; H4[Sync Weights] end UpdateBox --\u0026gt; E end W2 --\u0026gt; W3 æ•°æ®é¢„å¤„ç† ä»¥ GSM8K ä¸ºä¾‹ï¼Œé¢„å¤„ç†è„šæœ¬æ˜¯Â examples/data_preprocess/gsm8k_multiturn_w_tool.pyã€‚æ•´ä¸ªè„šæœ¬åªåšäº†ç»å…¸çš„ huggingface datasets mappingï¼Œæ ¸å¿ƒé€»è¾‘å¦‚ä¸‹ï¼š\n","keywords":["framework","verl","sglang"],"articleBody":" è¯¥partä¸»è¦èšç„¦ç›¸å…³æ¨¡å—åˆå§‹åŒ–éƒ¨åˆ†\nè¿˜æ˜¯ä»¥ verl å‡ºå‘ï¼Œåˆ†æå…¶ end to end mutli-turn RL è®­ç»ƒçš„å…¨è¿‡ç¨‹ã€‚æ•´ä½“ä¸Šï¼Œæˆ‘å¸Œæœ›è¦†ç›–æ‰€æœ‰é‡è¦çš„ class ä»¥åŠå‡½æ•°ï¼Œæ›´ç»†ç²’åº¦çš„ä»£ç ä¸å†å±•å¼€ã€‚\nä¸ºäº†å‰åå†…å®¹çš„ä¸€è‡´æ€§ï¼ŒåŸºäº 76f63cffa5 çš„ commit è¿›è¡Œåˆ†æã€‚\nè™½ç„¶æœ¬æ–‡ä»¥åˆ†æ verl çš„ä»£ç ä¸ºä¸»ï¼Œå†™å®Œä¹‹åæˆ‘æ‰æ„è¯†åˆ°ï¼Œç³»ç»Ÿè®¾è®¡é—®é¢˜æ˜¯éå¸¸é€šç”¨çš„ã€‚è¯¸å¦‚â€œlog probs é‡è®¡ç®—â€ï¼Œâ€œRollout Engine æ˜¾å­˜ç®¡ç†â€ç­‰ç­‰ç³»ç»Ÿè®¾è®¡ï¼Œæ˜¯å„å¤§ RL æ¡†æ¶éƒ½éœ€è¦è€ƒè™‘çš„æ ¸å¿ƒé—®é¢˜ã€‚\næ­¤å¤–å› ä¸ºæœ€è¿‘åœ¨å­¦ä¹ SGLangçš„å®ç°ï¼Œæœ¬æ–‡çš„æ¨ç†åç«¯é€‰æ‹©çš„æ˜¯SGLangå±•å¼€åˆ†æã€‚\næ•´ä¸ªè®­ç»ƒçš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼Œæˆ‘ä»¬ä¼šå…·ä½“å±•å¼€æ¯ä¸ªéƒ¨åˆ†ã€‚\nflowchart LR subgraph W2[\"Initialize\"] WP[Process Data] --\u003e A direction TB D1[Data Prepare] --\u003e A A[TaskRunner] --\u003e B1[RayPPOTrainer] B1 --\u003e Workers subgraph Workers[\"Workers\"] direction TB WA[ActorRolloutWorker] --\u003e WD[FSDP Engine] WB[CriticWorker] --\u003e WD WC[RewardModelWorker] --\u003e WD WD --\u003e WE[SGLang Engine] end Workers --\u003e C1[Hybrid Engine] end subgraph W3[\"Train Loop\"] direction TB E[DataLoader] --\u003e RolloutBox subgraph RolloutBox[\"Rollout\"] F1[Prepare Data] --\u003e F2[SGLang Async Rollout] F2 --\u003e F3[Multi-turn Chat Process] end RolloutBox --\u003e ExpBox subgraph ExpBox[\"Make Experience\"] G1[Recompute Log Probs] --\u003e G2[Compute Reward] G2 --\u003e G3[Compute Advantage] end ExpBox --\u003e UpdateBox subgraph UpdateBox[\"Train The Model\"] H1[Load FSDP Model Weight] --\u003e H2[Compute Gradient] H2 --\u003e H3[Weights Update] H3 --\u003e H4[Sync Weights] end UpdateBox --\u003e E end W2 --\u003e W3 æ•°æ®é¢„å¤„ç† ä»¥ GSM8K ä¸ºä¾‹ï¼Œé¢„å¤„ç†è„šæœ¬æ˜¯Â examples/data_preprocess/gsm8k_multiturn_w_tool.pyã€‚æ•´ä¸ªè„šæœ¬åªåšäº†ç»å…¸çš„ huggingface datasets mappingï¼Œæ ¸å¿ƒé€»è¾‘å¦‚ä¸‹ï¼š\nåŠ è½½ openai/gsm8k åŸå§‹æ•°æ®é›†ï¼ˆtrain/testï¼‰ã€‚ å¯¹æ¯æ¡åŸå§‹æ•°æ®ï¼Œç”Ÿæˆå¸¦æœ‰å·¥å…·è°ƒç”¨è¦æ±‚çš„ promptï¼ˆæ¯”å¦‚åœ¨ user turn å¼ºè°ƒæ¨¡å‹å¯ä»¥è°ƒç”¨Â calc_gsm8k_rewardÂ å·¥å…·ï¼Œæ¯ä¸ªqaè‡³å°‘è°ƒç”¨ä¸€æ¬¡ï¼‰ã€‚ åŒæ ·å¯¹äºæ¯æ¡åŸå§‹æ•°æ®ï¼Œè§£æç­”æ¡ˆï¼›å°† ground truth å†™å…¥ extra_info å­—æ®µã€‚ å­˜å‚¨ä¸º parquet æ–‡ä»¶ï¼Œåˆ†åˆ«ä¿ç•™ä¸º train.parquet å’Œ test.parquetï¼Œé»˜è®¤è·¯å¾„ä¸ºÂ ~/data/gsm8k/ã€‚ å¯åŠ¨è®­ç»ƒ ä¸€ä¸ªå…¸å‹çš„å¯åŠ¨å‘½ä»¤å¦‚ä¸‹ï¼š\n1 2 3 4 5 6 7 8 9 10 # now ç”¨äºç”Ÿæˆå®éªŒå¯åŠ¨çš„æ—¶é—´å°¾ç¼€ï¼Œé¿å…é‡å¤å¯åŠ¨å®éªŒæ—¶è¦†ç›–å·²æœ‰ wandb log function now() { date '+%Y-%m-%d-%H-%M' } export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 nohup bash examples/sglang_multiturn/run_qwen2.5-3b_gsm8k_multiturn.sh \\ trainer.experiment_name=qwen2.5-3b_rm-gsm8k-sgl-multiturn-$now \\ \u003e logs/gsm8k-$now.log 2\u003e\u00261 \u0026 è„šæœ¬é…ç½® verl çš„å„é¡¹å‚æ•°å®å±å¤æ‚ï¼Œæˆ‘ä»¬ä¼šå•ç‹¬ç¼–å†™æ–‡æ¡£æ¥åˆ†äº«å¯¹ verl å„ç±»å‚æ•°çš„ç†è§£ã€‚åœ¨è¿™ç¯‡æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦æ ¼å¤–å¼ºè°ƒçš„æ˜¯ verl å„ç±» config çš„è¦†ç›–å…³ç³»ã€‚verl çš„é…ç½®æ–‡ä»¶åˆ©ç”¨ hydra è¿›è¡Œäº†åˆ†å±‚è¦†ç›–çš„è®¾è®¡æ¨¡å¼ã€‚\nHydra ç®€ä»‹ Hydra æ˜¯ä¸€ä¸ªç”± Facebook Research å¼€å‘çš„ Python æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜é›…åœ°é…ç½®å¤æ‚çš„åº”ç”¨ç¨‹åºã€‚å®ƒç‰¹åˆ«é€‚ç”¨äºéœ€è¦ç®¡ç†å¤§é‡å‚æ•°å’Œè¿›è¡Œå¤šç»„å®éªŒçš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨å­¦ä¹ é¡¹ç›®ã€‚Hydra çš„æ ¸å¿ƒç‰¹ç‚¹åœ¨äºå…¶åŠ¨æ€ã€åˆ†å±‚å’Œå¯ç»„åˆçš„é…ç½®ç®¡ç†èƒ½åŠ›ã€‚Hydra çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š\nåˆ†å±‚é…ç½® (Hierarchical Configuration)ï¼šå¯ä»¥å°†é…ç½®åˆ†è§£æˆå¤šä¸ªå°å‹ã€æ¨¡å—åŒ–çš„ YAML æ–‡ä»¶ï¼Œå¹¶ä»¥ç›®å½•ç»“æ„è¿›è¡Œç»„ç»‡ã€‚è¿™ä½¿å¾—é…ç½®æ›´åŠ æ¸…æ™°ã€æ˜“äºç®¡ç†å’Œå¤ç”¨ã€‚ é…ç½®ç»„åˆ (Configuration Composition)ï¼šHydra èƒ½å¤Ÿå°†è¿™äº›ç‹¬ç«‹çš„é…ç½®æ¨¡å—åŠ¨æ€åœ°ç»„åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„é…ç½®å¯¹è±¡ã€‚ä½ å¯ä»¥é€šè¿‡åœ¨ä¸»é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š defaults åˆ—è¡¨æ¥é€‰æ‹©å’Œç»„åˆä¸åŒçš„é…ç½®ç»„ä»¶ã€‚ å‘½ä»¤è¡Œè¦†ç›– (Command-line Overrides)ï¼šè¿™æ˜¯ Hydra æœ€å¼ºå¤§çš„åŠŸèƒ½ä¹‹ä¸€ã€‚ä½ å¯ä»¥åœ¨è¿è¡Œåº”ç”¨ç¨‹åºæ—¶ï¼Œç›´æ¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ¥è¦†ç›–é…ç½®ä¸­çš„ä»»ä½•å€¼ã€‚è¿™ä½¿å¾—è¿›è¡Œå®éªŒå’Œå¿«é€Ÿè¿­ä»£å˜å¾—éå¸¸æ–¹ä¾¿ï¼Œæ— éœ€ä¿®æ”¹é…ç½®æ–‡ä»¶æœ¬èº«ã€‚ å¤šè¿è¡Œæ¨¡å¼ (Multi-run)ï¼šHydra å…è®¸ä½ é€šè¿‡ä¸€ä¸ªå‘½ä»¤è¿è¡Œå¤šä¸ªå…·æœ‰ä¸åŒé…ç½®çš„å®éªŒã€‚è¿™å¯¹äºè¶…å‚æ•°æœç´¢å’Œæ¨¡å‹æ¯”è¾ƒéå¸¸æœ‰ç”¨ã€‚ åŠ¨æ€å·¥ä½œç›®å½• (Dynamic Working Directory)ï¼šæ¯æ¬¡è¿è¡Œåº”ç”¨ç¨‹åºæ—¶ï¼ŒHydra éƒ½ä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„å·¥ä½œç›®å½•ï¼Œå¹¶å°†å½“å‰è¿è¡Œçš„é…ç½®å’Œè¾“å‡ºä¿å­˜åˆ°è¯¥ç›®å½•ä¸­ï¼Œç¡®ä¿å®éªŒçš„å¯å¤ç°æ€§ã€‚ å¯¹è±¡å®ä¾‹åŒ– (Object Instantiation)ï¼šHydra å¯ä»¥ç›´æ¥ä»é…ç½®ä¸­å®ä¾‹åŒ– Python å¯¹è±¡ï¼ˆç±»æˆ–å‡½æ•°ï¼‰ï¼Œè¿™å¤§å¤§ç®€åŒ–äº†ä»£ç ï¼Œä½¿é…ç½®æ›´å…·å£°æ˜æ€§ã€‚ Hydra å®ç°åˆ†å±‚è¦†ç›–çš„ä¸»è¦æœºåˆ¶æ˜¯ç»„åˆ (Composition) å’Œ å‘½ä»¤è¡Œè¦†ç›– (Command-line Overrides)ã€‚\nåˆ†å±‚é…ç½®çš„ç»„ç»‡ï¼š é€šå¸¸ä¼šåˆ›å»ºä¸€ä¸ª conf ç›®å½•ï¼Œå¹¶åœ¨å…¶ä¸­ç»„ç»‡é…ç½®ã€‚ä¾‹å¦‚ï¼š\n1 2 3 4 5 6 7 8 9 10 . â”œâ”€â”€ my_app.py â””â”€â”€ conf â”œâ”€â”€ config.yaml â”œâ”€â”€ model â”‚ â”œâ”€â”€ cnn.yaml â”‚ â””â”€â”€ rnn.yaml â””â”€â”€ dataset â”œâ”€â”€ cifar10.yaml â””â”€â”€ imagenet.yaml config.yaml æ˜¯ä½ çš„ä¸»é…ç½®æ–‡ä»¶ã€‚åœ¨ model ç›®å½•ä¸‹ï¼Œä½ å¯ä»¥å®šä¹‰ä¸åŒçš„æ¨¡å‹é…ç½®ï¼ˆå¦‚ cnn.yamlã€rnn.yamlï¼‰ï¼Œåœ¨ dataset ç›®å½•ä¸‹å®šä¹‰ä¸åŒçš„æ•°æ®é›†é…ç½®ï¼ˆå¦‚ cifar10.yamlã€imagenet.yamlï¼‰ã€‚\ndefaults åˆ—è¡¨è¿›è¡Œç»„åˆï¼š åœ¨ config.yaml ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨ç‰¹æ®Šçš„ defaults åˆ—è¡¨æ¥æŒ‡å®šé»˜è®¤åŠ è½½å“ªäº›é…ç½®ç»„ä»¶ã€‚\nconf/config.yaml ç¤ºä¾‹ï¼š\n1 2 3 4 5 6 7 8 defaults: - model: cnn # é»˜è®¤åŠ è½½ conf/model/cnn.yaml - dataset: cifar10 # é»˜è®¤åŠ è½½ conf/dataset/cifar10.yaml - _self_ # ç¡®ä¿å½“å‰æ–‡ä»¶ä¸­çš„å…¶ä»–é…ç½®é¡¹ä¹Ÿè¢«åŠ è½½ # å…¶ä»–åº”ç”¨çº§åˆ«çš„é»˜è®¤é…ç½® learning_rate: 0.001 epochs: 10 å½“ Hydra åŠ è½½ config.yaml æ—¶ï¼Œå®ƒä¼šæ ¹æ® defaults åˆ—è¡¨ä¸­çš„æŒ‡ç¤ºï¼Œè‡ªåŠ¨å°† conf/model/cnn.yaml å’Œ conf/dataset/cifar10.yaml çš„å†…å®¹åˆå¹¶åˆ°æœ€ç»ˆçš„é…ç½®å¯¹è±¡ä¸­ã€‚\nå‘½ä»¤è¡Œè¦†ç›–ï¼š è¿™æ˜¯å®ç°çµæ´»è¦†ç›–çš„å…³é”®ã€‚ä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ¥è¦†ç›–ä»»ä½•å·²åŠ è½½çš„é…ç½®å€¼ï¼ŒåŒ…æ‹¬åœ¨ defaults åˆ—è¡¨ä¸­æŒ‡å®šçš„ç»„ä»¶æˆ–å…¶å†…éƒ¨çš„ä»»ä½•å‚æ•°ã€‚\nè¦†ç›–æ•´ä¸ªé…ç½®ç»„ï¼š\nè¦åˆ‡æ¢æ¨¡å‹ä» cnn åˆ° rnnï¼Œä½ å¯ä»¥åœ¨å‘½ä»¤è¡Œä¸­è¿™æ ·è¿è¡Œï¼š 1 python my_app.py model=rnn è¿™å°†æŒ‡ç¤º Hydra åŠ è½½ conf/model/rnn.yamlï¼Œå¹¶ç”¨å®ƒæ¥æ›¿æ¢é»˜è®¤çš„ cnn é…ç½®ã€‚\nè¦†ç›–ç‰¹å®šå‚æ•°ï¼š\nä½ å¯ä»¥æ·±å…¥åˆ°é…ç½®çš„ä»»ä½•å±‚çº§æ¥è¦†ç›–ç‰¹å®šçš„å‚æ•°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³ä¿®æ”¹å­¦ä¹ ç‡æˆ–æ•°æ®é›†çš„æŸä¸ªå‚æ•°ï¼š 1 python my_app.py learning_rate=0.01 dataset.batch_size=64 è¿™é‡Œï¼Œlearning_rate ç›´æ¥è¦†ç›–äº† config.yaml ä¸­çš„å€¼ï¼Œè€Œ dataset.batch_size åˆ™è¦†ç›–äº† conf/dataset/cifar10.yamlï¼ˆæˆ–è€…ä½ é€šè¿‡ dataset=imagenet æŒ‡å®šçš„å…¶ä»–æ•°æ®é›†é…ç½®æ–‡ä»¶ï¼‰ä¸­çš„ batch_size å‚æ•°ã€‚\næ·»åŠ æ–°å‚æ•° (ä½¿ç”¨ +)ï¼š\nå¦‚æœä½ æƒ³æ·»åŠ ä¸€ä¸ªåœ¨é»˜è®¤é…ç½®ä¸­ä¸å­˜åœ¨çš„æ–°å‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨ + å‰ç¼€ï¼š 1 python my_app.py +optimizer.name=AdamW åŠ¨æ€è¦†ç›– (ä½¿ç”¨ ++)ï¼š\nå¦‚æœä½ å¸Œæœ›ä¿®æ”¹ä¸€ä¸ªå·²æœ‰å­—æ®µï¼Œæˆ–è€…åœ¨åŸé…ç½®ä¸­æ²¡æœ‰è¯¥å­—æ®µæ—¶è‡ªåŠ¨åˆ›å»ºå®ƒï¼Œå¯ä»¥ä½¿ç”¨ ++ã€‚è¿™ç§æ–¹å¼é€‚ç”¨äºéœ€è¦åŠ¨æ€æ·»åŠ æˆ–è¦†ç›–é…ç½®é¡¹çš„åœºæ™¯ï¼Œç¡®ä¿å­—æ®µæ€»æ˜¯è¢«è®¾ç½®ä¸ºä½ æŒ‡å®šçš„å€¼ï¼Œæ— è®ºå®ƒæ˜¯å¦å·²å­˜åœ¨ã€‚ 1 python my_app.py ++model.num_layers=10 Hydra å†…éƒ¨ä½¿ç”¨ OmegaConf åº“æ¥å¤„ç†è¿™äº›é…ç½®å¯¹è±¡ï¼Œå®ƒæä¾›äº†å¼ºå¤§çš„åˆå¹¶å’Œè§£æåŠŸèƒ½ï¼Œä½¿å¾—åˆ†å±‚è¦†ç›–å’Œå€¼æ’å€¼ï¼ˆä¾‹å¦‚ï¼Œå¼•ç”¨å…¶ä»–é…ç½®å€¼æˆ–ç¯å¢ƒå˜é‡ï¼‰å˜å¾—éå¸¸å®¹æ˜“ã€‚\nå›åˆ° verl multi turnï¼Œåœ¨æˆ‘ä»¬å¯åŠ¨çš„ run_qwen2.5-3b_gsm8k_multiturn.sh ä¸­ï¼Œè®¾ç½®äº†ï¼š\n1 2 3 4 5 6 PROJECT_DIR=\"$(pwd)\" CONFIG_PATH=\"$PROJECT_DIR/examples/sglang_multiturn/config\" python3 -m verl.trainer.main_ppo \\ --config-path=\"$CONFIG_PATH\" \\ --config-name='gsm8k_multiturn_grpo' \\ è¿™æ„å‘³ç€è¿™æ¬¡ä»»åŠ¡çš„é»˜è®¤ config æ˜¯ CONFIG_PATH ä¸‹çš„ gsm8k_multiturn_grpo.yamlï¼Œä¸”æ¥ä¸‹æ¥çš„å‚æ•°ä¼šè¦†ç›– gsm8k_multiturn_grpo.yaml ä¸­çš„é»˜è®¤å€¼ã€‚æ›´è¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬æ¥è§‚å¯Ÿ gsm8k_multiturn_grpo.yaml çš„å†…å®¹ï¼š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 hydra: searchpath: - file://verl/trainer/config defaults: - ppo_trainer - _self_ data: max_prompt_length: 1024 max_response_length: 1024 train_batch_size: 256 return_raw_chat: True actor_rollout_ref: hybrid_engine: True rollout: name: sglang multi_turn: enable: True max_turns: 5 # tool_config_path: \"./config/tool_config/gsm8k_tool_config.yaml\" è¿™é‡Œ hydra è¯­æ³•ï¼Œä¼šå» verl/trainer/config ç›®å½•ä¸‹å¯»æ‰¾ ppo_trainer.yaml ä½œä¸ºåŸºç¡€é…ç½®ï¼Œå¹¶ä¸”è¦†ç›–ã€‚å› æ­¤ï¼Œå¯åŠ¨ run_qwen2.5-3b_gsm8k_multiturn.sh æ—¶ï¼Œå…ˆåŠ è½½ gsm8k_multiturn_grpo.yaml ä½œä¸ºåŸºç¡€é…ç½®å¹¶è¦†ç›–ï¼Œç„¶ååŠ è½½ ppo_trainer.yaml å¹¶è¦†ç›–ã€‚æœ€ç»ˆåˆå¹¶è¿™ä¸‰çº§é…ç½®ï¼Œå¾—åˆ°æœ€ç»ˆçš„ configã€‚\næœ€åï¼Œæ³¨æ„åˆ°åœ¨ run_qwen2.5-3b_gsm8k_multiturn.sh çš„æœ€åï¼Œæˆ‘ä»¬ï¼Œæˆ‘ä»¬è®¾ç½®äº† actor_rollout_ref.rollout.multi_turn.tool_config_path=\"$PROJECT_DIR/examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml\"ï¼Œè¿™é‡ŒæŒ‡å®š multi_turn çš„ tool_config_path ä¸º examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yamlã€‚è¿™ä¸€æ–‡ä»¶ä»…ä»…é…ç½®äº† gsm8k çš„ tool è°ƒç”¨ï¼Œå¹¶ä¸ä¼šè¦†ç›–ä¹‹å‰è®­ç»ƒçš„ configã€‚\nè®­ç»ƒä¸»å…¥å£ä¸åˆå§‹åŒ– Ray Actorï¼ŒRay Task å’Œ Ray Worker åœ¨ä»‹ç» verl çš„è®­ç»ƒä¸»å…¥å£ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆä»‹ç» Ray çš„ä¸€äº›æ ¸å¿ƒæ¦‚å¿µã€‚Ray æ˜¯ä¸€ä¸ªç»Ÿä¸€è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ç®€å•åœ°ä»å•æœºåˆ°å¤§å‹åˆ†å¸ƒå¼é›†ç¾¤çš„æ‰©å±•ï¼Œæä¾›æ„å»ºå’Œè¿è¡Œåˆ†å¸ƒå¼åº”ç”¨çš„åº•å±‚åŸºç¡€è®¾æ–½å’Œä¸€ç»„æ ¸å¿ƒåŸè¯­ã€‚Ray é€šè¿‡ä»¥ä¸‹åŠŸèƒ½å®ç°è¿™ä¸€ç›®æ ‡ï¼š\nç»Ÿä¸€ APIï¼šRay æä¾›äº†ä¸€å¥—ç®€å•æ˜“ç”¨çš„ Python APIï¼Œå°†æ™®é€šå‡½æ•°è½¬æ¢ä¸ºåˆ†å¸ƒå¼ä»»åŠ¡ï¼Œå°† Python ç±»è½¬æ¢ä¸ºåˆ†å¸ƒå¼æœåŠ¡ï¼Œä¹Ÿå³ Ray Actorã€‚Ray Actor å†…éƒ¨æŒä¹…å­˜å‚¨çš„æ•°æ®ç§°ä¸ºçŠ¶æ€ï¼Œå¯ä»¥åœ¨ Actor çš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸå†…è¢«å¤šæ¬¡è®¿é—®ã€ä¿®æ”¹å’Œç»´æŠ¤ï¼Œè€Œä¸ä¼šåœ¨æ¯æ¬¡æ–¹æ³•è°ƒç”¨ç»“æŸåæ¶ˆå¤±ã€‚ å¼¹æ€§ä¼¸ç¼©ï¼šRay å¯ä»¥å°†åº”ç”¨ä»å•ä¸ªæœºå™¨æ— ç¼æ‰©å±•åˆ°æ‹¥æœ‰æ•°åƒä¸ªèŠ‚ç‚¹çš„é›†ç¾¤ï¼Œå¹¶èƒ½æ ¹æ®éœ€æ±‚è‡ªåŠ¨æ‰©ç¼©å®¹ã€‚ å®¹é”™æ€§ï¼šRay å†…ç½®äº†å®¹é”™æœºåˆ¶ï¼Œå¯ä»¥å¤„ç†èŠ‚ç‚¹æ•…éšœå’Œä»»åŠ¡å¤±è´¥ï¼Œç¡®ä¿åº”ç”¨çš„å¥å£®æ€§ã€‚ æ€§èƒ½ä¼˜åŒ–ï¼šRay ä¼˜åŒ–äº†åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦ã€å†…å­˜ç®¡ç†å’Œæ•°æ®ä¼ è¾“ï¼Œä»¥å®ç°é«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—ã€‚ Ray Task å’Œ Ray Actor éƒ½æ˜¯ç”¨äºåˆ†å¸ƒå¼è®¡ç®—çš„æ ¸å¿ƒåŸè¯­ï¼Œä½†å®ƒä»¬å„è‡ªæœåŠ¡äºä¸åŒçš„ç›®çš„ï¼Œä¸»è¦åŒºåˆ«åœ¨äºæ˜¯å¦ç»´æŠ¤çŠ¶æ€ã€‚\nRay Task æ˜¯ Ray ä¸­æœ€åŸºæœ¬çš„è®¡ç®—å•å…ƒï¼Œä»£è¡¨ä¸€ä¸ªæ— çŠ¶æ€çš„è¿œç¨‹å‡½æ•°ã€‚Ray Task çš„æ¯æ¬¡æ‰§è¡Œéƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œä¸ä¿ç•™ä¹‹å‰çš„ä»»ä½•ä¿¡æ¯ã€‚å°±åƒè°ƒç”¨ä¸€ä¸ªæ™®é€šå‡½æ•°ï¼Œæ‰§è¡Œå®Œåå°±æ¸…é™¤å†…éƒ¨çŠ¶æ€ã€‚æˆ‘ä»¬è°ƒç”¨ä¸€ä¸ª Ray Task åï¼Œä¼šç«‹å³è¿”å›å¾—åˆ°ä¸€ä¸ª Ray ObjectRefï¼Œè€Œä¸æ˜¯å®é™…çš„ç»“æœã€‚ä¸»ç¨‹åºå¯ä»¥ç»§ç»­æ‰§è¡Œå…¶ä»–æ“ä½œï¼Œè€Œ Ray Task åˆ™åœ¨åå°å¹¶è¡Œè¿è¡Œã€‚æˆ‘ä»¬éœ€è¦ä½¿ç”¨ ray.get() æ¥è·å– Task çš„å®é™…ç»“æœã€‚ Ray Task éå¸¸é€‚åˆå¹¶è¡Œæ‰§è¡Œå¤§é‡ç‹¬ç«‹ã€ä¸€æ¬¡æ€§çš„è®¡ç®—ä»»åŠ¡ï¼Œè­¬å¦‚æ•°æ®æ‰¹å¤„ç†ã€ç‹¬ç«‹çš„æ¨¡å‹æ¨ç†ç­‰åœºæ™¯ã€‚\nRay Actor æ˜¯ä¸€ç§ç‰¹æ®Šçš„ Ray Taskï¼Œæ­£å¦‚å‰æ–‡æ‰€è¿°ï¼Œå®ƒæ˜¯ä¸€ä¸ªæŒç»­è¿è¡Œçš„ã€æœ‰è‡ªå·±çš„çŠ¶æ€å’Œæ–¹æ³•çš„è¿œç¨‹å¯¹è±¡ã€‚å½“æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª Ray Actor åï¼ŒRay ä¼šåœ¨é›†ç¾¤ä¸­çš„æŸä¸ª Ray Worker ä¸Šå¯åŠ¨ä¸€ä¸ªä¸“é—¨çš„è¿›ç¨‹æ¥æ‰˜ç®¡è¿™ä¸ªå¯¹è±¡ã€‚è¯¥è¿›ç¨‹ä¼šä¸€ç›´è¿è¡Œï¼Œç›´åˆ°è¢«é”€æ¯ã€‚Actor å¯ä»¥ç»´æŠ¤å†…éƒ¨å˜é‡ï¼Œå¹¶ä¸”è¿™äº›å˜é‡åœ¨ Actor çš„ç”Ÿå‘½å‘¨æœŸå†…æ˜¯æŒä¹…å­˜åœ¨çš„ã€‚æ¯æ¬¡è°ƒç”¨ Actor çš„æ–¹æ³•ï¼Œéƒ½å¯ä»¥è®¿é—®å’Œä¿®æ”¹è¿™äº›çŠ¶æ€ã€‚è¿™ä¸æ™®é€šçš„ Ray Task ä¸åŒï¼Œæ™®é€š Task æ‰§è¡Œå®Œä¼šæ¸…é™¤å†…éƒ¨çŠ¶æ€ã€‚Ray Actor æ”¯æŒå¹¶å‘è¯·æ±‚ï¼ŒRay ä¼šè´Ÿè´£å°†è¿™äº›è¯·æ±‚åºåˆ—åŒ–æ‰§è¡Œï¼Œä¿è¯ Actor å†…éƒ¨çŠ¶æ€çš„ä¸€è‡´æ€§å’Œçº¿ç¨‹å®‰å…¨ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ @ray.remote è£…é¥°å™¨å°†ä¸€ä¸ª Python ç±»è½¬æ¢ä¸ºä¸€ä¸ª Ray Actor ç±»ï¼Œç„¶åé€šè¿‡ .remote() æ–¹æ³•å®ä¾‹åŒ–ä¸€ä¸ªè¿œç¨‹ Actorã€‚\næœ€åï¼ŒRay Worker æ˜¯ Ray é›†ç¾¤ä¸­çœŸæ­£æ‰§è¡Œä»£ç çš„å·¥ä½œå•å…ƒã€‚ä¸€ä¸ª Ray é›†ç¾¤é€šå¸¸ç”±ä¸€ä¸ª Head Node å’Œå¤šä¸ª Worker Nodes ç»„æˆã€‚æ¯ä¸ªèŠ‚ç‚¹ä¸Šéƒ½ä¼šè¿è¡Œä¸€ä¸ªæˆ–å¤šä¸ª Ray Worker è¿›ç¨‹ã€‚æ— è®ºæ˜¯æ™®é€šçš„ Ray Task è¿˜æ˜¯ Ray Actor çš„æ–¹æ³•ï¼Œæœ€ç»ˆéƒ½æ˜¯ç”± Ray Worker è¿›ç¨‹æ¥æ‰§è¡Œçš„ã€‚æ¯ä¸ª Ray Worker éƒ½ä¼šè¢«åˆ†é…ä¸€å®šçš„è®¡ç®—èµ„æºï¼ˆå¦‚ CPUã€GPUï¼‰ã€‚å½“ä½ æäº¤ä¸€ä¸ª Ray Task æˆ–åˆ›å»ºä¸€ä¸ª Ray Actor æ—¶ï¼ŒRay çš„è°ƒåº¦å™¨ä¼šæ‰¾åˆ°ä¸€ä¸ªæœ‰è¶³å¤Ÿèµ„æºçš„ Worker æ¥è¿è¡Œå®ƒã€‚Worker è¿›ç¨‹ä¹‹é—´ä»¥åŠ Worker è¿›ç¨‹ä¸å¤´èŠ‚ç‚¹ä¹‹é—´ä¼šè¿›è¡Œé€šä¿¡ï¼Œä»¥åè°ƒä»»åŠ¡æ‰§è¡Œã€ä¼ è¾“æ•°æ®å’Œç®¡ç†çŠ¶æ€ã€‚ä¸€ä¸ª Ray Worker é€šå¸¸å°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ Python è¿›ç¨‹ã€‚å¯¹äºæ™®é€šçš„ Ray Taskï¼ŒRay Worker ç›¸å½“äºå‡½æ•°è§£é‡Šå™¨ï¼Œæ‰§è¡Œå®Œä»»åŠ¡åå¯èƒ½ä¼šè¢«å¤ç”¨å»æ‰§è¡Œå…¶ä»–ä»»åŠ¡ã€‚è€Œå¯¹äº Ray Actorï¼ŒRay ä¼šå¯åŠ¨ä¸€ä¸ªä¸“é—¨çš„ Worker è¿›ç¨‹æ¥æ‰˜ç®¡è¿™ä¸ª Actorï¼Œè¿™ä¸ª Worker è¿›ç¨‹çš„ç”Ÿå‘½å‘¨æœŸä¸ Actor çš„ç”Ÿå‘½å‘¨æœŸç»‘å®šã€‚\nrun_ppo() å’Œ TaskRunner.run() æœ‰äº† ray çš„æ¦‚å¿µï¼Œæˆ‘ä»¬å›åˆ°æ•´ä¸ª RL è®­ç»ƒæµç¨‹çš„èµ·ç‚¹ï¼šverl.trainer.main_ppo.py ä¸­çš„ run_ppo()ï¼Œå®ƒè´Ÿè´£åˆå§‹åŒ– Ray é›†ç¾¤ï¼Œé…ç½® CPU èµ„æºå’Œè¿è¡Œæ—¶ç¯å¢ƒå˜é‡ï¼Œå¹¶åˆ›å»ºè¿œç¨‹ TaskRunner å®ä¾‹ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 def run_ppo(config) -\u003e None: # åˆå§‹åŒ– Ray é›†ç¾¤ï¼Œé…ç½® CPU èµ„æºå’Œè¿è¡Œæ—¶ç¯å¢ƒå˜é‡ ray.init( runtime_env={\"env_vars\": {...}}, num_cpus=config.ray_init.num_cpus, ) # åˆ›å»ºè¿œç¨‹ TaskRunner å®ä¾‹ # TaskRunner æ˜¯ Ray ä¸­çš„ä¸€ä¸ªè¿œç¨‹ actorï¼Œå®ƒå°†åœ¨ Ray é›†ç¾¤ä¸Šå¼‚æ­¥æ‰§è¡Œä¸»è¦çš„è®­ç»ƒä»»åŠ¡ runner = TaskRunner.remote() # å¼‚æ­¥æ‰§è¡Œè¿œç¨‹ä»»åŠ¡ runner.run()ï¼Œå¹¶ç­‰å¾…å…¶å®Œæˆ # é€šè¿‡ ray.get() é˜»å¡ç›´åˆ°è¿œç¨‹ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œç¡®ä¿æ•´ä¸ªåˆå§‹åŒ–æµç¨‹çš„é¡ºåºæ€§ ray.get(runner.run.remote(config)) ActorRolloutRefWorker å’Œ RayWorkerGroup çš„ç›¸äº’å…³ç³» TaskRunner æ˜¯ verl ä¸­å®ç° PPO/GRPO è®­ç»ƒçš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒé€šè¿‡å°†æ•´ä¸ª RL è®­ç»ƒæµç¨‹å°è£…åœ¨ä¸€ä¸ªç‹¬ç«‹çš„ Ray Actor ä¸­ï¼Œå®ç°äº†ä»»åŠ¡çš„å°è£…ã€èµ„æºéš”ç¦»å’Œåˆ†å¸ƒå¼åè°ƒã€‚ä¸ºäº†è§£é‡Šæ¸…æ¥š TaskRunnerï¼Œæˆ‘ä»¬å°† verl å½“ä¸­æœ€è®©äººè´¹è§£ä¸”æœ€å¤æ‚çš„ ActorRolloutRefWorker å’Œ RayWorkerGroup è¿™ä¸¤ä¸ªç±»æå‰è§£é‡Šæ¸…æ¥šã€‚\næˆ‘ä»¬å…ˆä¸è®¨è®ºè¿™ä¸¤ä¸ªç±»åŠå…¶åŸºç±»çš„å…·ä½“æ„ä¹‰ï¼Œå…ˆè®¨è®ºæ¸…æ¥šå…¶å®ä¾‹å¯¹è±¡çš„åˆ›å»ºè¿‡ç¨‹ã€‚æˆ‘ä»¬æ³¨æ„åˆ°è¿™æ®µ TaskRunner çš„åˆå§‹åŒ–ä¸­å¼•å…¥ ActorRolloutRefWorker å’Œ RayWorkerGroup çš„ç›¸å…³ä»£ç ï¼š\nTaskRunner ä¸­å¼•å…¥ ActorRolloutRefWorker 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # Define worker classes based on the actor strategy. if config.actor_rollout_ref.actor.strategy in [\"fsdp\", \"fsdp2\"]: assert config.critic.strategy in [\"fsdp\", \"fsdp2\"] from verl.single_controller.ray import RayWorkerGroup from verl.workers.fsdp_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == \"async\" else ActorRolloutRefWorker ray_worker_group_cls = RayWorkerGroup elif config.actor_rollout_ref.actor.strategy == \"megatron\": assert config.actor_rollout_ref.actor.strategy == config.critic.strategy from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup from verl.workers.megatron_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == \"async\" else ActorRolloutRefWorker ray_worker_group_cls = NVMegatronRayWorkerGroup else: raise NotImplementedError from verl.trainer.ppo.ray_trainer import ResourcePoolManager, Role # Map roles to their corresponding remote worker classes. role_worker_mapping = { Role.ActorRollout: ray.remote(actor_rollout_cls), Role.Critic: ray.remote(CriticWorker), } # Define the resource pool specification. # Map roles to the resource pool. global_pool_id = \"global_pool\" resource_pool_spec = { global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes, } mapping = { Role.ActorRollout: global_pool_id, Role.Critic: global_pool_id, } å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œåœ¨ TaskRunner çš„åˆå§‹åŒ–ä¸­ï¼Œä¼šæ ¹æ®å„ç±»é…ç½®å¼•å…¥å¯¹åº”çš„ ActorRolloutRefWorker / AsyncActorRolloutRefWorker ç±»ä»¥åŠ RayWorkerGroup / NVMegatronRayWorkerGroup ç±»ã€‚å¯¹äº SGLang è€Œè¨€ï¼Œä¸å­˜åœ¨ AsyncActorRolloutRefWorkerã€‚ActorRolloutRefWorker ç±»ç›´æ¥é€šè¿‡ ray.remote(ActorRolloutRefWorker) åˆ›å»ºä¸€ä¸ªè¿œç¨‹çš„ Ray Actorï¼Œå°†å…¶åŒ…è£…æˆä¸€ä¸ª Ray Actor ç±»ã€‚æ­¤æ—¶è¿˜è¿˜æ²¡æœ‰åˆ›å»ºä»»ä½•å®ä¾‹ï¼Œä¹Ÿæ²¡æœ‰åˆ†é…èµ„æºã€‚é‚£ä¹ˆï¼ŒActorRolloutRefWorker ç±»åˆ°åº•åœ¨å“ªå„¿å®ä¾‹åŒ–å¹¶åˆ†é…èµ„æºçš„å‘¢ï¼Ÿ\nå®é™…ä¸Šï¼Œåœ¨ main_ppo.py çš„ 172 è¡Œï¼Œæ„é€ äº† RayPPOTrainer ç±»ï¼Œéšåè°ƒç”¨äº† RayPPOTrainer.init_workers() æ–¹æ³•ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æŸ¥çœ‹ RayPPOTrainer.init_workers() æ–¹æ³•çš„ç›¸å…³ä»£ç ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæ¯ä¸€ä¸ª RL worker ç±»ï¼ˆæ¯”å¦‚ ActorRolloutRefWorkerï¼‰éƒ½ä¼šåˆ›é€ ä¸€ä¸ª work groupï¼ˆverl ä¸­çš„å„ç§ wg å˜é‡ï¼‰ï¼Œéšåè°ƒç”¨æ¯ä¸ª worker group çš„ init_model() æ–¹æ³•ï¼Œè€Œè¿™äº› worker group å®é™…ä¸Šéƒ½æ˜¯ RayWorkerGroup çš„å®ä¾‹ã€‚RayWorkerGroup çš„æ ¸å¿ƒä½œç”¨æ˜¯èµ„æºè°ƒåº¦çš„æ ¸å¿ƒä¸­é—´å±‚ï¼Œç»Ÿä¸€äº†å„ç§ RL workerï¼ˆæ¯”å¦‚ ActorRolloutRefWorkerã€CriticWorkerï¼‰çš„æ¥å£ï¼Œè¿›è¡Œç»Ÿä¸€ç®¡ç†ï¼š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # RayWorkerGroup å®ä¾‹ï¼ŒæŒ‡å®šèµ„æºæ±  å¹¶è§„å®šè§’è‰²å’Œå¯¹åº”çš„ç±» wg_dict = self.ray_worker_group_cls( resource_pool=resource_pool, # åªéœ€è¦æŒ‡å®šèµ„æºæ±  ray_cls_with_init=worker_dict_cls, # ä¸€ä¸ªåŒ…å«æ•°ä¸ªworkerçš„ç±» ï¼ˆe.g. actor_rollï¼Œ critic, refï¼‰ device_name=self.device_name, ) #é€šè¿‡.spawn()è·å–è§’è‰²å¯¹Ray Actorå®ä¾‹çš„æ˜ å°„ wg_dict.spawn(prefix_set=class_dict.keys()) # æ‰€æœ‰ worker éƒ½é€šè¿‡ç›¸åŒçš„æ¨¡å¼åˆ›å»ºï¼Œæˆ‘è¿™é‡Œè¿›è¡Œç®€åŒ–ï¼Œå®é™…ä¸Šçš„ä»£ç æ¯”è¾ƒç¹ç actor_rollout_wg = RayWorkerGroup(resource_pool, actor_rollout_cls) critic_wg = RayWorkerGroup(resource_pool, critic_cls) ref_policy_wg = RayWorkerGroup(resource_pool, ref_policy_cls) å„ç§ worker group å®é™…ä¸Šçš„åˆå§‹åŒ– è¿™éƒ¨åˆ†ä»£ç åœ¨ ray_trainer.py ä¸­ï¼š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # 1. ä¸ºæ¯ä¸ªè§’è‰²ï¼ˆä¾‹å¦‚ actor_rolloutã€criticã€refï¼‰æŒ‡å®šç”¨å“ªä¸ªç±»åˆå§‹åŒ– workerï¼Œå¹¶ä¸”è¯´æ˜åœ¨å“ªä¸ªèµ„æºæ± é‡Œåˆ†é…å®ƒä»¬ self.resource_pool_manager.create_resource_pool() self.resource_pool_to_cls = {pool: {} for pool in self.resource_pool_manager.resource_pool_dict.values()} resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout) actor_rollout_cls = RayClassWithInitArgs( cls=self.role_worker_mapping[Role.ActorRollout], config=self.config.actor_rollout_ref, role=\"actor_rollout\", ) self.resource_pool_to_cls[resource_pool][\"actor_rollout\"] = actor_rollout_cls # 2. æ ¹æ®èµ„æºæ± å’Œè§’è‰²ï¼Œæ‰¹é‡åˆ›å»ºå¤šä¸ª worker å®ä¾‹ï¼ˆRay Actorï¼‰å¹¶ç»Ÿä¸€ç®¡ç†å®ƒä»¬ï¼Œèµ‹äºˆå¯¹åº”çš„èŒè´£ for resource_pool, class_dict in self.resource_pool_to_cls.items(): worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict) wg_dict = self.ray_worker_group_cls(resource_pool=resource_pool, ray_cls_with_init=worker_dict_cls, device_name=self.device_name, **wg_kwargs) spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys()) all_wg.update(spawn_wg) # 3.è°ƒç”¨ init_model() å®Œæˆæ¨¡å‹åŠ è½½ if self.use_critic: self.critic_wg = all_wg[\"critic\"] self.critic_wg.init_model() if self.use_reference_policy and not self.ref_in_actor: self.ref_policy_wg = all_wg[\"ref\"] self.ref_policy_wg.init_model() if self.use_rm: self.rm_wg = all_wg[\"rm\"] self.rm_wg.init_model() # we should create rollout at the end so that vllm can have a better estimation of kv cache memory self.actor_rollout_wg = all_wg[\"actor_rollout\"] self.actor_rollout_wg.init_model() # create async rollout manager and request scheduler self.async_rollout_mode = False if self.config.actor_rollout_ref.rollout.mode == \"async\": from verl.workers.rollout.async_server import AsyncLLMServerManager self.async_rollout_mode = True self.async_rollout_manager = AsyncLLMServerManager( config=self.config, worker_group=self.actor_rollout_wg, ) æ³¨æ„åˆ° ray_worker_group_cls å°±æ˜¯ RayWorkerGroup ç±»ï¼Œè€Œ worker_dict_cls å°±æ˜¯ ActorRolloutRefWorker ç±»ï¼Œæ‰€ä»¥æˆ‘çš„ç®€åŒ–æ˜¯å¾ˆåˆç†çš„ã€‚\nå¦‚æ­¤ä»¥æ¥ï¼ŒActorRolloutRefWorker å§”æ‰˜ç»™ RayWorkerGroup è¿›è¡Œåˆå§‹åŒ–ã€‚RayWorkerGroup è¿™ä¸ªç±»å°±æ˜¯ä¸“é—¨ç”¨äºèµ„æºè°ƒåº¦çš„ã€‚é€šè¿‡å…¶ç»Ÿä¸€çš„ _init_with_resource_pool æ–¹æ³•ï¼Œä¸ºæ¯ä¸ª GPU åˆ›å»ºä¸€ä¸ª workerï¼Œæœ€ç»ˆå®ä¾‹åŒ–æ¯ç§ RL worker å¹¶åˆ†é…èµ„æºã€‚\n1 2 3 4 5 6 7 8 def _init_with_resource_pool(self, resource_pool, ray_cls_with_init, ...): # ä» Ray ç”³è¯· Placement Groups pgs = resource_pool.get_placement_groups(strategy=strategy, device_name=self.device_name) # ä¸ºæ¯ä¸ª GPU åˆ›å»ºä¸€ä¸ª worker for local_rank in range(local_world_size): worker = ray_cls_with_init(placement_group=pg, placement_group_bundle_idx=local_rank, ...) self._workers.append(worker) è¯»åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬åŸºæœ¬å¯¹ verl æœ‰äº†ä¸€äº›æ„Ÿè§‰ã€‚æ³¨æ„åˆ°ï¼Œåœ¨ verl å½“ä¸­æœ‰ä¸¤ä¸ªå¸¦æœ‰ Worker çš„ base classï¼Œä¸€ä¸ªå°±å«åš Workerï¼Œå¦ä¸€ä¸ªå«åš WorkerGroupã€‚Worker æ˜¯ RL é‡Œé¢çš„é€»è¾‘ç±»ï¼ˆæ¯”å¦‚ actor å’Œ criticï¼‰,å®é™…ç®¡ç† RL çš„æ•°æ®æµï¼Œè€Œ WorkerGroup åªç”¨äºåˆ†å¸ƒå¼ç³»ç»Ÿçš„èµ„æºè°ƒåº¦ã€‚\næ­¤å¤–ï¼Œä» actor_rollout_wg å’Œ ref_policy_wg çš„å®ä¾‹åŒ–å½“ä¸­ï¼Œä¹Ÿèƒ½çœ‹å‡ºä¸€äº›å­¦é—®ã€‚åœ¨ ActorRolloutRefWorker çš„è®¾è®¡å½“ä¸­ï¼ŒActor Trainingï¼ŒActor Rollout å’Œ Reference model æ˜¯ç”¨åŒä¸€ä¸ª worker class è¿›è¡Œç®¡ç†çš„ã€‚ä½†æ˜¯ï¼Œä¹‹åå§”æ‰˜ç»™ RayWorkerGroup åˆ›å»º worker group å¹¶ä¸”è°ƒç”¨èµ„æºçš„æ—¶å€™ï¼ŒActor Training å’Œ Actor Rollout æ˜¯ç”±åŒä¸€ç»„ RayWorkerGroup è¿›è¡Œèµ„æºç®¡ç†çš„ï¼ˆè¿™äºŒè€…æœ¬æ¥å°±è¦è¢«æ”¾åœ¨åŒä¸€ä¸ªèµ„æºç»„ä¸Šåš hybird engineï¼‰ï¼Œè€Œ Reference Model æ˜¯ç”±å¦ä¸€ç»„ RayWorkerGroup ç®¡ç†èµ„æºçš„ã€‚\næœ€åï¼Œæˆ‘å»é—®äº†ç›¸å…³å¼€å‘è€…ï¼Œä»–ä»¬ä¹Ÿè®¤ä¸ºæŠŠ Actor Rolloutï¼ŒActor Training å’Œ Reference Model æ”¾åœ¨åŒä¸€ä¸ª worker é‡Œæ˜¯ bad design ğŸ˜‚ï¼Œä¸ç”¨çº ç»“è¿™ç§è®¾è®¡æ˜¯å¦æœ‰ä»€ä¹ˆé«˜ç»è¿œç©ï¼Œå®Œå…¨æ²¡æœ‰ã€‚\nActorRolloutRefWorker.__init__() å¦‚å‰æ–‡æ‰€è¯´ï¼ŒActorRolloutRefWorker æ˜¯ verl ä¸­ç”¨äºç®¡ç† Actor Trainingï¼ŒActor Rollout å’Œ Reference Model çš„ worker classã€‚æˆ‘ä»¬å…·ä½“æ¥åˆ†æå…¶é€»è¾‘ä¸Šå®ç°çš„åŠŸèƒ½ã€‚æ³¨æ„ï¼Œæœ¬æ–‡æ¡£åªåˆ†æ FSDP backend ä¸‹çš„å®ç°ï¼Œmegatron ç•™ä½œåæ–‡ã€‚\nè°ƒç”¨ Worker åŸºç±»çš„æ„é€ å‡½æ•°ï¼Œå¹¶ä¿å­˜é…ç½®ã€‚ å¦‚æœ PyTorch åˆ†å¸ƒå¼ç¯å¢ƒå°šæœªåˆå§‹åŒ–ï¼Œåˆ™è¿›è¡Œåˆå§‹åŒ–ï¼ŒåŒ…æ‹¬è®¾ç½®é€šä¿¡åç«¯å’Œè¿›ç¨‹ç»„ã€‚ ä¸º FSDP åˆ›å»ºè®¾å¤‡ç½‘æ ¼ï¼Œç”¨äºæ¨¡å‹å‚æ•°çš„åˆ†ç‰‡ã€‚ å¦‚æœå¯ç”¨ Ulysses åºåˆ—å¹¶è¡Œï¼Œåˆ™åˆå§‹åŒ–å…¶è®¾å¤‡ç½‘æ ¼ã€‚ æ ¹æ®ä¼ å…¥çš„ role å‚æ•°è®¾ç½® Worker çš„å…·ä½“è§’è‰²ï¼ˆactor, rollout, refï¼‰ã€‚ æ ¹æ® Worker è§’è‰²é…ç½® profilerï¼Œç”¨äºæ€§èƒ½åˆ†æã€‚ é…ç½® parameter offload å’Œ optimizer offloadã€‚ ä¸º Actorï¼ŒRollout å’Œ Reference åˆ†åˆ« normalize batch sizeã€‚ ç¬¬ 8 æ­¥ä¸­é…ç½®äº†éå¸¸å¤šçš„ batch sizeï¼›verl çš„ batch size å‚æ•°æ»¡å¤©é£ï¼Œè™½ç„¶æˆ‘ä¸ªäººè®¤ä¸ºåå­—åŸºæœ¬æ˜¯å‡†ç¡®çš„ï¼Œä½†æ˜¯ç”±äºåå­—å¤ªåƒäº†ï¼Œä¸€å®šè¦åšå‡ºä¸€äº›åŒºåˆ†ã€‚äº‹å®ä¸Šï¼Œå‚æ•°åˆ†ææˆ‘ä»¬æœ‰å•ç‹¬çš„æ–‡æ¡£ï¼Œæˆ‘å…ˆæŠŠä¸€éƒ¨åˆ†å†…å®¹æå‰å…¬å¸ƒäº†ã€‚\ndata.train_batch_sizeï¼šåœ¨ä¸€æ¬¡å®Œæ•´çš„ PPO è¿­ä»£ï¼ˆä» rollout åˆ° trainï¼‰ä¸­ï¼Œä»æ•°æ®é›†ä¸­é‡‡æ ·å¹¶ç”¨äºç”Ÿæˆ experience çš„æ€»æ ·æœ¬æ•°é‡ï¼Œå†³å®šäº†æ¯æ¬¡ policy æ›´æ–°æ‰€ä¾æ®çš„æ•°æ®é‡ã€‚ actor_rollout_ref.actor.ppo_mini_batch_sizeï¼šè¿™ä¸ªå‚æ•°çš„åå­—å…¶å®æ˜¯å‡†ç¡®çš„ï¼Œå› ä¸º mini batch SGD å°±æ˜¯æ•°æ®åˆ°è¾¾äº†ä¸€ä¸ª mini batch å°±æ›´æ–°ä¸€æ¬¡æ¨¡å‹å‚æ•°ã€‚åœ¨ verl ä¸­ï¼Œæ¨¡å‹ä¼šåœ¨æ•°æ®ç´¯ç§¯åˆ°ä¸€ä¸ª mini batch åæ›´æ–°ä¸€æ¬¡å‚æ•°ã€‚ actor_rollout_ref.actor.ppo_micro_batch_size_per_gpuï¼šè¿™é‡Œå…¶å®æ˜¯ gradient accumulation çš„å‚æ•°ã€‚ç”±äºä¸€ä¸ª mini batch çš„æ•°æ®é‡å¯èƒ½ä»ç„¶å¤ªå¤§ï¼Œæ— æ³•ä¸€æ¬¡æ€§å‰å‘å’Œåå‘ä¼ æ’­ï¼Œå› æ­¤éœ€è¦å°†å…¶è¿›ä¸€æ­¥æ‹†åˆ†ä¸º micro batchã€‚æ¯ä¸ª micro batch ä¼šè®¡ç®—ä¸€æ¬¡æ¢¯åº¦å¹¶ä¸”ç´¯è®¡ï¼Œä½†æ˜¯ä¸ä¼šç«‹åˆ»æ›´æ–°æ¨¡å‹å‚æ•°ã€‚å¤„ç†å®Œæ•´ä¸ª mini batch åï¼Œæ‰ç”¨ç´¯ç§¯çš„æ¢¯åº¦è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°ã€‚ æ­¤å¤–ï¼Œåœ¨ verl ä¸­ï¼Œç”±äº verl å¼ºè°ƒ SPMD ç­–ç•¥ï¼Œå¯ä»¥ç†è§£ä¸ºæ¯ä¸ª RL worker æ‰€å æ®çš„æ¯ä¸ª GPU ä¸Šå¸Œæœ›è¿›è¡Œå®Œå…¨ä¸€è‡´çš„æ“ä½œï¼Œæ‰€ä»¥ verl ä¼šè¦æ±‚æ¯ä¸ª GPU çš„ micro batch size ç›¸åŒã€‚å› æ­¤ï¼Œverl ä¼šæ£€æŸ¥ train batch size / gpu æ˜¯å¦æ•´é™¤ (ref)ï¼Œå¦‚æœä¸æ•´é™¤ï¼Œåˆ™æŠ¥é”™ã€‚è¿™ä¸ªè®¾å®šå…¶å®å®Œå…¨æ²¡å¿…è¦ï¼›å¯¹äº rollout è€Œè¨€ï¼ŒSGLang å®Œå…¨ä¸éœ€è¦å‘é€çš„è¯·æ±‚æ•°é‡æ•´é™¤ DP æˆ–è€… TP sizeï¼Œæ›´ä½•å†µç›´æ¥è¦æ•´é™¤ gpu æ•°é‡å‘¢ï¼Ÿä½†æ˜¯ï¼Œå› ä¸º verl ä¼šç”¨ all gather ä» rollout çš„æ¯ä¸ª worker é‡Œæ”¶é›†æ•°æ®ï¼Œè¿™å°±è¦æ±‚ rollout çš„æ¯ä¸ª worker ä¸Šåˆ†åˆ°çš„æ•°æ®ä¸€è‡´ã€‚æ›´è¿›ä¸€æ­¥ï¼Œä¸ºäº† SPMDï¼Œåˆè¦æ±‚ rollout çš„æ¯ä¸ª gpu ä¸Šåˆ†åˆ°çš„æ•°æ®ä¸€è‡´ã€‚æœ€ç»ˆï¼Œè¿™å°±å¯¼è‡´ verl çš„ train batch size å¿…é¡»æ•´é™¤ gpu æ•°é‡ï¼›åœ¨ GRPO ä¸‹æ˜¯ real train batch size éœ€è¦æ•´é™¤ n gpusï¼Œç­‰äº train batch size * sampling params ä¸­çš„ nã€‚\nåŒºåˆ†å¥½ mini batch å’Œ micro batch åï¼Œæˆ‘ä¹Ÿæ˜¯æœ€è¿‘æ‰æ˜ç™½ PPO ä¸­æ˜¯å¦‚ä½•ç»´æŠ¤ on policy çš„ã€‚æˆ‘ä¹‹å‰ä¸€ç›´ä»¥ä¸ºæˆ‘ä»¬éƒ½æ˜¯åœ¨åšä¸¥æ ¼ on policy çš„è®­ç»ƒï¼Œä½†æ˜¯ä¸€ä¸ª train batch size ä¸‹æœ‰å¥½å‡ ä¸ª mini batchï¼Œä¼¼ä¹ç¬¬ä¸€ä¸ª mini batch ç»“æŸä¹‹åï¼Œç›®æ ‡ç­–ç•¥ï¼ˆtarget policyï¼Œè¢«è®­ç»ƒçš„ policyï¼‰å’Œè¡Œä¸ºç­–ç•¥ï¼ˆbehavior policyï¼Œç”¨äºåœ¨ç¯å¢ƒä¸­é‡‡æ ·çš„ policyï¼‰å°±ä¸ä¸€è‡´äº†ã€‚ä¸€æ¬¡é‡‡æ ·ä¼šè®­ç»ƒå¾ˆå¤šä¸ª mini batchï¼Œä»ç¬¬ä¸€ä¸ª mini batch ç»“æŸå°±ä¸æ˜¯ on policy äº†ã€‚äº‹å®ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œæˆ‘ä»¬æ³¨æ„åˆ° PPO çš„ loss functionï¼š\n$$ L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min(r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t) \\right] $$\nå…¶ä¸­çš„ $r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)}$ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹ä¼˜åŠ¿å‡½æ•°çš„çŸ«æ­£æ¯”ä¾‹ï¼Œè€Œ $\\hat{A}t$ å°±æ˜¯ advantageã€‚å¯¹äº LLM çš„ PPO è€Œè¨€ï¼Œ$\\pi{\\theta_{old}}(a_t | s_t)$ ä»£è¡¨ç€é‡‡æ ·æ—¶ behavior policy åœ¨ç»™å®š $s_t$ æ—¶ï¼Œé€‰æ‹© $a_t$ çš„æ¦‚ç‡ï¼Œè€Œ $\\pi_\\theta(a_t | s_t)$ å°±æ˜¯ target policy åœ¨è®­ç»ƒä¸­çš„æ¯ä¸€æ­¥ç»™å®š $s_t$ æ—¶ï¼Œé€‰æ‹© $a_t$ çš„æ¦‚ç‡ã€‚å¯¹ LLM è€Œè¨€ï¼Œs_t æ˜¯ prompt å‰ç¼€ï¼Œè€Œ a_t ä»…ä»…æ˜¯ prompt åçš„é‚£ä¸€ä¸ª tokenã€‚è¿™ä¸€æ¦‚ç‡å…¶å®å°±æ˜¯ inference å¾—åˆ°çš„ log probsï¼›æˆ‘ä»¬å°†æ”¶é›†å¾—åˆ°çš„ (prompt, action) åˆ†åˆ«ç»è¿‡ target policy å’Œ behaviour policy å¾—åˆ° log probsï¼Œç„¶åäºŒè€… log probs ç›¸å‡å†å–å¯¹æ•°ï¼Œå°±æ˜¯çŸ«æ­£é¡¹çš„å€¼ã€‚ä»è€Œï¼Œå³ä¾¿ç¬¬ä¸€ä¸ª mini batch ä¹‹å target policy å°±å·²ç»å’Œ behaviour policy ä¸ä¸€è‡´äº†ï¼Œä»ç„¶å¯ä»¥é€šè¿‡ log probs è¿›è¡ŒçŸ«æ­£ï¼Œä¹Ÿå³ importance samplingã€‚\nè¿™æ ·ä¸€æ¥ï¼Œåˆæœ‰äº†ä¸¤ä¸ªé—®é¢˜ï¼šlog probs åº”è¯¥å¦‚ä½•å¾—åˆ°ï¼Ÿå®é™…ä¸Šæ¯æ¬¡é‡‡æ ·æ—¶éƒ½æ˜¯å‘é€ç»™ rollout å›ºå®šæ•°é‡çš„ requestsï¼Œå¦‚æœæ¯ä¸ª (prompt, action) å¯¹éƒ½ä¼šè®¡ç®—ä¸€æ¬¡ loss çš„è¯ï¼Œå²‚ä¸æ˜¯æ›´é•¿çš„ sequence ä¼šè®¡ç®—æ›´å¤šæ¬¡ï¼Ÿ\nå¯¹äºç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œè¿™åˆæ˜¯ç»å…¸çš„ç²¾åº¦é—®é¢˜ã€‚å¦‚åŒæˆ‘åœ¨é“¾æ¥åˆ°çš„æ–‡ç« ä¸­æ‰€è¯´çš„ï¼Œrollout engine ç›®å‰åªæœ‰é‡‡æ ·å¾—åˆ°çš„ token èƒ½ç”¨ï¼Œè€Œå¾—åˆ°çš„ log probs ä»¥åŠ reward ç²¾åº¦éƒ½ä¸å¤Ÿï¼Œä¸èƒ½ç”¨äºè®­ç»ƒã€‚behaviour policy å’Œ target policy ä¸ºäº†åš importance sampling æ‰€éœ€çš„ log probs éƒ½å¾—ç”¨ training engine é‡ç®—ã€‚ä¸è¿‡è¦ç®—èµ·æ¥ä¹Ÿä¸éº»çƒ¦ï¼Œåœ¨ç¬¬ä¸€ä¸ª mini batch å¯åŠ¨å‰ï¼Œè¿™æ—¶å€™ target behaviour æ˜¯ä¸€è‡´çš„ï¼Œé‡ç®— log probs å¹¶ä¸”å­˜ä¸‹æ¥å³å¯ã€‚\nå¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œçš„ç¡®å¦‚æ­¤ã€‚ä¸€æ¡å¾ˆé•¿çš„ prompt + answer åºåˆ—ç¡®å®ä¼šäº§ç”Ÿéå¸¸å¤šçš„ (prompt, action) å¯¹ï¼Œå…¶ä¸­æ¯ä¸ªå¯¹éƒ½å¯ä»¥çœ‹ä½œä¸€ä¸ª (state, action) å¯¹ã€‚è€Œä¸”ç†è®ºä¸Šæ¯ä¸ªè¿™æ ·çš„ (prompt, action) å¯¹éƒ½ä¼šå‚ä¸ Loss çš„è®¡ç®—ã€‚è¿™ç¡®å®å¯èƒ½å¯¼è‡´é•¿åºåˆ—ä¸­çš„ token ä¼šåœ¨ Loss è®¡ç®—ä¸­å æ®æ›´å¤§çš„æ¯”ä¾‹ï¼Œè®©æ¨¡å‹è¿‡åº¦å…³æ³¨é•¿åºåˆ—çš„ä¼˜åŒ–ï¼Œè€Œå¯¹çŸ­åºåˆ—çš„ä¼˜åŒ–ä¸è¶³ã€‚ä¸è¿‡ï¼Œverl çš„ rollout engine ä¼šè‡ªåŠ¨å¯¹æ¯ä¸ª (prompt, action) å¯¹è¿›è¡ŒåŠ æƒï¼Œä»è€Œè®©é•¿åºåˆ—å’ŒçŸ­åºåˆ—çš„ token åœ¨ Loss è®¡ç®—ä¸­å æ®ç›¸åŒçš„æƒé‡ã€‚ä¸ºäº†ç¼“è§£è¿™ç§æƒ…å†µï¼Œæœ‰å¾ˆå¤šç›¸å…³æ–¹æ³•ï¼š\næ ·æœ¬åŠ æƒæ–¹æ³• åºåˆ—çº§åˆ«åŠ æƒï¼š ä¸€ç§ç›´æ¥çš„æ–¹æ³•æ˜¯åœ¨è®¡ç®— Loss æ—¶ï¼Œç»™æ¥è‡ªä¸åŒåºåˆ—çš„æ ·æœ¬èµ‹äºˆä¸åŒçš„æƒé‡ã€‚ä¾‹å¦‚ï¼Œç»™æ¯ä¸ªå®Œæ•´åºåˆ—ä¸€ä¸ªå›ºå®šçš„æƒé‡ï¼ˆæ¯”å¦‚ 1ï¼‰ï¼Œç„¶åå°†è¿™ä¸ªæƒé‡å‡åŒ€åˆ†é…ç»™è¯¥åºåˆ—ä¸­çš„æ¯ä¸ª (prompt, action) å¯¹ã€‚è¿™æ ·ï¼Œæ— è®ºåºåˆ—å¤šé•¿ï¼Œå®ƒå¯¹æ€» Loss çš„è´¡çŒ®éƒ½ç›¸åŒã€‚å¦‚æœä¸€ä¸ªåºåˆ—æœ‰ N ä¸ª tokenï¼Œé‚£ä¹ˆæ¯ä¸ª (prompt, action) å¯¹çš„æƒé‡å°±æ˜¯ 1/Nã€‚\næŒ‰é•¿åº¦åˆ†æ¡¶ï¼š åœ¨æ•°æ®æ”¶é›†åï¼Œå¯ä»¥æ ¹æ®åºåˆ—é•¿åº¦å¯¹æ ·æœ¬è¿›è¡Œæ’åºï¼Œå¹¶å°è¯•å°†ç›¸ä¼¼é•¿åº¦çš„åºåˆ—æ”¾å…¥åŒä¸€ä¸ª mini-batchã€‚è¿™æœ‰åŠ©äºæé«˜è®¡ç®—æ•ˆç‡ï¼Œå› ä¸ºå¯ä»¥å‡å°‘ paddingï¼Œä½†å¯¹äºè§£å†³ Loss è´¡çŒ®ä¸å‡è¡¡çš„ä½œç”¨æœ‰é™ã€‚\nå›ºå®š Token æ•°é‡çš„æ‰¹æ¬¡ï¼š æœ€å¸¸è§ä¸”æœ‰æ•ˆçš„æ–¹æ³•æ˜¯æ„å»ºæ‰¹æ¬¡æ—¶ï¼Œä¸å›ºå®šæ ·æœ¬æ•°é‡ï¼Œè€Œæ˜¯å›ºå®šæ‰¹æ¬¡ä¸­çš„æ€» token æ•°é‡ã€‚è¿™æ ·ï¼Œä¸€ä¸ª mini-batch å¯èƒ½åŒ…å« 4 æ¡é•¿åºåˆ—ï¼Œä¹Ÿå¯èƒ½åŒ…å« 40 æ¡çŸ­åºåˆ—ï¼Œç¡®ä¿æ¯æ¬¡æ›´æ–°æ—¶å¤„ç†çš„æ€»è®¡ç®—é‡å’Œæ¢¯åº¦æ¥æºçš„æ€» token æ•°æ˜¯æ’å®šçš„ï¼Œä»è€Œç¼“è§£é•¿çŸ­åºåˆ—çš„ä¸å‡è¡¡é—®é¢˜ã€‚\nLoss å½’ä¸€åŒ–ï¼šåœ¨è®¡ç®—æ¯ä¸ª mini-batch çš„ Loss æ—¶ï¼Œå¯ä»¥å°†å…¶é™¤ä»¥è¯¥ mini-batch ä¸­å®é™…çš„ token æ•°é‡ã€‚è¿™ç¡®ä¿äº† Loss å€¼ä¸ä¼šä»…ä»…å› ä¸ºæ‰¹æ¬¡ä¸­åŒ…å«äº†æ›´å¤š token è€Œå¢å¤§ï¼Œä»è€Œä¸ºä¸åŒå¤§å°çš„ mini-batchesï¼ˆå¦‚æœä¸æ˜¯æŒ‰å›ºå®š token æ•°æ„å»ºï¼‰æä¾›ä¸€ä¸ªå…¬å¹³çš„æ¯”è¾ƒåŸºç¡€ã€‚\næˆªæ–­ï¼šè®¾å®šä¸€ä¸ª max_length å‚æ•°ï¼Œé™åˆ¶æ¨¡å‹ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡ã€‚è™½ç„¶è¿™ä¸ç›´æ¥è§£å†³å·²æœ‰é•¿åºåˆ—çš„æƒé‡é—®é¢˜ï¼Œä½†å¯ä»¥é˜²æ­¢ç”Ÿæˆè¿‡é•¿çš„åºåˆ—ï¼Œä»è€Œé™åˆ¶æç«¯ä¸å‡è¡¡çš„å‘ç”Ÿã€‚\nwhateverï¼Œè§£é‡Šäº†è¿™ä¹ˆå¤šï¼Œé¡ºç€ç†è§£ verl çš„æ¡†æ¶è¿›ä¸€æ­¥å­¦ä¹ äº† RL ç®—æ³•å’Œç³»ç»Ÿï¼Œè¿™é‡Œå…¶å®å’Œ multi-turn éƒ½è¿˜æ²¡æœ‰å…³ç³»ï¼Œæˆ‘ä»¬è¿˜æ˜¯å›åˆ° ActorRolloutRefWorker çš„æºç ä¸Šã€‚\nActorRolloutRefWorker.__init__ æºç  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__(self, config: DictConfig, role: str): # åˆå§‹åŒ– Worker åŸºç±» Worker.__init__(self) # å­˜å‚¨é…ç½®ä¿¡æ¯ self.config = config import torch.distributed # å¦‚æœåˆ†å¸ƒå¼ç¯å¢ƒå°šæœªåˆå§‹åŒ–ï¼Œåˆ™è¿›è¡Œåˆå§‹åŒ– if not torch.distributed.is_initialized(): rank = int(os.environ.get(\"RANK\", 0)) world_size = int(os.environ.get(\"WORLD_SIZE\", 1)) torch.distributed.init_process_group(backend=f\"cpu:gloo,{get_device_name()}:{get_nccl_backend()}\", rank=rank, world_size=world_size) # ä¸º FSDP æ„å»ºè®¾å¤‡ç½‘æ ¼ world_size = torch.distributed.get_world_size() self.device_mesh = create_device_mesh(world_size=world_size, fsdp_size=self.config.actor.fsdp_config.fsdp_size) # ä¸º Ulysses åºåˆ—å¹¶è¡Œæ„å»ºè®¾å¤‡ç½‘æ ¼ self.ulysses_device_mesh = None self.ulysses_sequence_parallel_size = self.config.actor.get(\"ulysses_sequence_parallel_size\", 1) dp = world_size // self.ulysses_sequence_parallel_size if self.ulysses_sequence_parallel_size \u003e 1: self.ulysses_device_mesh = init_device_mesh(device_name, mesh_shape=(dp, self.ulysses_sequence_parallel_size), mesh_dim_names=[\"dp\", \"sp\"]) # åˆå§‹åŒ– Ulysses åˆ†ç‰‡ç®¡ç†å™¨ self.ulysses_sharding_manager = FSDPUlyssesShardingManager(self.ulysses_device_mesh) # è·å– LoRA rank å’Œæ˜¯å¦ä½¿ç”¨ LoRA çš„æ ‡å¿— self._lora_rank = self.config.model.get(\"lora_rank\", 0) self._is_lora = self._lora_rank \u003e 0 # è®¾ç½® Worker è§’è‰²å’Œç›¸å…³æ ‡å¿— self.role = role assert self.role in [\"actor\", \"rollout\", \"ref\", \"actor_rollout\", \"actor_rollout_ref\"] self._is_actor = self.role in [\"actor\", \"actor_rollout\", \"actor_rollout_ref\"] self._is_rollout = self.role in [\"rollout\", \"actor_rollout\", \"actor_rollout_ref\"] self._is_ref = self.role in [\"ref\", \"actor_rollout_ref\"] profiler_config: Optional[ProfilerConfig] = None # æ ¹æ®è§’è‰²è·å–æ€§èƒ½åˆ†æé…ç½® if self._is_actor: profiler_config = omega_conf_to_dataclass(config.actor.get(\"profiler\", {}), ProfilerConfig) if self._is_rollout: profiler_config = omega_conf_to_dataclass(config.rollout.get(\"profiler\", {}), ProfilerConfig) if self._is_ref: profiler_config = omega_conf_to_dataclass(config.ref.get(\"profiler\", {}), ProfilerConfig) # åˆå§‹åŒ–åˆ†å¸ƒå¼æ€§èƒ½åˆ†æå™¨ DistProfilerExtension.__init__(self, DistProfiler(rank=self.rank, config=profiler_config)) # è®¾ç½®å‚æ•°å’Œä¼˜åŒ–å™¨å¸è½½æ ‡å¿— self._is_offload_param = False self._is_offload_optimizer = False if self._is_actor: self._is_offload_param = self.config.actor.fsdp_config.get(\"param_offload\", False) self._is_offload_optimizer = self.config.actor.fsdp_config.get(\"optimizer_offload\", False) elif self._is_ref: self._is_offload_param = self.config.ref.fsdp_config.get(\"param_offload\", False) # è§„èŒƒåŒ– actor ç›¸å…³é…ç½® if self._is_actor: self.config.actor.ppo_mini_batch_size *= self.config.rollout.n self.config.actor.ppo_mini_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size assert self.config.actor.ppo_mini_batch_size \u003e 0, f\"ppo_mini_batch_size {self.config.actor.ppo_mini_batch_size} should be larger than 0 after normalization\" # micro bsz if self.config.actor.ppo_micro_batch_size is not None: self.config.actor.ppo_micro_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size self.config.actor.ppo_micro_batch_size_per_gpu = self.config.actor.ppo_micro_batch_size if self.config.actor.ppo_micro_batch_size_per_gpu is not None: assert self.config.actor.ppo_mini_batch_size % self.config.actor.ppo_micro_batch_size_per_gpu == 0, f\"normalized ppo_mini_batch_size {self.config.actor.ppo_mini_batch_size} should be divisible by ppo_micro_batch_size_per_gpu {self.config.actor.ppo_micro_batch_size_per_gpu}\" assert self.config.actor.ppo_mini_batch_size // self.config.actor.ppo_micro_batch_size_per_gpu \u003e 0, f\"normalized ppo_mini_batch_size {self.config.actor.ppo_mini_batch_size} should be larger than ppo_micro_batch_size_per_gpu {self.config.actor.ppo_micro_batch_size_per_gpu}\" # è§„èŒƒåŒ– rollout ç›¸å…³é…ç½® if self._is_rollout and self.config.rollout.log_prob_micro_batch_size is not None: self.config.rollout.log_prob_micro_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size self.config.rollout.log_prob_micro_batch_size_per_gpu = self.config.rollout.log_prob_micro_batch_size # è§„èŒƒåŒ– ref ç›¸å…³é…ç½® if self._is_ref and self.config.ref.log_prob_micro_batch_size is not None: self.config.ref.log_prob_micro_batch_size //= self.device_mesh.size() // self.ulysses_sequence_parallel_size self.config.ref.log_prob_micro_batch_size_per_gpu = self.config.ref.log_prob_micro_batch_size ActorRolloutRefWorker._build_model_optimizer() è¿™éƒ¨åˆ†æºç å’Œç±»å†™çš„è¿˜æ˜¯å¾ˆç›´ç™½çš„ï¼Œä¸ç”¨å¤ªå¤šè§£é‡Šï¼š\nåˆå§‹åŒ– Hugging Face é…ç½®ï¼Œè·å– Generation Configï¼Œå¹¶è®¾ç½®æ¨¡å‹çš„æ•°æ®ç±»å‹ï¼ˆActor ä½¿ç”¨ fp32ï¼ŒReference ä½¿ç”¨ bf16ï¼‰ã€‚ ä½¿ç”¨ Hugging Face çš„ AutoModelForCausalLM æˆ– AutoModelForVision2Seq ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½åŸºç¡€æ¨¡å‹ã€‚ åº”ç”¨å„ç§ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬ Liger kernelã€èåˆ kernelã€æ¢¯åº¦æ£€æŸ¥ç‚¹ã€LoRA ç­‰ã€‚ æ ¹æ®é…ç½®é€‰æ‹© FSDP æˆ– FSDP2 ç­–ç•¥ï¼Œå°†æ¨¡å‹å°è£…åˆ°åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ä¸­ï¼Œæ”¯æŒå‚æ•°åˆ†ç‰‡å’Œæ··åˆç²¾åº¦è®­ç»ƒã€‚ å¦‚æœå½“å‰ Worker æ˜¯ Actor è§’è‰²ï¼Œåˆ™åˆå§‹åŒ– AdamW ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚ ActorRolloutRefWorker._build_model_optimizer æºç  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def _build_model_optimizer( self, model_path, fsdp_config, optim_config, override_model_config, use_remove_padding=False, use_fused_kernels=False, enable_gradient_checkpointing=False, trust_remote_code=False, use_liger=False, role=\"actor\", enable_activation_offload=False, ): from torch import optim from torch.distributed.fsdp import CPUOffload, MixedPrecision from transformers import AutoConfig, AutoModelForCausalLM, AutoModelForVision2Seq from verl.utils.model import get_generation_config, print_model_size, update_model_config from verl.utils.torch_dtypes import PrecisionType assert role in [\"actor\", \"ref\"] log_gpu_memory_usage(f\"Before init {role} from HF AutoModel\", logger=logger) local_path = model_path self.tokenizer = hf_tokenizer(local_path, trust_remote_code=trust_remote_code) self.processor = hf_processor(local_path, trust_remote_code=trust_remote_code) torch_dtype = fsdp_config.get(\"model_dtype\", None) if torch_dtype is None: torch_dtype = torch.float32 if self._is_actor else torch.bfloat16 else: torch_dtype = PrecisionType.to_dtype(torch_dtype) actor_model_config = AutoConfig.from_pretrained(local_path, trust_remote_code=trust_remote_code, attn_implementation=\"flash_attention_2\") if getattr(actor_model_config, \"model_type\", None) == \"kimi_vl\": actor_model_config.text_config.topk_method = \"greedy\" self.generation_config = get_generation_config(local_path, trust_remote_code=trust_remote_code) override_config_kwargs = { \"bos_token_id\": self.tokenizer.bos_token_id, \"eos_token_id\": self.tokenizer.eos_token_id, \"pad_token_id\": self.tokenizer.pad_token_id, } override_config_kwargs.update(override_model_config) update_model_config(actor_model_config, override_config_kwargs=override_config_kwargs) # å¦‚æœæ˜¯ rank 0 è¿›ç¨‹ï¼Œæ‰“å°æ›´æ–°åçš„æ¨¡å‹é…ç½® if self.rank == 0: print(f\"Model config after override: {actor_model_config}\") init_context = get_init_weight_context_manager(use_meta_tensor=not actor_model_config.tie_word_embeddings, mesh=self.device_mesh) with init_context(), warnings.catch_warnings(): warnings.simplefilter(\"ignore\") if type(actor_model_config) in AutoModelForVision2Seq._model_mapping.keys(): actor_module_class = AutoModelForVision2Seq else: actor_module_class = AutoModelForCausalLM actor_module = actor_module_class.from_pretrained( pretrained_model_name_or_path=local_path, torch_dtype=torch_dtype, config=actor_model_config, trust_remote_code=trust_remote_code, ) if use_liger: from liger_kernel.transformers.monkey_patch import _apply_liger_kernel_to_instance _apply_liger_kernel_to_instance(model=actor_module) fused_kernel_options = self.config.model.get(\"fused_kernel_options\", None) fused_kernels_backend = fused_kernel_options.get(\"impl_backend\", None) if fused_kernel_options is not None else None apply_monkey_patch( model=actor_module, use_remove_padding=use_remove_padding, ulysses_sp_size=self.ulysses_sequence_parallel_size, use_fused_kernels=use_fused_kernels, fused_kernels_backend=fused_kernels_backend, ) actor_module.to(torch_dtype) if enable_gradient_checkpointing: actor_module.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False}) if self._is_lora: print(\"Applying LoRA to actor module\") actor_module.enable_input_require_grads() lora_config = {\"task_type\": TaskType.CAUSAL_LM, \"r\": self.config.model.lora_rank, \"lora_alpha\": self.config.model.lora_alpha, \"target_modules\": convert_to_regular_types(self.config.model.target_modules), \"bias\": \"none\"} actor_module = get_peft_model(actor_module, LoraConfig(**lora_config)) torch.distributed.barrier() if self.rank == 0: print_model_size(actor_module) log_gpu_memory_usage(f\"After init {role} from HF AutoModel\", logger=logger) mixed_precision_config = fsdp_config.get(\"mixed_precision\", None) if mixed_precision_config is not None: param_dtype = PrecisionType.to_dtype(mixed_precision_config.get(\"param_dtype\", \"bf16\")) reduce_dtype = PrecisionType.to_dtype(mixed_precision_config.get(\"reduce_dtype\", \"fp32\")) buffer_dtype = PrecisionType.to_dtype(mixed_precision_config.get(\"buffer_dtype\", \"fp32\")) else: param_dtype = torch.bfloat16 reduce_dtype = torch.float32 buffer_dtype = torch.float32 mixed_precision = MixedPrecision(param_dtype=param_dtype, reduce_dtype=reduce_dtype, buffer_dtype=buffer_dtype) auto_wrap_policy = get_fsdp_wrap_policy(module=actor_module, config=fsdp_config.get(\"wrap_policy\", None), is_lora=self.config.model.get(\"lora_rank\", 0) \u003e 0) # TODO(zhangchi.usc1992, shengguangming) fix me. Current, auto_wrap_policy causes HFRollout to hang in Gemma if self._is_rollout and self.config.rollout.name == \"hf\": auto_wrap_policy = None # å¦‚æœæ˜¯ rank 0 è¿›ç¨‹ï¼Œæ‰“å°åŒ…è£…ç­–ç•¥ if self.rank == 0: print(f\"wrap_policy: {auto_wrap_policy}\") fsdp_mesh = self.device_mesh sharding_strategy = get_sharding_strategy(fsdp_mesh) # TODO: æ·»åŠ  transformer ç­–ç•¥ # æˆ‘ä»¬å¼ºåˆ¶ reference policy ä½¿ç”¨ CPUOffload æ¥èŠ‚çœå†…å­˜ # æˆ‘ä»¬å¼ºåˆ¶å…³é—­ actor çš„ CPUOffloadï¼Œå› ä¸ºå®ƒåœ¨ä½¿ç”¨ grad accumulation æ—¶ä¼šå¯¼è‡´ä¸æ­£ç¡®çš„ç»“æœ cpu_offload = None if role == \"actor\" else CPUOffload(offload_params=True) # æ ¹æ®é…ç½®çš„ç­–ç•¥ï¼Œå°†æ¨¡å‹å°è£…åˆ° FSDP ä¸­ fsdp_strategy = self.config.actor.strategy if fsdp_strategy == \"fsdp\": actor_module_fsdp = FSDP( actor_module, cpu_offload=cpu_offload, param_init_fn=init_fn, use_orig_params=False, auto_wrap_policy=auto_wrap_policy, device_id=get_device_id(), sharding_strategy=sharding_strategy, # zero3 mixed_precision=mixed_precision, sync_module_states=True, device_mesh=self.device_mesh, forward_prefetch=self.config.actor.fsdp_config.forward_prefetch, ) elif fsdp_strategy == \"fsdp2\": assert CPUOffloadPolicy is not None, \"PyTorch version \u003e= 2.4 is required for using fully_shard API (FSDP2)\" mp_policy = MixedPrecisionPolicy(param_dtype=param_dtype, reduce_dtype=reduce_dtype, cast_forward_inputs=True) if role == \"actor\" and fsdp_config.offload_policy: cpu_offload = CPUOffloadPolicy(pin_memory=True) self._is_offload_param = False self._is_offload_optimizer = False else: cpu_offload = None if role == \"actor\" else CPUOffloadPolicy(pin_memory=True) fsdp_kwargs = { \"mesh\": fsdp_mesh, \"mp_policy\": mp_policy, \"offload_policy\": cpu_offload, \"reshard_after_forward\": fsdp_config.reshard_after_forward, } full_state = actor_module.state_dict() apply_fsdp2(actor_module, fsdp_kwargs, fsdp_config) fsdp2_load_full_state_dict(actor_module, full_state, fsdp_mesh, cpu_offload) actor_module_fsdp = actor_module else: raise NotImplementedError(f\"not implement {fsdp_strategy}\") # å¦‚æœå¯ç”¨äº†æ¿€æ´»å¸è½½ï¼Œåˆ™å¯ç”¨å®ƒ if enable_activation_offload: enable_activation_offloading(actor_module_fsdp, fsdp_strategy, enable_gradient_checkpointing) # è®°å½• FSDP åˆå§‹åŒ–ä¹‹åçš„ GPU å†…å­˜ä½¿ç”¨æƒ…å†µ log_gpu_memory_usage(f\"After {role} FSDP init\", logger=logger) # TODO: add more optimizer args into config if role == \"actor\" and optim_config is not None: from verl.utils.torch_functional import get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup actor_optimizer = optim.AdamW( actor_module_fsdp.parameters(), lr=optim_config.lr, betas=optim_config.get(\"betas\", (0.9, 0.999)), weight_decay=optim_config.get(\"weight_decay\", 1e-2), ) total_steps = optim_config.get(\"total_training_steps\", 0) num_warmup_steps = int(optim_config.get(\"lr_warmup_steps\", -1)) warmup_style = optim_config.get(\"warmup_style\", \"constant\") min_lr_ratio = optim_config.get(\"min_lr_ratio\", 0.0) num_cycles = optim_config.get(\"num_cycles\", 0.5) if num_warmup_steps \u003c 0: num_warmup_steps_ratio = optim_config.get(\"lr_warmup_steps_ratio\", 0.0) num_warmup_steps = int(num_warmup_steps_ratio * total_steps) if self.rank == 0: print(f\"Total steps: {total_steps}, num_warmup_steps: {num_warmup_steps}\") if warmup_style == \"constant\": actor_lr_scheduler = get_constant_schedule_with_warmup(optimizer=actor_optimizer, num_warmup_steps=num_warmup_steps) elif warmup_style == \"cosine\": actor_lr_scheduler = get_cosine_schedule_with_warmup(optimizer=actor_optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps, min_lr_ratio=min_lr_ratio, num_cycles=num_cycles) else: raise NotImplementedError(f\"Warmup style {warmup_style} is not supported\") log_gpu_memory_usage(f\"After {role} optimizer init\", logger=logger) else: actor_optimizer = None actor_lr_scheduler = None return actor_module_fsdp, actor_optimizer, actor_lr_scheduler, actor_model_config è¿™é‡Œä»£ç å¾ˆç›´ç™½ã€‚æœ‰ä¸€ä¸ªç‚¹å€¼å¾—å•ç‹¬æ‹å‡ºæ¥è®²ä¸€ä¸‹ï¼šä»”ç»†è§‚å¯Ÿ actor_module çš„ dtypeï¼Œç›´è§‰å‘Šè¯‰æˆ‘ï¼Œactor_module çš„ dtype åº”è¯¥æ˜¯ bf16 çš„ï¼Œè€Œ gradient å’Œ optimizer çš„ dtype æ˜¯ fp32 çš„ã€‚å¯æ˜¯ actor_module çš„ default dtype è¢«è®¾ä¸ºäº† fp32ï¼Œç„¶åä» fp32 load äº†æ¨¡å‹ã€‚å®é™…ä¸Šè¿™æ˜¯å› ä¸º pytorch çš„å„ç§ optimizer éƒ½æ˜¯ç›´æ¥å’Œ parameter ç»‘å®šçš„ï¼Œç”¨ bf16 çš„ parameter åˆå§‹åŒ–çš„ optimizer ä¹Ÿæ˜¯ bf16ã€‚æ‰€ä»¥ model å…ˆ load äº† fp32ï¼Œç„¶ååˆå§‹åŒ– optimizer ä½œä¸ºæ··åˆç²¾åº¦ï¼Œæœ€åæŠŠ model è½¬æˆ bf16ã€‚\nActorRolloutRefWorker._build_rollout() è¿™æ˜¯å¯¹æˆ‘è€Œè¨€æœ€æ¸…æ™°çš„åœ°æ–¹ï¼Œå®é™…ä¸Šä¹Ÿæ˜¯æœ€ç†Ÿæ‚‰çš„ã€‚åœ¨è¿™é‡Œç»ˆäºå¼•å…¥äº† SGLangï¼š\nè®¾å¤‡ç½‘æ ¼åˆ›å»ºï¼šä¸º Rollout åˆ›å»ºæ¨ç†å¼ é‡å¹¶è¡Œï¼ˆinfer_tpï¼‰è®¾å¤‡ç½‘æ ¼ã€‚ SGLang Rollout æ„å»ºï¼šå¯¼å…¥å¹¶å®ä¾‹åŒ– SGLangRollout å’Œ FSDPSGLangShardingManagerã€‚FSDPSGLangShardingManager è´Ÿè´£åœ¨ FSDP è®­ç»ƒæ ¼å¼å’Œ SGLang æ¨ç†æ ¼å¼ä¹‹é—´è½¬æ¢æ¨¡å‹æƒé‡ã€‚ ActorRolloutRefWorker._build_rollout éƒ¨åˆ†æºç  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def _build_rollout(self, trust_remote_code=False): from torch.distributed.device_mesh import init_device_mesh infer_tp = self.config.rollout.tensor_model_parallel_size dp = self.world_size // infer_tp assert self.world_size % infer_tp == 0, f\"rollout world_size: {self.world_size} is not divisible by infer_tp: {infer_tp}\" rollout_device_mesh = init_device_mesh(device_name, mesh_shape=(dp, infer_tp), mesh_dim_names=[\"dp\", \"infer_tp\"]) rollout_name = self.config.rollout.name if rollout_name in [\"sglang\", \"sglang_async\"]: if rollout_name == \"sglang_async\": warnings.warn( \"'sglang_async' has been deprecated and merged into 'sglang'. Please use 'sglang' going forward.\", DeprecationWarning, stacklevel=2, ) from verl.workers.rollout.sglang_rollout import SGLangRollout from verl.workers.sharding_manager.fsdp_sglang import FSDPSGLangShardingManager local_path = copy_to_local(self.config.model.path) log_gpu_memory_usage(f\"Before building {rollout_name} rollout\", logger=logger) rollout = SGLangRollout( actor_module=local_path, config=self.config.rollout, tokenizer=self.tokenizer, model_hf_config=self.actor_model_config, trust_remote_code=trust_remote_code, ) log_gpu_memory_usage(f\"After building {rollout_name} rollout\", logger=logger) if torch.distributed.get_world_size() == 1: self.config.rollout.load_format = \"dummy_hf\" rollout_sharding_manager = FSDPSGLangShardingManager( module=self.actor_module_fsdp, inference_engine=rollout._engine, model_config=self.actor_model_config, full_params=\"hf\" in self.config.rollout.load_format, device_mesh=rollout_device_mesh, offload_param=self._is_offload_param, ) log_gpu_memory_usage(\"After building sharding manager\", logger=logger) else: raise NotImplementedError(f\"Rollout name: {self.config.rollout.name} is not supported\") return rollout, rollout_sharding_manager SGLangRollout.__init__() äº‹å·²è‡³æ­¤ï¼Œå†å¾€ä¸‹çœ‹ä¸€å±‚ SGLang å…·ä½“çš„åˆå§‹åŒ–ï¼š\nè°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°å¹¶è®¾ç½®é…ç½®å’Œè®¾å¤‡ç½‘æ ¼ã€‚ é€šè¿‡ _initialize_tools() åˆå§‹åŒ–å·¥å…· schemasã€map å’Œè§£æå™¨ï¼Œæ”¯æŒ Multi-turn å¯¹è¯ä¸­çš„å·¥å…·ä½¿ç”¨ã€‚ åˆå§‹åŒ– SGLang æ¨ç†æ‰€éœ€çš„åˆ†å¸ƒå¼ç¯å¢ƒã€‚ é€šè¿‡ _verify_config() éªŒè¯æ¨¡å‹é…ç½®ã€‚ é€šè¿‡ _init_inference_engine() åˆå§‹åŒ– SGLang æ¨ç†å¼•æ“ã€‚ é€šè¿‡ _init_sampling_params() åˆå§‹åŒ–ç”Ÿæˆåºåˆ—çš„é‡‡æ ·å‚æ•°ã€‚ è®¾ç½® Tokenizer å’Œ padding token IDã€‚ SGLangRollout.__init__ éƒ¨åˆ†æºç  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class SGLangRollout(BaseRollout): def __init__( self, actor_module: str, config: DictConfig, tokenizer, model_hf_config, port=None, trust_remote_code: bool = False, device_mesh: DeviceMesh | None = None, **kwargs, ): \"\"\"Synchronized SGLang rollout engine. Args: actor_module: Huggingface model name or path to the model. The model should be supported by SGLang. config: A DictConfig object containing SGLang-specific operational parameters and rollout settings. Refer to https://docs.sglang.ai/backend/server_arguments.html tokenizer: The tokenizer instance compatible with the actor_module. model_hf_config: The Hugging Face model's configuration (e.g., `transformers.PretrainedConfig`). It provides architectural details and hyperparameters like `max_position_embeddings`, used by SGLang for correct model initialization. This is the model's inherent design, not SGLang's runtime behavior. port: Optional port for multi-node initialization when nnodes \u003e 1. trust_remote_code: Whether or not to allow for custom models defined on the Hub in their own modeling files. device_mesh: Optional `DeviceMesh` object for distributed setup. **kwargs: Additional keyword arguments, primarily `train_tp` for Megatron Backend integration to initialize hybrid engine process groups. \"\"\" super().__init__() self.config = config self._device_mesh_cpu = device_mesh os.environ.setdefault(\"SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK\", \"true\") ( self._tool_schemas, self._tool_map, self._tool_call_parser_type, self._sgl_tools, self._function_call_parser, ) = self._initialize_tools(config, tokenizer) self.interaction: dict[str, BaseInteraction] = self._intitalize_interaction(config) # If turn on `free_cache_engine`, SGLang engine's KV cache # will be freed after each `generate_sequences` call. assert not (not config.enforce_eager and config.free_cache_engine), \"disable CUDA graph (enforce_eager = False) if free cache engine\" logger.info(f\"tool_schemas: {self._tool_schemas}, tool_map: {self._tool_map}, tool_call_parser_type: {self._tool_call_parser_type}, sgl_tools: {self._sgl_tools}, function_call_parser: {self._function_call_parser}\") self._init_distributed_env(device_mesh_cpu=device_mesh, **kwargs) self._verify_config(model_hf_config=model_hf_config) # initialize the inference engine self._init_inference_engine(trust_remote_code, actor_module, port) self._init_sampling_params(**kwargs) self.tokenizer = tokenizer self.pad_token_id = tokenizer.pad_token_id SGLangRollout.AsyncEngine å…³äº SGLangRollout è°ƒç”¨ tool çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬åœ¨ä¸‹æ–‡çš„è®­ç»ƒå¾ªç¯ä¸­å†å±•å¼€ï¼Œè¿™é‡Œå…ˆè®¨è®ºå®Œ SGLang çš„åˆå§‹åŒ–ã€‚ä¸ºäº†è°ƒç”¨ SGLang engine çš„æ¥å£ï¼Œverl è¿›è¡Œäº†ä¸€å±‚å°è£…ï¼Œå®ç°äº†æˆ‘ä»¬å¯¹ SGLang é™¤å¼€ rollout ä¹‹å¤–çš„æ‰€æœ‰æ¥å£ï¼š\nrelease and resume memory occupationï¼šåœ¨è®­ç»ƒæ—¶é‡Šæ”¾æ‰æ˜¾å­˜å ç”¨å¹¶åœ¨è®­ç»ƒåæ¢å¤ã€‚ update weights from tensorï¼šè®­ç»ƒç»“æŸåæ›´æ–°æ¨¡å‹æƒé‡ã€‚ flush cacheï¼šæ¨¡å‹å‚æ•°æ›´æ–°ååˆ·æ–° KV cacheï¼Œå› ä¸ºä¹‹å‰çš„ KV cache å·²ç»å¤±æ•ˆäº†ã€‚ è¿™é‡Œæ¶‰åŠåˆ°äº†éå¸¸æ·±å…¥çš„å†…å­˜ç®¡ç†é—®é¢˜ï¼Œè¯»è€…å¯¹ SGLang engine åœ¨ verl é‡Œçš„æ˜¾å­˜ç®¡ç†æ„Ÿå…´è¶£ï¼Œæ¬¢è¿é˜…è¯»æ ‡å“¥çš„åšå®¢ optimizing Memory Usage in verlï¼Œå†™çš„éå¸¸æ·±å…¥æµ…å‡ºã€‚\nSGLangRollout ä½•æ—¶éœ€è¦ flush cache è¿™ä¸€éƒ¨åˆ†å†…å®¹éœ€è¦å•ç‹¬æ‹å‡ºæ¥è®²è®²ã€‚SGLang engine çš„ release å’Œ resume éœ€è¦ä¿ç•™ CUDA Graphï¼Œå¦åˆ™ rollout æ•ˆç‡ä¼šå¤§å¹…é™ä½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åŸºäº tom çš„ torch_memory_saver å®ç°äº†ç‹¬ç«‹çš„æ˜¾å­˜ç®¡ç†ã€‚ç®€å•æ¥è¯´ï¼Œæˆ‘ä»¬æœ‰ï¼š\npauseï¼›ä¿ç•™ mem savor ä½œç”¨åŸŸå†…æŒ‡å®š tensor çš„ virtual addressï¼Œä½†æ˜¯å°†å…¶ physical memory é‡Šæ”¾å›æ˜¾å­˜ç®¡ç†å™¨ã€‚ resumeï¼›å°†å…ˆå‰ pause çš„ tensor é‡æ–°ç”³è¯·ä¸€å— physical memoryï¼Œå¹¶å°†å…¶ virtual address æ˜ å°„åˆ°æ–°çš„ physical memoryã€‚ æ³¨æ„ï¼Œæ•´ä¸ª pause å’Œ resume çš„è¿‡ç¨‹ä¸­ï¼Œtensor çš„ virtual address ä¸ä¼šå‘ç”Ÿå˜åŒ–ï¼Œåªæ˜¯è¿™å— virtual address æ˜ å°„åˆ°çš„ physical memory æ”¹å˜äº†ã€‚å› æ­¤ï¼ŒCUDA Graph å¹¶æ²¡æœ‰å¤±æ•ˆï¼Œä¸å˜çš„ virtual address è®©è®¡ç®—æµä»æ—§å¯ä»¥æ­£å¸¸æ‰§è¡Œã€‚\nverl å†…çš„ release_memory_occupation å’Œ resume_memory_occupation å°±æ˜¯åŸºäº pause å’Œ resume å®ç°çš„ã€‚å¬ä¸Šå»æ˜¯ä¸ªå®Œç¾çš„æ•…äº‹ï¼Œæˆ‘ä»¬ç”šè‡³å®ç°äº† mutli-stage çš„æ˜¾å­˜ç®¡ç†ï¼Œèƒ½å¤Ÿç‹¬ç«‹ release å’Œ resume kv cache å’Œ model weightsã€‚\nä¸è¿‡ï¼Œå¯¹äº kv cache è€Œè¨€ï¼Œåœ¨ kv cache è¢« release æ‰ä¹‹åï¼Œå®é™…ä¸Š kv cache çš„ tensor ä»æ—§ä¿ç•™ï¼Œåªæ˜¯å…¶ virtual address æ˜ å°„åˆ°çš„ physical memory è¢«é‡Šæ”¾äº†ã€‚ä¸æ­¤åŒæ—¶ï¼Œradix tree ä»æ—§ç´¢å¼•ç€æ•´ä¸ª kv cacheã€‚å½“ kv cache è¢« resume ä¹‹åï¼Œä¸€æ–¹é¢ä¹‹å‰ç‰©ç†å†…å­˜ä¸Šä¹‹å‰çš„ kv cache å·²ç»ä¸å¤å­˜åœ¨äº†ï¼Œå¦ä¸€æ–¹é¢æ¨¡å‹çš„å‚æ•°ä¹Ÿè¢«æ›´æ–°ã€‚å‡ºäºè¿™ä¸¤ç‚¹ï¼Œæˆ‘ä»¬ä¸€å®šè¦ä½¿ç”¨ flush cache æ¥å£æ¥åˆ·æ–° kv cache çš„ç´¢å¼•ï¼ˆradix treeï¼‰ã€‚\nè¿™é‡Œåˆæœ‰ä¸ªéå¸¸æœ‰è¶£çš„è®¾è®¡ã€‚ä¹ä¸€æƒ³ kv cache çš„ç®¡ç†è¿™ä¹ˆéº»çƒ¦ï¼Œè¿˜è¦ flushï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥ delete kv cache ä»¥åŠ delete model weights å†é‡æ–°åˆå§‹åŒ–å‘¢ï¼Ÿæ˜¾ç„¶ï¼Œè¿™æ ·æ²¡æ³•åˆ©ç”¨å·²æœ‰çš„ cuda graphï¼Œéå¸¸æ¶ˆè€—æ—¶é—´ã€‚ä¿ç•™ virtual address ä¸å˜ä½†æ˜¯æ›´æ¢ physical memory çš„æ–¹æ¡ˆï¼Œè®© verl èƒ½å¤ŸæŒç»­åˆ©ç”¨å·²å»ºå¥½çš„ cuda graphã€‚\næœ€åä¸€ä¸ªé—®é¢˜ï¼Œä¸€å…±è¦å‡ æ¬¡ flush cache å‘¢ï¼Ÿæˆ‘ä¸ªäººç†è§£ï¼Œåœ¨ä¸€æ•´ä¸ª training engine è¢« pauseï¼Œresume ç„¶å update weights çš„è¿‡ç¨‹ä¸­ï¼Œå¿…é¡»è¦æœ‰ä¸€æ¬¡ flush cache æ¥åˆ·æ–° kv cache çš„ç´¢å¼•ï¼Œåªæ˜¯ verl å½“ä¸­ä¸ºäº†ä¿é™©ï¼Œåˆ·æ–°äº†å¾ˆå¤šæ¬¡ç½¢äº†ã€‚\nSGLangRollout.AsyncEngine æºç  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class AsyncEngine(sglang.srt.entrypoints.engine.Engine): def __init__(self, **kwargs): super().__init__(**kwargs) # default to use dummy load format, which need to reload weights in first time self._need_reload = True async def release_memory_occupation(self): \"\"\"Release GPU occupation temporarily.\"\"\" obj = ReleaseMemoryOccupationReqInput() return await self.tokenizer_manager.release_memory_occupation(obj, None) async def resume_memory_occupation(self): return await self.tokenizer_manager.resume_memory_occupation(obj, None) async def update_weights_from_tensor( self, named_tensors: List[Tuple[str, torch.Tensor]], # noqa: UP006 load_format: Optional[str] = None, flush_cache: bool = True, ): \"\"\"Update weights from distributed source. If there are going to be more updates, set `flush_cache` to be false to avoid duplicated cache cleaning operation.\"\"\" obj = UpdateWeightsFromTensorReqInput( serialized_named_tensors=[MultiprocessingSerializer.serialize(named_tensors) for _ in range(self.server_args.tp_size)], load_format=load_format, flush_cache=flush_cache, ) return await self.tokenizer_manager.update_weights_from_tensor(obj, None) async def flush_cache(self): return await self.tokenizer_manager.flush_cache() SGLangRollout._init_inference_engine() SGLangRollout._init_inference_engine() åˆå§‹åŒ–äº†å°è£…çš„ AsyncEngineã€‚\nSGLangRollout._init_inference_engine æºç  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def _init_inference_engine(self, trust_remote_code, actor_module, port): # initialize the inference engine nnodes = -(-self._tp_size // len(self.visible_devices_set)) if nnodes \u003e 1: ip = get_ip() port = get_open_port() if port is None else port [ip, port] = broadcast_pyobj( [ip, port], rank=self._rank, dist_group=self._device_mesh_cpu.get_group(\"tp\"), src=self._device_mesh_cpu[\"tp\"].mesh[0].item(), force_cpu_device=False, ) dist_init_addr = f\"[{ip}]:{port}\" if is_ipv6(ip) else f\"{ip}:{port}\" else: dist_init_addr = None load_format = \"dummy\" if self.config.load_format.startswith(\"dummy\") else self.config.load_format tp_size_per_node = self._tp_size // nnodes node_rank = self._tp_rank // tp_size_per_node first_rank_in_node = self._tp_rank % tp_size_per_node == 0 if first_rank_in_node: rank = dist.get_rank() os.environ[\"SGLANG_BLOCK_NONZERO_RANK_CHILDREN\"] = \"0\" self._engine = AsyncEngine( model_path=actor_module, dtype=self.config.dtype, mem_fraction_static=self.config.gpu_memory_utilization, enable_memory_saver=True, base_gpu_id=0, gpu_id_step=1, tp_size=self._tp_size, node_rank=node_rank, load_format=load_format, dist_init_addr=dist_init_addr, nnodes=nnodes, trust_remote_code=trust_remote_code, # NOTE(linjunrong): add rank to prevent SGLang generate same port inside PortArgs.init_new # when random.seed is being set during training port=30000 + rank, # NOTE(Chenyang): if you want to debug the SGLang engine output # please set the following parameters # Otherwise, it will make the engine run too slow # log_level=\"INFO\", # log_requests=True, # log_requests_level=2, # max_running_requests=1, ) else: self._engine = None self.sharding_manager = None self.is_sleep = True è¿™é‡Œæœ€å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒSGLang engine å¹¶æ²¡æœ‰ä¸¥æ ¼å®ç° verl æ‰€å¸Œæœ›çš„ SPMD æ¨¡å¼ï¼ˆæ¯ä¸ª GPU ä¸Šçš„è¿›ç¨‹å®Œå…¨ä¸€æ ·ï¼‰ï¼Œè€Œæ˜¯é‡‡ç”¨äº† mock çš„ SPMDã€‚ä¸¾ä¾‹æ¥è¯´ï¼Œå‡è®¾ tp size = 4ï¼ŒæŒ‰ç…§ verl çš„è®¾è®¡ï¼Œåº”è¯¥è¦ 4 å¼  GPU ä¸Šæ¯ä¸ªéƒ½è¿è¡Œä¸€ä¸ªç›¸åŒçš„ SGLang engineã€‚å®é™…ä¸Šçš„å®ç°æ˜¯åœ¨ GPU 0 ä¸Šå¯åŠ¨ä¸€ä¸ªè¿›ç¨‹å æ®å…¨éƒ¨ GPUï¼Œè€Œ GPU 1 2 3 ä¸Šä»…ä»…ä¿ç•™ä¸€ä¸ªç©ºè¿›ç¨‹ Noneã€‚è™½ç„¶ verl team èµ·åˆè®¾å®šä¸­è®¤ä¸ºä¸¥æ ¼çš„ SPMD æ„ä¹‰å·¨å¤§ï¼Œä½†å®é™…ä½¿ç”¨ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸º mock çš„ SPMD å·²ç»è¶³å¤Ÿæ»¡è¶³æ€§èƒ½éœ€æ±‚ã€‚\nã€TODOã€‘ è¿™ä¹ˆæè¿°å¯èƒ½ä¸ä¸¥è°¨ã€‚\nTaskRunner.run() å¾€ä¸‹èµ°äº†è¿™ä¹ˆå¤šå±‚ï¼Œæˆ‘ä»¬ç»ˆäºèƒ½å¤Ÿç»§ç»­å›åˆ° TaskRunner ç±»ã€‚ğŸ˜­\nã€TODOã€‘ä¸Šæ–‡å…¶å®ä¸»è¦æ˜¯ Actor Rolloutï¼Œè¿˜æ²¡æœ‰å…·ä½“è¯´ Actor çš„ training forward and backwardã€‚ä»¥åŠ Referenceï¼Œreward å’Œ critic çš„ training forward and backwardã€‚\nåŠ è½½ã€è§£æå’ŒéªŒè¯è®­ç»ƒä»»åŠ¡çš„é…ç½®ï¼ˆä½¿ç”¨ OmegaConfï¼‰ï¼Œç¡®ä¿æ‰€æœ‰å‚æ•°çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚ å°†æ¨¡å‹æ–‡ä»¶ä»è¿œç¨‹è·¯å¾„å¤åˆ¶åˆ°æœ¬åœ°ï¼Œç¡®ä¿æ‰€æœ‰ Worker éƒ½å¯ä»¥è®¿é—®ã€‚ ç»„ä»¶åˆå§‹åŒ–ï¼š åˆå§‹åŒ– Tokenizer å’Œ Processorï¼Œç”¨äºæ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®çš„å¤„ç†ã€‚ æ ¹æ®é…ç½®ä¸­æŒ‡å®šçš„ Actor ç­–ç•¥ï¼ˆå¦‚ fsdp æˆ– megatronï¼‰ï¼ŒåŠ¨æ€é€‰æ‹©ç›¸åº”çš„ Worker ç±»ï¼ˆä¾‹å¦‚ ActorRolloutRefWorker å’Œ CriticWorkerï¼‰ï¼Œå¹¶ç¡®å®šä½¿ç”¨çš„ RayWorkerGroup ç±»å‹ã€‚ å®šä¹‰ Ray èµ„æºæ± çš„è§„æ ¼å’Œè§’è‰²åˆ°èµ„æºæ± çš„æ˜ å°„ï¼Œç”¨äº GPU èµ„æºçš„åˆ†é…å’Œç®¡ç†ã€‚ åŠ è½½ç”¨äºè®­ç»ƒå’ŒéªŒè¯çš„å¥–åŠ±æ¨¡å‹ã€‚ åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼Œä»¥åŠè®­ç»ƒæ•°æ®é‡‡æ ·å™¨ã€‚ åˆ›å»º RayPPOTrainer å®ä¾‹ï¼Œå®ƒæ˜¯ç®¡ç†æ‰€æœ‰è®¡ç®—èµ„æºå’Œè®­ç»ƒæµç¨‹çš„ä¸­å¤®åè°ƒå™¨ã€‚ è°ƒç”¨ RayPPOTrainer çš„ init_workers() æ–¹æ³•ï¼Œå°†é…ç½®çš„ Worker ç±»å®ä¾‹åŒ–åˆ° Ray é›†ç¾¤çš„ GPU ä¸Šï¼Œä¸ºå®é™…è®¡ç®—åšå‡†å¤‡ã€‚ è°ƒç”¨ RayPPOTrainer çš„ fit() æ–¹æ³•ï¼Œå¯åŠ¨ PPO è®­ç»ƒå¾ªç¯ã€‚ TaskRunner.run æºç  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @ray.remote(num_cpus=1) class TaskRunner: def run(self, config): from pprint import pprint from omegaconf import OmegaConf from verl.utils.fs import copy_to_local import socket import os print(f\"TaskRunner hostname: {socket.gethostname()}, PID: {os.getpid()}\") pprint(OmegaConf.to_container(config, resolve=True)) OmegaConf.resolve(config) # æ¨¡å‹ä¸‹è½½ local_path = copy_to_local(config.actor_rollout_ref.model.path, use_shm=config.actor_rollout_ref.model.get(\"use_shm\", False)) # Tokenizer å’Œ Processor åˆå§‹åŒ– from verl.utils import hf_processor, hf_tokenizer trust_remote_code = config.data.get(\"trust_remote_code\", False) tokenizer = hf_tokenizer(local_path, trust_remote_code=trust_remote_code) processor = hf_processor(local_path, trust_remote_code=trust_remote_code, use_fast=True) # Worker ç±»å‹é€‰æ‹© if config.actor_rollout_ref.actor.strategy in [\"fsdp\", \"fsdp2\"]: from verl.single_controller.ray import RayWorkerGroup from verl.workers.fsdp_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == \"async\" else ActorRolloutRefWorker ray_worker_group_cls = RayWorkerGroup elif config.actor_rollout_ref.actor.strategy == \"megatron\": assert config.actor_rollout_ref.actor.strategy == config.critic.strategy from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup from verl.workers.megatron_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker actor_rollout_cls = AsyncActorRolloutRefWorker if config.actor_rollout_ref.rollout.mode == \"async\" else ActorRolloutRefWorker ray_worker_group_cls = NVMegatronRayWorkerGroup else: raise NotImplementedError from verl.trainer.ppo.ray_trainer import ResourcePoolManager, Role # è§’è‰²åˆ° Worker ç±»çš„æ˜ å°„ role_worker_mapping = { Role.ActorRollout: ray.remote(actor_rollout_cls), Role.Critic: ray.remote(CriticWorker), } # èµ„æºæ± è§„æ ¼å’Œè§’è‰²æ˜ å°„ global_pool_id = \"global_pool\" resource_pool_spec = { global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes, } mapping = { Role.ActorRollout: global_pool_id, Role.Critic: global_pool_id, } # Reward Model Worker çš„åˆå§‹åŒ– if config.reward_model.enable: if config.reward_model.strategy in [\"fsdp\", \"fsdp2\"]: from verl.workers.fsdp_workers import RewardModelWorker elif config.reward_model.strategy == \"megatron\": from verl.workers.megatron_workers import RewardModelWorker else: raise NotImplementedError role_worker_mapping[Role.RewardModel] = ray.remote(RewardModelWorker) mapping[Role.RewardModel] = global_pool_id # Reference Policy Worker çš„åˆå§‹åŒ– if config.algorithm.use_kl_in_reward or config.actor_rollout_ref.actor.use_kl_loss: role_worker_mapping[Role.RefPolicy] = ray.remote(ActorRolloutRefWorker) mapping[Role.RefPolicy] = global_pool_id # åŠ è½½å¥–åŠ±ç®¡ç†å™¨ reward_fn = load_reward_manager(config, tokenizer, num_examine=0, **config.reward_model.get(\"reward_kwargs\", {})) val_reward_fn = load_reward_manager(config, tokenizer, num_examine=1, **config.reward_model.get(\"reward_kwargs\", {})) resource_pool_manager = ResourcePoolManager(resource_pool_spec=resource_pool_spec, mapping=mapping) from verl.utils.dataset.rl_dataset import collate_fn, create_rl_dataset, create_rl_sampler # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›† train_dataset = create_rl_dataset(config.data.train_files, config.data, tokenizer, processor) val_dataset = create_rl_dataset(config.data.val_files, config.data, tokenizer, processor) train_sampler = create_rl_sampler(config.data, train_dataset) # åˆå§‹åŒ– PPO è®­ç»ƒå™¨ trainer = RayPPOTrainer( config=config, tokenizer=tokenizer, processor=processor, role_worker_mapping=role_worker_mapping, resource_pool_manager=resource_pool_manager, ray_worker_group_cls=ray_worker_group_cls, reward_fn=reward_fn, val_reward_fn=val_reward_fn, train_dataset=train_dataset, val_dataset=val_dataset, collate_fn=collate_fn, train_sampler=train_sampler, device_name=config.trainer.device, ) # åˆå§‹åŒ–è®­ç»ƒå™¨çš„ Workers trainer.init_workers() # å¯åŠ¨è®­ç»ƒè¿‡ç¨‹ trainer.fit() RayPPOTrainer.__init__() ä¿å­˜ä¼ å…¥çš„é…ç½®å¯¹è±¡ã€tokenizerã€processorã€è§’è‰²åˆ° Worker çš„æ˜ å°„ã€èµ„æºæ± ç®¡ç†å™¨ä»¥åŠ WorkerGroup ç±»ã€‚ æ ¹æ®é…ç½®å¯ç”¨æˆ–ç¦ç”¨ Criticã€Reference Policyã€Reward Model å’Œ Hybrid Engine ç­‰åŠŸèƒ½ç»„ä»¶ã€‚ è°ƒç”¨ _validate_config() æ–¹æ³•éªŒè¯é…ç½®çš„åˆç†æ€§ã€‚ å­˜å‚¨è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€collate å‡½æ•°å’Œè®­ç»ƒæ•°æ®é‡‡æ ·å™¨ã€‚ RayPPOTrainer æºç  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class RayPPOTrainer: # TODO: support each role have individual ray_worker_group_cls, # i.e., support different backend of different role def __init__( self, config, tokenizer, role_worker_mapping: dict[Role, WorkerType], resource_pool_manager: ResourcePoolManager, ray_worker_group_cls: RayWorkerGroup = RayWorkerGroup, processor=None, reward_fn=None, val_reward_fn=None, train_dataset: Optional[Dataset] = None, val_dataset: Optional[Dataset] = None, collate_fn=None, train_sampler: Optional[Sampler] = None, device_name=\"cuda\", ): \"\"\" Initialize distributed PPO trainer with Ray backend. Note that this trainer runs on the driver process on a single CPU/GPU node. Args: config: Configuration object containing training parameters. tokenizer: Tokenizer used for encoding and decoding text. role_worker_mapping (dict[Role, WorkerType]): Mapping from roles to worker classes. resource_pool_manager (ResourcePoolManager): Manager for Ray resource pools. ray_worker_group_cls (RayWorkerGroup, optional): Class for Ray worker groups. Defaults to RayWorkerGroup. processor: Optional data processor, used for multimodal data. reward_fn: Function for computing rewards during training. val_reward_fn: Function for computing rewards during validation. train_dataset (Optional[Dataset], optional): Training dataset. Defaults to None. val_dataset (Optional[Dataset], optional): Validation dataset. Defaults to None. collate_fn: Function to collate data samples into batches. train_sampler (Optional[Sampler], optional): Sampler for the training dataset. Defaults to None. device_name (str, optional): Device name for training (e.g., \"cuda\", \"cpu\"). Defaults to \"cuda\". \"\"\" # Store the tokenizer for text processing self.tokenizer = tokenizer self.processor = processor self.config = config self.reward_fn = reward_fn self.val_reward_fn = val_reward_fn self.hybrid_engine = config.actor_rollout_ref.hybrid_engine assert self.hybrid_engine, \"Currently, only support hybrid engine\" if self.hybrid_engine: assert Role.ActorRollout in role_worker_mapping, f\"{role_worker_mapping.keys()=}\" self.role_worker_mapping = role_worker_mapping self.resource_pool_manager = resource_pool_manager self.use_reference_policy = Role.RefPolicy in role_worker_mapping self.use_rm = Role.RewardModel in role_worker_mapping self.ray_worker_group_cls = ray_worker_group_cls self.device_name = device_name self.validation_generations_logger = ValidationGenerationsLogger() # if ref_in_actor is True, the reference policy will be actor without lora applied self.ref_in_actor = config.actor_rollout_ref.model.get(\"lora_rank\", 0) \u003e 0 # define in-reward KL control # kl loss control currently not suppoorted if config.algorithm.use_kl_in_reward: self.kl_ctrl_in_reward = core_algos.get_kl_controller(config.algorithm.kl_ctrl) if self.config.algorithm.adv_estimator == AdvantageEstimator.GAE: self.use_critic = True elif self.config.algorithm.adv_estimator in [ AdvantageEstimator.GRPO, AdvantageEstimator.GRPO_PASSK, AdvantageEstimator.REINFORCE_PLUS_PLUS, AdvantageEstimator.REMAX, AdvantageEstimator.RLOO, AdvantageEstimator.OPO, AdvantageEstimator.REINFORCE_PLUS_PLUS_BASELINE, ]: self.use_critic = False else: raise NotImplementedError self._validate_config() self._create_dataloader(train_dataset, val_dataset, collate_fn, train_sampler) RayPPOTrainer.init_workers() init_workers() å‡½æ•°è´Ÿè´£åœ¨ Ray é›†ç¾¤ä¸Šå®ä¾‹åŒ–å’Œåˆå§‹åŒ– ActorRolloutã€Criticã€Reference Policy å’Œ Reward Model Workersã€‚\nåˆ›å»ºèµ„æºæ± ï¼šé€šè¿‡ ResourcePoolManager åˆ›å»º Ray èµ„æºæ± ã€‚ åˆå§‹åŒ–èµ„æºæ± åˆ°ç±»çš„æ˜ å°„ï¼šä¸ºæ¯ä¸ªèµ„æºæ± åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨ä¸åŒè§’è‰² Worker çš„ RayClassWithInitArgs åŒ…è£…å™¨ã€‚RayClassWithInitArgs ç”¨äºå»¶è¿Ÿåˆå§‹åŒ– Workerï¼Œå­˜å‚¨äº† Worker çš„ç±»å’Œåˆå§‹åŒ–å‚æ•°ã€‚ åˆ›å»ºä¸åŒè§’è‰²çš„ Worker çš„ RayClassWithInitArgs å®ä¾‹ï¼šæ ¹æ®é…ç½®å¯ç”¨æƒ…å†µï¼Œä¸º ActorRolloutã€Criticã€Reference Policy å’Œ Reward Model åˆ›å»ºå¯¹åº”çš„ RayClassWithInitArgs å®ä¾‹ã€‚ åˆå§‹åŒ– WorkerGroupï¼šéå†æ‰€æœ‰èµ„æºæ± ï¼Œå°†åŒä¸€èµ„æºæ± ä¸­çš„å¤šä¸ª Worker ç±»é€šè¿‡ create_colocated_worker_cls ç»„åˆæˆä¸€ä¸ªå…±ç½®ç±»ï¼Œç„¶åå®ä¾‹åŒ– RayWorkerGroupã€‚RayWorkerGroup è´Ÿè´£åœ¨å¤šä¸ª GPU ä¸Šå¯åŠ¨å¤šä¸ª Worker å®ä¾‹ã€‚æœ€åè°ƒç”¨ spawn() æ–¹æ³•åœ¨ Ray ä¸­å®é™…åˆ›å»º Worker å®ä¾‹ã€‚ åˆå§‹åŒ–å„ä¸ª Workerï¼šæ ¹æ®è§’è‰²ä»åˆ›å»ºçš„ WorkerGroup å­—å…¸ä¸­è·å–å¯¹åº”çš„ WorkerGroupï¼Œå¹¶è°ƒç”¨å…¶ init_model() æ–¹æ³•ï¼ŒæŒ‰ç…§ä¾èµ–å…³ç³»ä¾æ¬¡åˆå§‹åŒ–ä¸åŒçš„ Worker æ¨¡å—ã€‚ActorRollout Worker é€šå¸¸æœ€ååˆå§‹åŒ–ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€‚ RayPPOTrainer.init_workers æºç  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def init_workers(self): \"\"\"Initialize distributed training workers using Ray backend. Creates: 1. Ray resource pools from configuration 2. Worker groups for each role (actor, critic, etc.) \"\"\" self.resource_pool_manager.create_resource_pool() self.resource_pool_to_cls = {pool: {} for pool in self.resource_pool_manager.resource_pool_dict.values()} # create actor and rollout if self.hybrid_engine: resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout) actor_rollout_cls = RayClassWithInitArgs( cls=self.role_worker_mapping[Role.ActorRollout], config=self.config.actor_rollout_ref, role=\"actor_rollout\", ) self.resource_pool_to_cls[resource_pool][\"actor_rollout\"] = actor_rollout_cls else: raise NotImplementedError # create critic if self.use_critic: resource_pool = self.resource_pool_manager.get_resource_pool(Role.Critic) critic_cls = RayClassWithInitArgs(cls=self.role_worker_mapping[Role.Critic], config=self.config.critic) self.resource_pool_to_cls[resource_pool][\"critic\"] = critic_cls # create reference policy if needed if self.use_reference_policy: resource_pool = self.resource_pool_manager.get_resource_pool(Role.RefPolicy) ref_policy_cls = RayClassWithInitArgs(self.role_worker_mapping[Role.RefPolicy], config=self.config.actor_rollout_ref, role=\"ref\") self.resource_pool_to_cls[resource_pool][\"ref\"] = ref_policy_cls # create a reward model if reward_fn is None if self.use_rm: # we create a RM here resource_pool = self.resource_pool_manager.get_resource_pool(Role.RewardModel) rm_cls = RayClassWithInitArgs(self.role_worker_mapping[Role.RewardModel], config=self.config.reward_model) self.resource_pool_to_cls[resource_pool][\"rm\"] = rm_cls # initialize WorkerGroup # NOTE: if you want to use a different resource pool for each role, which can support different parallel size, # you should not use `create_colocated_worker_cls`. # Instead, directly pass different resource pool to different worker groups. # See https://github.com/volcengine/verl/blob/master/examples/ray/tutorial.ipynb for more information. all_wg = {} wg_kwargs = {} # Setting up kwargs for RayWorkerGroup if OmegaConf.select(self.config.trainer, \"ray_wait_register_center_timeout\") is not None: wg_kwargs[\"ray_wait_register_center_timeout\"] = self.config.trainer.ray_wait_register_center_timeout if OmegaConf.select(self.config.trainer, \"profile_steps\") is not None: wg_kwargs[\"profile_steps\"] = OmegaConf.select(self.config.trainer, \"profile_steps\") assert OmegaConf.select(self.config.trainer, \"worker_nsight_options\") is not None, \"worker_nsight_options must be set when profile_steps is set\" wg_kwargs[\"worker_nsight_options\"] = OmegaConf.to_container(OmegaConf.select(self.config.trainer, \"worker_nsight_options\")) for resource_pool, class_dict in self.resource_pool_to_cls.items(): worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict) wg_dict = self.ray_worker_group_cls(resource_pool=resource_pool, ray_cls_with_init=worker_dict_cls, device_name=self.device_name, **wg_kwargs) spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys()) all_wg.update(spawn_wg) if self.use_critic: self.critic_wg = all_wg[\"critic\"] self.critic_wg.init_model() if self.use_reference_policy and not self.ref_in_actor: self.ref_policy_wg = all_wg[\"ref\"] self.ref_policy_wg.init_model() if self.use_rm: self.rm_wg = all_wg[\"rm\"] self.rm_wg.init_model() # we should create rollout at the end so that vllm can have a better estimation of kv cache memory self.actor_rollout_wg = all_wg[\"actor_rollout\"] self.actor_rollout_wg.init_model() # create async rollout manager and request scheduler self.async_rollout_mode = False if self.config.actor_rollout_ref.rollout.mode == \"async\": from verl.workers.rollout.async_server import AsyncLLMServerManager self.async_rollout_mode = True self.async_rollout_manager = AsyncLLMServerManager( config=self.config, worker_group=self.actor_rollout_wg, ) ","wordCount":"5605","inLanguage":"en","image":"https://pillumina.github.io/imgs/icon_head.png","datePublished":"2025-08-03T15:30:12+08:00","dateModified":"2025-08-03T15:30:12+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://pillumina.github.io/posts/aiinfra/07-verl-multiturn-1/"},"publisher":{"@type":"Organization","name":"CctoctoFX","logo":{"@type":"ImageObject","url":"https://pillumina.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://pillumina.github.io/ accesskey=h title="CctoctoFX (Alt + H)"><img src=https://pillumina.github.io/apple-touch-icon.png alt aria-label=logo height=30>CctoctoFX</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://pillumina.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://pillumina.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://pillumina.github.io/posts/aiinfra/ title="AI Infra"><span>AI Infra</span></a></li><li><a href=https://pillumina.github.io/posts/llmtheory/ title=Thoery><span>Thoery</span></a></li><li><a href=https://pillumina.github.io/posts/programming/ title=Programming><span>Programming</span></a></li><li><a href=https://pillumina.github.io/social/ title=Social><span>Social</span></a></li><li><a href=https://pillumina.github.io/open_courses/ title=Study><span>Study</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://pillumina.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://pillumina.github.io/posts/>Posts</a>&nbsp;Â»&nbsp;<a href=https://pillumina.github.io/posts/aiinfra/>AI Infra</a></div><h1 class="post-title entry-hint-parent">[VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰</h1><div class=post-meta><span title='2025-08-03 15:30:12 +0800 CST'>August 3, 2025</span>&nbsp;Â·&nbsp;27 min&nbsp;Â·&nbsp;5605 words&nbsp;Â·&nbsp;Me</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#æ•°æ®é¢„å¤„ç†><strong>æ•°æ®é¢„å¤„ç†</strong></a></li><li><a href=#å¯åŠ¨è®­ç»ƒ>å¯åŠ¨è®­ç»ƒ</a></li><li><a href=#è„šæœ¬é…ç½®>è„šæœ¬é…ç½®</a></li><li><a href=#è®­ç»ƒä¸»å…¥å£ä¸åˆå§‹åŒ–>è®­ç»ƒä¸»å…¥å£ä¸åˆå§‹åŒ–</a><ul><li><a href=#ray-actorray-task-å’Œ-ray-worker>Ray Actorï¼ŒRay Task å’Œ Ray Worker</a></li><li><a href=#run_ppo-å’Œ-taskrunnerrun><code>run_ppo()</code> å’Œ <code>TaskRunner.run()</code></a></li><li><a href=#actorrolloutrefworker-å’Œ-rayworkergroup-çš„ç›¸äº’å…³ç³»>ActorRolloutRefWorker å’Œ RayWorkerGroup çš„ç›¸äº’å…³ç³»</a></li><li><a href=#actorrolloutrefworker__init__><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/workers/fsdp_workers.py#L101><code>ActorRolloutRefWorker.__init__()</code></a></a></li><li><a href=#actorrolloutrefworker_build_model_optimizer><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/fsdp_workers.py#L177><code>ActorRolloutRefWorker._build_model_optimizer()</code></a></a></li><li><a href=#actorrolloutrefworker_build_rollout><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/fsdp_workers.py#L394><code>ActorRolloutRefWorker._build_rollout()</code></a></a></li><li><a href=#sglangrollout__init__><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L208><code>SGLangRollout.__init__()</code></a></a></li><li><a href=#sglangrolloutasyncengine><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L124><code>SGLangRollout.AsyncEngine</code></a></a></li><li><a href=#sglangrollout_init_inference_engine><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L325><code>SGLangRollout._init_inference_engine()</code></a></a></li><li><a href=#taskrunnerrun><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L64><code>TaskRunner.run()</code></a></a></li><li><a href=#rayppotrainer__init__><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L277><code>RayPPOTrainer.__init__()</code></a></a></li><li><a href=#rayppotrainerinit_workers><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L715><code>RayPPOTrainer.init_workers()</code></a></a></li></ul></li></ul></nav></div></details></div><div class=post-content><blockquote><p>è¯¥partä¸»è¦èšç„¦ç›¸å…³æ¨¡å—åˆå§‹åŒ–éƒ¨åˆ†</p></blockquote><p>è¿˜æ˜¯ä»¥ verl å‡ºå‘ï¼Œåˆ†æå…¶ end to end mutli-turn RL è®­ç»ƒçš„å…¨è¿‡ç¨‹ã€‚æ•´ä½“ä¸Šï¼Œæˆ‘å¸Œæœ›è¦†ç›–æ‰€æœ‰é‡è¦çš„ class ä»¥åŠå‡½æ•°ï¼Œæ›´ç»†ç²’åº¦çš„ä»£ç ä¸å†å±•å¼€ã€‚</p><p>ä¸ºäº†å‰åå†…å®¹çš„ä¸€è‡´æ€§ï¼ŒåŸºäº <a href=https://github.com/volcengine/verl/commit/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39>76f63cffa5</a> çš„ commit è¿›è¡Œåˆ†æã€‚</p><p>è™½ç„¶æœ¬æ–‡ä»¥åˆ†æ verl çš„ä»£ç ä¸ºä¸»ï¼Œå†™å®Œä¹‹åæˆ‘æ‰æ„è¯†åˆ°ï¼Œç³»ç»Ÿè®¾è®¡é—®é¢˜æ˜¯éå¸¸é€šç”¨çš„ã€‚è¯¸å¦‚â€œlog probs é‡è®¡ç®—â€ï¼Œâ€œRollout Engine æ˜¾å­˜ç®¡ç†â€ç­‰ç­‰ç³»ç»Ÿè®¾è®¡ï¼Œæ˜¯å„å¤§ RL æ¡†æ¶éƒ½éœ€è¦è€ƒè™‘çš„æ ¸å¿ƒé—®é¢˜ã€‚</p><p>æ­¤å¤–å› ä¸ºæœ€è¿‘åœ¨å­¦ä¹ SGLangçš„å®ç°ï¼Œæœ¬æ–‡çš„æ¨ç†åç«¯é€‰æ‹©çš„æ˜¯SGLangå±•å¼€åˆ†æã€‚</p><hr><p>æ•´ä¸ªè®­ç»ƒçš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼Œæˆ‘ä»¬ä¼šå…·ä½“å±•å¼€æ¯ä¸ªéƒ¨åˆ†ã€‚</p><pre class=mermaid>
  flowchart LR
subgraph W2[&#34;Initialize&#34;]
WP[Process Data] --&gt; A
direction TB D1[Data Prepare] --&gt; A
A[TaskRunner] --&gt; B1[RayPPOTrainer]
B1 --&gt; Workers

    subgraph Workers[&#34;Workers&#34;]
        direction TB
                WA[ActorRolloutWorker] --&gt; WD[FSDP Engine]
        WB[CriticWorker] --&gt; WD
        WC[RewardModelWorker] --&gt; WD
        WD --&gt; WE[SGLang Engine]
    end
    
    Workers --&gt; C1[Hybrid Engine]
end

subgraph W3[&#34;Train Loop&#34;]
    direction TB
    E[DataLoader] --&gt; RolloutBox
    
    subgraph RolloutBox[&#34;Rollout&#34;]
        F1[Prepare Data] --&gt; F2[SGLang Async Rollout]
        F2 --&gt; F3[Multi-turn Chat Process]
    end
    
    RolloutBox --&gt; ExpBox
    
    subgraph ExpBox[&#34;Make Experience&#34;]
        G1[Recompute Log Probs] --&gt; G2[Compute Reward]
        G2 --&gt; G3[Compute Advantage]
    end
    
    ExpBox --&gt; UpdateBox
    
    subgraph UpdateBox[&#34;Train The Model&#34;]
        H1[Load FSDP Model Weight] --&gt; H2[Compute Gradient]
        H2 --&gt; H3[Weights Update]
        H3 --&gt; H4[Sync Weights]
    end
    
    UpdateBox --&gt; E
end

W2 --&gt; W3
</pre><h2 id=æ•°æ®é¢„å¤„ç†><strong>æ•°æ®é¢„å¤„ç†</strong><a hidden class=anchor aria-hidden=true href=#æ•°æ®é¢„å¤„ç†>#</a></h2><p>ä»¥ <a href=https://huggingface.co/datasets/openai/gsm8k>GSM8K</a> ä¸ºä¾‹ï¼Œé¢„å¤„ç†è„šæœ¬æ˜¯Â <code>examples/data_preprocess/gsm8k_multiturn_w_tool.py</code>ã€‚æ•´ä¸ªè„šæœ¬åªåšäº†ç»å…¸çš„ huggingface datasets mappingï¼Œæ ¸å¿ƒé€»è¾‘å¦‚ä¸‹ï¼š</p><ol><li>åŠ è½½ openai/gsm8k åŸå§‹æ•°æ®é›†ï¼ˆtrain/testï¼‰ã€‚</li><li>å¯¹æ¯æ¡åŸå§‹æ•°æ®ï¼Œç”Ÿæˆå¸¦æœ‰å·¥å…·è°ƒç”¨è¦æ±‚çš„ promptï¼ˆæ¯”å¦‚åœ¨ user turn å¼ºè°ƒæ¨¡å‹å¯ä»¥è°ƒç”¨Â <code>calc_gsm8k_reward</code>Â å·¥å…·ï¼Œæ¯ä¸ªqaè‡³å°‘è°ƒç”¨ä¸€æ¬¡ï¼‰ã€‚</li><li>åŒæ ·å¯¹äºæ¯æ¡åŸå§‹æ•°æ®ï¼Œè§£æç­”æ¡ˆï¼›å°† ground truth å†™å…¥ extra_info å­—æ®µã€‚</li><li>å­˜å‚¨ä¸º parquet æ–‡ä»¶ï¼Œåˆ†åˆ«ä¿ç•™ä¸º train.parquet å’Œ test.parquetï¼Œé»˜è®¤è·¯å¾„ä¸ºÂ <code>~/data/gsm8k/</code>ã€‚</li></ol><h2 id=å¯åŠ¨è®­ç»ƒ>å¯åŠ¨è®­ç»ƒ<a hidden class=anchor aria-hidden=true href=#å¯åŠ¨è®­ç»ƒ>#</a></h2><p>ä¸€ä¸ªå…¸å‹çš„å¯åŠ¨å‘½ä»¤å¦‚ä¸‹ï¼š</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># now ç”¨äºç”Ÿæˆå®éªŒå¯åŠ¨çš„æ—¶é—´å°¾ç¼€ï¼Œé¿å…é‡å¤å¯åŠ¨å®éªŒæ—¶è¦†ç›–å·²æœ‰ wandb log</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>function</span> now<span class=o>()</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    date <span class=s1>&#39;+%Y-%m-%d-%H-%M&#39;</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>CUDA_VISIBLE_DEVICES</span><span class=o>=</span>0,1,2,3,4,5,6,7
</span></span><span class=line><span class=cl>nohup bash examples/sglang_multiturn/run_qwen2.5-3b_gsm8k_multiturn.sh <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    trainer.experiment_name<span class=o>=</span>qwen2.5-3b_rm-gsm8k-sgl-multiturn-<span class=nv>$now</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    &gt; logs/gsm8k-<span class=nv>$now</span>.log 2&gt;<span class=p>&amp;</span><span class=m>1</span> <span class=p>&amp;</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=è„šæœ¬é…ç½®>è„šæœ¬é…ç½®<a hidden class=anchor aria-hidden=true href=#è„šæœ¬é…ç½®>#</a></h2><p>verl çš„å„é¡¹å‚æ•°å®å±å¤æ‚ï¼Œæˆ‘ä»¬ä¼šå•ç‹¬ç¼–å†™æ–‡æ¡£æ¥åˆ†äº«å¯¹ verl å„ç±»å‚æ•°çš„ç†è§£ã€‚åœ¨è¿™ç¯‡æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦æ ¼å¤–å¼ºè°ƒçš„æ˜¯ verl å„ç±» config çš„è¦†ç›–å…³ç³»ã€‚verl çš„é…ç½®æ–‡ä»¶åˆ©ç”¨ hydra è¿›è¡Œäº†<strong>åˆ†å±‚è¦†ç›–</strong>çš„è®¾è®¡æ¨¡å¼ã€‚</p><details><summary>Hydra ç®€ä»‹</summary><p><a href=https://github.com/facebookresearch/hydra><strong>Hydra</strong></a> æ˜¯ä¸€ä¸ªç”± Facebook Research å¼€å‘çš„ Python æ¡†æ¶ï¼Œæ—¨åœ¨<strong>ä¼˜é›…åœ°é…ç½®å¤æ‚çš„åº”ç”¨ç¨‹åº</strong>ã€‚å®ƒç‰¹åˆ«é€‚ç”¨äºéœ€è¦ç®¡ç†å¤§é‡å‚æ•°å’Œè¿›è¡Œå¤šç»„å®éªŒçš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨å­¦ä¹ é¡¹ç›®ã€‚Hydra çš„æ ¸å¿ƒç‰¹ç‚¹åœ¨äºå…¶<strong>åŠ¨æ€ã€åˆ†å±‚å’Œå¯ç»„åˆçš„é…ç½®ç®¡ç†èƒ½åŠ›</strong>ã€‚Hydra çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š</p><ul><li><strong>åˆ†å±‚é…ç½® (Hierarchical Configuration)</strong>ï¼šå¯ä»¥å°†é…ç½®åˆ†è§£æˆå¤šä¸ªå°å‹ã€æ¨¡å—åŒ–çš„ YAML æ–‡ä»¶ï¼Œå¹¶ä»¥ç›®å½•ç»“æ„è¿›è¡Œç»„ç»‡ã€‚è¿™ä½¿å¾—é…ç½®æ›´åŠ æ¸…æ™°ã€æ˜“äºç®¡ç†å’Œå¤ç”¨ã€‚</li><li><strong>é…ç½®ç»„åˆ (Configuration Composition)</strong>ï¼šHydra èƒ½å¤Ÿå°†è¿™äº›ç‹¬ç«‹çš„é…ç½®æ¨¡å—åŠ¨æ€åœ°ç»„åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„é…ç½®å¯¹è±¡ã€‚ä½ å¯ä»¥é€šè¿‡åœ¨ä¸»é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š <code>defaults</code> åˆ—è¡¨æ¥é€‰æ‹©å’Œç»„åˆä¸åŒçš„é…ç½®ç»„ä»¶ã€‚</li><li><strong>å‘½ä»¤è¡Œè¦†ç›– (Command-line Overrides)</strong>ï¼šè¿™æ˜¯ Hydra æœ€å¼ºå¤§çš„åŠŸèƒ½ä¹‹ä¸€ã€‚ä½ å¯ä»¥åœ¨è¿è¡Œåº”ç”¨ç¨‹åºæ—¶ï¼Œç›´æ¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ¥è¦†ç›–é…ç½®ä¸­çš„ä»»ä½•å€¼ã€‚è¿™ä½¿å¾—è¿›è¡Œå®éªŒå’Œå¿«é€Ÿè¿­ä»£å˜å¾—éå¸¸æ–¹ä¾¿ï¼Œæ— éœ€ä¿®æ”¹é…ç½®æ–‡ä»¶æœ¬èº«ã€‚</li><li><strong>å¤šè¿è¡Œæ¨¡å¼ (Multi-run)</strong>ï¼šHydra å…è®¸ä½ é€šè¿‡ä¸€ä¸ªå‘½ä»¤è¿è¡Œå¤šä¸ªå…·æœ‰ä¸åŒé…ç½®çš„å®éªŒã€‚è¿™å¯¹äºè¶…å‚æ•°æœç´¢å’Œæ¨¡å‹æ¯”è¾ƒéå¸¸æœ‰ç”¨ã€‚</li><li><strong>åŠ¨æ€å·¥ä½œç›®å½• (Dynamic Working Directory)</strong>ï¼šæ¯æ¬¡è¿è¡Œåº”ç”¨ç¨‹åºæ—¶ï¼ŒHydra éƒ½ä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„å·¥ä½œç›®å½•ï¼Œå¹¶å°†å½“å‰è¿è¡Œçš„é…ç½®å’Œè¾“å‡ºä¿å­˜åˆ°è¯¥ç›®å½•ä¸­ï¼Œç¡®ä¿å®éªŒçš„å¯å¤ç°æ€§ã€‚</li><li><strong>å¯¹è±¡å®ä¾‹åŒ– (Object Instantiation)</strong>ï¼šHydra å¯ä»¥ç›´æ¥ä»é…ç½®ä¸­å®ä¾‹åŒ– Python å¯¹è±¡ï¼ˆç±»æˆ–å‡½æ•°ï¼‰ï¼Œè¿™å¤§å¤§ç®€åŒ–äº†ä»£ç ï¼Œä½¿é…ç½®æ›´å…·å£°æ˜æ€§ã€‚</li></ul><p>Hydra å®ç°åˆ†å±‚è¦†ç›–çš„ä¸»è¦æœºåˆ¶æ˜¯<strong>ç»„åˆ (Composition)</strong> å’Œ <strong>å‘½ä»¤è¡Œè¦†ç›– (Command-line Overrides)</strong>ã€‚</p><ol><li><strong>åˆ†å±‚é…ç½®çš„ç»„ç»‡</strong>ï¼š</li></ol><p>é€šå¸¸ä¼šåˆ›å»ºä¸€ä¸ª <code>conf</code> ç›®å½•ï¼Œå¹¶åœ¨å…¶ä¸­ç»„ç»‡é…ç½®ã€‚ä¾‹å¦‚ï¼š</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=l>.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=l>â”œâ”€â”€ my_app.py</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=l>â””â”€â”€ conf</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=l>â”œâ”€â”€ config.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=l>â”œâ”€â”€ model</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=l>â”‚   â”œâ”€â”€ cnn.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=l>â”‚   â””â”€â”€ rnn.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=l>â””â”€â”€ dataset</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=l>â”œâ”€â”€ cifar10.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=l>â””â”€â”€ imagenet.yaml</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p><code>config.yaml</code> æ˜¯ä½ çš„ä¸»é…ç½®æ–‡ä»¶ã€‚åœ¨ <code>model</code> ç›®å½•ä¸‹ï¼Œä½ å¯ä»¥å®šä¹‰ä¸åŒçš„æ¨¡å‹é…ç½®ï¼ˆå¦‚ <code>cnn.yaml</code>ã€<code>rnn.yaml</code>ï¼‰ï¼Œåœ¨ <code>dataset</code> ç›®å½•ä¸‹å®šä¹‰ä¸åŒçš„æ•°æ®é›†é…ç½®ï¼ˆå¦‚ <code>cifar10.yaml</code>ã€<code>imagenet.yaml</code>ï¼‰ã€‚</p><ol start=2><li><strong><code>defaults</code> åˆ—è¡¨è¿›è¡Œç»„åˆ</strong>ï¼š</li></ol><p>åœ¨ <code>config.yaml</code> ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨ç‰¹æ®Šçš„ <code>defaults</code> åˆ—è¡¨æ¥æŒ‡å®šé»˜è®¤åŠ è½½å“ªäº›é…ç½®ç»„ä»¶ã€‚</p><p><strong><code>conf/config.yaml</code> ç¤ºä¾‹ï¼š</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>defaults</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>model</span><span class=p>:</span><span class=w> </span><span class=l>cnn      </span><span class=w> </span><span class=c># é»˜è®¤åŠ è½½ conf/model/cnn.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>dataset</span><span class=p>:</span><span class=w> </span><span class=l>cifar10</span><span class=w> </span><span class=c># é»˜è®¤åŠ è½½ conf/dataset/cifar10.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>_self_         </span><span class=w> </span><span class=c># ç¡®ä¿å½“å‰æ–‡ä»¶ä¸­çš„å…¶ä»–é…ç½®é¡¹ä¹Ÿè¢«åŠ è½½</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># å…¶ä»–åº”ç”¨çº§åˆ«çš„é»˜è®¤é…ç½®</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>learning_rate</span><span class=p>:</span><span class=w> </span><span class=m>0.001</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>epochs</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>å½“ Hydra åŠ è½½ <code>config.yaml</code> æ—¶ï¼Œå®ƒä¼šæ ¹æ® <code>defaults</code> åˆ—è¡¨ä¸­çš„æŒ‡ç¤ºï¼Œè‡ªåŠ¨å°† <code>conf/model/cnn.yaml</code> å’Œ <code>conf/dataset/cifar10.yaml</code> çš„å†…å®¹åˆå¹¶åˆ°æœ€ç»ˆçš„é…ç½®å¯¹è±¡ä¸­ã€‚</p><ol start=3><li><strong>å‘½ä»¤è¡Œè¦†ç›–</strong>ï¼š</li></ol><p>è¿™æ˜¯å®ç°çµæ´»è¦†ç›–çš„å…³é”®ã€‚ä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ¥è¦†ç›–ä»»ä½•å·²åŠ è½½çš„é…ç½®å€¼ï¼ŒåŒ…æ‹¬åœ¨ <code>defaults</code> åˆ—è¡¨ä¸­æŒ‡å®šçš„ç»„ä»¶æˆ–å…¶å†…éƒ¨çš„ä»»ä½•å‚æ•°ã€‚</p><ul><li><strong>è¦†ç›–æ•´ä¸ªé…ç½®ç»„</strong>ï¼š<br>è¦åˆ‡æ¢æ¨¡å‹ä» <code>cnn</code> åˆ° <code>rnn</code>ï¼Œä½ å¯ä»¥åœ¨å‘½ä»¤è¡Œä¸­è¿™æ ·è¿è¡Œï¼š</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python my_app.py <span class=nv>model</span><span class=o>=</span>rnn
</span></span></code></pre></td></tr></table></div></div><p>è¿™å°†æŒ‡ç¤º Hydra åŠ è½½ <code>conf/model/rnn.yaml</code>ï¼Œå¹¶ç”¨å®ƒæ¥æ›¿æ¢é»˜è®¤çš„ <code>cnn</code> é…ç½®ã€‚</p><ul><li><strong>è¦†ç›–ç‰¹å®šå‚æ•°</strong>ï¼š<br>ä½ å¯ä»¥æ·±å…¥åˆ°é…ç½®çš„ä»»ä½•å±‚çº§æ¥è¦†ç›–ç‰¹å®šçš„å‚æ•°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³ä¿®æ”¹å­¦ä¹ ç‡æˆ–æ•°æ®é›†çš„æŸä¸ªå‚æ•°ï¼š</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python my_app.py <span class=nv>learning_rate</span><span class=o>=</span>0.01 dataset.batch_size<span class=o>=</span><span class=m>64</span>
</span></span></code></pre></td></tr></table></div></div><p>è¿™é‡Œï¼Œ<code>learning_rate</code> ç›´æ¥è¦†ç›–äº† <code>config.yaml</code> ä¸­çš„å€¼ï¼Œè€Œ <code>dataset.batch_size</code> åˆ™è¦†ç›–äº† <code>conf/dataset/cifar10.yaml</code>ï¼ˆæˆ–è€…ä½ é€šè¿‡ <code>dataset=imagenet</code> æŒ‡å®šçš„å…¶ä»–æ•°æ®é›†é…ç½®æ–‡ä»¶ï¼‰ä¸­çš„ <code>batch_size</code> å‚æ•°ã€‚</p><ul><li><strong>æ·»åŠ æ–°å‚æ•° (ä½¿ç”¨ <code>+</code>)</strong>ï¼š<br>å¦‚æœä½ æƒ³æ·»åŠ ä¸€ä¸ªåœ¨é»˜è®¤é…ç½®ä¸­ä¸å­˜åœ¨çš„æ–°å‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨ <code>+</code> å‰ç¼€ï¼š</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python my_app.py +optimizer.name<span class=o>=</span>AdamW
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>åŠ¨æ€è¦†ç›– (ä½¿ç”¨ <code>++</code>)</strong>ï¼š<br>å¦‚æœä½ å¸Œæœ›ä¿®æ”¹ä¸€ä¸ªå·²æœ‰å­—æ®µï¼Œæˆ–è€…åœ¨åŸé…ç½®ä¸­æ²¡æœ‰è¯¥å­—æ®µæ—¶è‡ªåŠ¨åˆ›å»ºå®ƒï¼Œå¯ä»¥ä½¿ç”¨ ++ã€‚è¿™ç§æ–¹å¼é€‚ç”¨äºéœ€è¦åŠ¨æ€æ·»åŠ æˆ–è¦†ç›–é…ç½®é¡¹çš„åœºæ™¯ï¼Œç¡®ä¿å­—æ®µæ€»æ˜¯è¢«è®¾ç½®ä¸ºä½ æŒ‡å®šçš„å€¼ï¼Œæ— è®ºå®ƒæ˜¯å¦å·²å­˜åœ¨ã€‚</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python my_app.py ++model.num_layers<span class=o>=</span><span class=m>10</span>
</span></span></code></pre></td></tr></table></div></div><p>Hydra å†…éƒ¨ä½¿ç”¨ <a href="https://www.google.com/search?q=https://omegaconf.readthedocs.io/en/2.3_latest/">OmegaConf</a> åº“æ¥å¤„ç†è¿™äº›é…ç½®å¯¹è±¡ï¼Œå®ƒæä¾›äº†å¼ºå¤§çš„åˆå¹¶å’Œè§£æåŠŸèƒ½ï¼Œä½¿å¾—åˆ†å±‚è¦†ç›–å’Œå€¼æ’å€¼ï¼ˆä¾‹å¦‚ï¼Œå¼•ç”¨å…¶ä»–é…ç½®å€¼æˆ–ç¯å¢ƒå˜é‡ï¼‰å˜å¾—éå¸¸å®¹æ˜“ã€‚</p></details><p>å›åˆ° verl multi turnï¼Œåœ¨æˆ‘ä»¬å¯åŠ¨çš„ <code>run_qwen2.5-3b_gsm8k_multiturn.sh</code> ä¸­ï¼Œè®¾ç½®äº†ï¼š</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>PROJECT_DIR</span><span class=o>=</span><span class=s2>&#34;</span><span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=nv>CONFIG_PATH</span><span class=o>=</span><span class=s2>&#34;</span><span class=nv>$PROJECT_DIR</span><span class=s2>/examples/sglang_multiturn/config&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>python3 -m verl.trainer.main_ppo <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --config-path<span class=o>=</span><span class=s2>&#34;</span><span class=nv>$CONFIG_PATH</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --config-name<span class=o>=</span><span class=s1>&#39;gsm8k_multiturn_grpo&#39;</span> <span class=se>\
</span></span></span></code></pre></td></tr></table></div></div><p>è¿™æ„å‘³ç€è¿™æ¬¡ä»»åŠ¡çš„é»˜è®¤ config æ˜¯ <code>CONFIG_PATH</code> ä¸‹çš„ <code>gsm8k_multiturn_grpo.yaml</code>ï¼Œä¸”æ¥ä¸‹æ¥çš„å‚æ•°ä¼šè¦†ç›– <code>gsm8k_multiturn_grpo.yaml</code> ä¸­çš„é»˜è®¤å€¼ã€‚æ›´è¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬æ¥è§‚å¯Ÿ <code>gsm8k_multiturn_grpo.yaml</code> çš„å†…å®¹ï¼š</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>hydra</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>searchpath</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>file://verl/trainer/config</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>defaults</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>ppo_trainer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>_self_</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>data</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>max_prompt_length</span><span class=p>:</span><span class=w> </span><span class=m>1024</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>max_response_length</span><span class=p>:</span><span class=w> </span><span class=m>1024</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>train_batch_size</span><span class=p>:</span><span class=w> </span><span class=m>256</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>return_raw_chat</span><span class=p>:</span><span class=w> </span><span class=kc>True</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>actor_rollout_ref</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>hybrid_engine</span><span class=p>:</span><span class=w> </span><span class=kc>True</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>rollout</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>sglang</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>multi_turn</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>enable</span><span class=p>:</span><span class=w> </span><span class=kc>True</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>max_turns</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=c># tool_config_path: &#34;./config/tool_config/gsm8k_tool_config.yaml&#34;</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>è¿™é‡Œ hydra è¯­æ³•ï¼Œä¼šå» <code>verl/trainer/config</code> ç›®å½•ä¸‹å¯»æ‰¾ <code>ppo_trainer.yaml</code> ä½œä¸ºåŸºç¡€é…ç½®ï¼Œå¹¶ä¸”è¦†ç›–ã€‚å› æ­¤ï¼Œå¯åŠ¨ <code>run_qwen2.5-3b_gsm8k_multiturn.sh</code> æ—¶ï¼Œå…ˆåŠ è½½ <code>gsm8k_multiturn_grpo.yaml</code> ä½œä¸ºåŸºç¡€é…ç½®å¹¶è¦†ç›–ï¼Œç„¶ååŠ è½½ <code>ppo_trainer.yaml</code> å¹¶è¦†ç›–ã€‚æœ€ç»ˆåˆå¹¶è¿™ä¸‰çº§é…ç½®ï¼Œå¾—åˆ°æœ€ç»ˆçš„ configã€‚</p><p>æœ€åï¼Œæ³¨æ„åˆ°åœ¨ <code>run_qwen2.5-3b_gsm8k_multiturn.sh</code> çš„æœ€åï¼Œæˆ‘ä»¬ï¼Œæˆ‘ä»¬è®¾ç½®äº† <code>actor_rollout_ref.rollout.multi_turn.tool_config_path="$PROJECT_DIR/examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml"</code>ï¼Œè¿™é‡ŒæŒ‡å®š multi_turn çš„ tool_config_path ä¸º <code>examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml</code>ã€‚è¿™ä¸€æ–‡ä»¶ä»…ä»…é…ç½®äº† gsm8k çš„ tool è°ƒç”¨ï¼Œå¹¶ä¸ä¼šè¦†ç›–ä¹‹å‰è®­ç»ƒçš„ configã€‚</p><h2 id=è®­ç»ƒä¸»å…¥å£ä¸åˆå§‹åŒ–>è®­ç»ƒä¸»å…¥å£ä¸åˆå§‹åŒ–<a hidden class=anchor aria-hidden=true href=#è®­ç»ƒä¸»å…¥å£ä¸åˆå§‹åŒ–>#</a></h2><h3 id=ray-actorray-task-å’Œ-ray-worker>Ray Actorï¼ŒRay Task å’Œ Ray Worker<a hidden class=anchor aria-hidden=true href=#ray-actorray-task-å’Œ-ray-worker>#</a></h3><p>åœ¨ä»‹ç» verl çš„è®­ç»ƒä¸»å…¥å£ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆä»‹ç» Ray çš„ä¸€äº›æ ¸å¿ƒæ¦‚å¿µã€‚Ray æ˜¯ä¸€ä¸ªç»Ÿä¸€è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ç®€å•åœ°ä»å•æœºåˆ°å¤§å‹åˆ†å¸ƒå¼é›†ç¾¤çš„æ‰©å±•ï¼Œæä¾›æ„å»ºå’Œè¿è¡Œåˆ†å¸ƒå¼åº”ç”¨çš„åº•å±‚åŸºç¡€è®¾æ–½å’Œä¸€ç»„æ ¸å¿ƒåŸè¯­ã€‚Ray é€šè¿‡ä»¥ä¸‹åŠŸèƒ½å®ç°è¿™ä¸€ç›®æ ‡ï¼š</p><ol><li><strong>ç»Ÿä¸€ API</strong>ï¼šRay æä¾›äº†ä¸€å¥—ç®€å•æ˜“ç”¨çš„ Python APIï¼Œå°†æ™®é€šå‡½æ•°è½¬æ¢ä¸ºåˆ†å¸ƒå¼ä»»åŠ¡ï¼Œå°† Python ç±»è½¬æ¢ä¸ºåˆ†å¸ƒå¼æœåŠ¡ï¼Œä¹Ÿå³ Ray Actorã€‚Ray Actor å†…éƒ¨æŒä¹…å­˜å‚¨çš„æ•°æ®ç§°ä¸ºçŠ¶æ€ï¼Œå¯ä»¥åœ¨ Actor çš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸå†…è¢«å¤šæ¬¡è®¿é—®ã€ä¿®æ”¹å’Œç»´æŠ¤ï¼Œè€Œä¸ä¼šåœ¨æ¯æ¬¡æ–¹æ³•è°ƒç”¨ç»“æŸåæ¶ˆå¤±ã€‚</li><li><strong>å¼¹æ€§ä¼¸ç¼©</strong>ï¼šRay å¯ä»¥å°†åº”ç”¨ä»å•ä¸ªæœºå™¨æ— ç¼æ‰©å±•åˆ°æ‹¥æœ‰æ•°åƒä¸ªèŠ‚ç‚¹çš„é›†ç¾¤ï¼Œå¹¶èƒ½æ ¹æ®éœ€æ±‚è‡ªåŠ¨æ‰©ç¼©å®¹ã€‚</li><li><strong>å®¹é”™æ€§</strong>ï¼šRay å†…ç½®äº†å®¹é”™æœºåˆ¶ï¼Œå¯ä»¥å¤„ç†èŠ‚ç‚¹æ•…éšœå’Œä»»åŠ¡å¤±è´¥ï¼Œç¡®ä¿åº”ç”¨çš„å¥å£®æ€§ã€‚</li><li><strong>æ€§èƒ½ä¼˜åŒ–</strong>ï¼šRay ä¼˜åŒ–äº†åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦ã€å†…å­˜ç®¡ç†å’Œæ•°æ®ä¼ è¾“ï¼Œä»¥å®ç°é«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—ã€‚</li></ol><p>Ray Task å’Œ Ray Actor éƒ½æ˜¯ç”¨äºåˆ†å¸ƒå¼è®¡ç®—çš„æ ¸å¿ƒåŸè¯­ï¼Œä½†å®ƒä»¬å„è‡ªæœåŠ¡äºä¸åŒçš„ç›®çš„ï¼Œä¸»è¦åŒºåˆ«åœ¨äº<strong>æ˜¯å¦ç»´æŠ¤çŠ¶æ€</strong>ã€‚</p><p>Ray Task æ˜¯ Ray ä¸­æœ€åŸºæœ¬çš„è®¡ç®—å•å…ƒï¼Œä»£è¡¨ä¸€ä¸ªæ— çŠ¶æ€çš„è¿œç¨‹å‡½æ•°ã€‚Ray Task çš„æ¯æ¬¡æ‰§è¡Œéƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œä¸ä¿ç•™ä¹‹å‰çš„ä»»ä½•ä¿¡æ¯ã€‚å°±åƒè°ƒç”¨ä¸€ä¸ªæ™®é€šå‡½æ•°ï¼Œæ‰§è¡Œå®Œåå°±æ¸…é™¤å†…éƒ¨çŠ¶æ€ã€‚æˆ‘ä»¬è°ƒç”¨ä¸€ä¸ª Ray Task åï¼Œä¼šç«‹å³è¿”å›å¾—åˆ°ä¸€ä¸ª Ray ObjectRefï¼Œè€Œä¸æ˜¯å®é™…çš„ç»“æœã€‚ä¸»ç¨‹åºå¯ä»¥ç»§ç»­æ‰§è¡Œå…¶ä»–æ“ä½œï¼Œè€Œ Ray Task åˆ™åœ¨åå°å¹¶è¡Œè¿è¡Œã€‚æˆ‘ä»¬éœ€è¦ä½¿ç”¨ <code>ray.get()</code> æ¥è·å– Task çš„å®é™…ç»“æœã€‚ Ray Task éå¸¸é€‚åˆå¹¶è¡Œæ‰§è¡Œå¤§é‡ç‹¬ç«‹ã€ä¸€æ¬¡æ€§çš„è®¡ç®—ä»»åŠ¡ï¼Œè­¬å¦‚æ•°æ®æ‰¹å¤„ç†ã€ç‹¬ç«‹çš„æ¨¡å‹æ¨ç†ç­‰åœºæ™¯ã€‚</p><p>Ray Actor æ˜¯ä¸€ç§ç‰¹æ®Šçš„ Ray Taskï¼Œæ­£å¦‚å‰æ–‡æ‰€è¿°ï¼Œå®ƒæ˜¯ä¸€ä¸ªæŒç»­è¿è¡Œçš„ã€æœ‰è‡ªå·±çš„çŠ¶æ€å’Œæ–¹æ³•çš„è¿œç¨‹å¯¹è±¡ã€‚å½“æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª Ray Actor åï¼ŒRay ä¼šåœ¨é›†ç¾¤ä¸­çš„æŸä¸ª <strong>Ray Worker</strong> ä¸Šå¯åŠ¨ä¸€ä¸ªä¸“é—¨çš„è¿›ç¨‹æ¥æ‰˜ç®¡è¿™ä¸ªå¯¹è±¡ã€‚è¯¥è¿›ç¨‹ä¼šä¸€ç›´è¿è¡Œï¼Œç›´åˆ°è¢«é”€æ¯ã€‚Actor å¯ä»¥ç»´æŠ¤å†…éƒ¨å˜é‡ï¼Œå¹¶ä¸”è¿™äº›å˜é‡åœ¨ Actor çš„ç”Ÿå‘½å‘¨æœŸå†…æ˜¯æŒä¹…å­˜åœ¨çš„ã€‚æ¯æ¬¡è°ƒç”¨ Actor çš„æ–¹æ³•ï¼Œéƒ½å¯ä»¥è®¿é—®å’Œä¿®æ”¹è¿™äº›çŠ¶æ€ã€‚è¿™ä¸æ™®é€šçš„ Ray Task ä¸åŒï¼Œæ™®é€š Task æ‰§è¡Œå®Œä¼šæ¸…é™¤å†…éƒ¨çŠ¶æ€ã€‚Ray Actor æ”¯æŒå¹¶å‘è¯·æ±‚ï¼ŒRay ä¼šè´Ÿè´£å°†è¿™äº›è¯·æ±‚åºåˆ—åŒ–æ‰§è¡Œï¼Œä¿è¯ Actor å†…éƒ¨çŠ¶æ€çš„ä¸€è‡´æ€§å’Œçº¿ç¨‹å®‰å…¨ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ <code>@ray.remote</code> è£…é¥°å™¨å°†ä¸€ä¸ª Python ç±»è½¬æ¢ä¸ºä¸€ä¸ª Ray Actor ç±»ï¼Œç„¶åé€šè¿‡ <code>.remote()</code> æ–¹æ³•å®ä¾‹åŒ–ä¸€ä¸ªè¿œç¨‹ Actorã€‚</p><p>æœ€åï¼ŒRay Worker æ˜¯ Ray é›†ç¾¤ä¸­çœŸæ­£æ‰§è¡Œä»£ç çš„å·¥ä½œå•å…ƒã€‚ä¸€ä¸ª Ray é›†ç¾¤é€šå¸¸ç”±ä¸€ä¸ª Head Node å’Œå¤šä¸ª Worker Nodes ç»„æˆã€‚æ¯ä¸ªèŠ‚ç‚¹ä¸Šéƒ½ä¼šè¿è¡Œä¸€ä¸ªæˆ–å¤šä¸ª Ray Worker è¿›ç¨‹ã€‚æ— è®ºæ˜¯æ™®é€šçš„ Ray Task è¿˜æ˜¯ Ray Actor çš„æ–¹æ³•ï¼Œæœ€ç»ˆéƒ½æ˜¯ç”± Ray Worker è¿›ç¨‹æ¥æ‰§è¡Œçš„ã€‚æ¯ä¸ª Ray Worker éƒ½ä¼šè¢«åˆ†é…ä¸€å®šçš„è®¡ç®—èµ„æºï¼ˆå¦‚ CPUã€GPUï¼‰ã€‚å½“ä½ æäº¤ä¸€ä¸ª Ray Task æˆ–åˆ›å»ºä¸€ä¸ª Ray Actor æ—¶ï¼ŒRay çš„è°ƒåº¦å™¨ä¼šæ‰¾åˆ°ä¸€ä¸ªæœ‰è¶³å¤Ÿèµ„æºçš„ Worker æ¥è¿è¡Œå®ƒã€‚Worker è¿›ç¨‹ä¹‹é—´ä»¥åŠ Worker è¿›ç¨‹ä¸å¤´èŠ‚ç‚¹ä¹‹é—´ä¼šè¿›è¡Œé€šä¿¡ï¼Œä»¥åè°ƒä»»åŠ¡æ‰§è¡Œã€ä¼ è¾“æ•°æ®å’Œç®¡ç†çŠ¶æ€ã€‚ä¸€ä¸ª Ray Worker é€šå¸¸å°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ Python è¿›ç¨‹ã€‚å¯¹äºæ™®é€šçš„ Ray Taskï¼ŒRay Worker ç›¸å½“äºå‡½æ•°è§£é‡Šå™¨ï¼Œæ‰§è¡Œå®Œä»»åŠ¡åå¯èƒ½ä¼šè¢«å¤ç”¨å»æ‰§è¡Œå…¶ä»–ä»»åŠ¡ã€‚è€Œå¯¹äº Ray Actorï¼ŒRay ä¼šå¯åŠ¨ä¸€ä¸ªä¸“é—¨çš„ Worker è¿›ç¨‹æ¥æ‰˜ç®¡è¿™ä¸ª Actorï¼Œè¿™ä¸ª Worker è¿›ç¨‹çš„ç”Ÿå‘½å‘¨æœŸä¸ Actor çš„ç”Ÿå‘½å‘¨æœŸç»‘å®šã€‚</p><h3 id=run_ppo-å’Œ-taskrunnerrun><code>run_ppo()</code> å’Œ <code>TaskRunner.run()</code><a hidden class=anchor aria-hidden=true href=#run_ppo-å’Œ-taskrunnerrun>#</a></h3><p>æœ‰äº† ray çš„æ¦‚å¿µï¼Œæˆ‘ä»¬å›åˆ°æ•´ä¸ª RL è®­ç»ƒæµç¨‹çš„èµ·ç‚¹ï¼š<code>verl.trainer.main_ppo.py</code> ä¸­çš„ <a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L35><code>run_ppo()</code></a>ï¼Œå®ƒè´Ÿè´£åˆå§‹åŒ– Ray é›†ç¾¤ï¼Œé…ç½® CPU èµ„æºå’Œè¿è¡Œæ—¶ç¯å¢ƒå˜é‡ï¼Œå¹¶åˆ›å»ºè¿œç¨‹ TaskRunner å®ä¾‹ã€‚</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>run_ppo</span><span class=p>(</span><span class=n>config</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># åˆå§‹åŒ– Ray é›†ç¾¤ï¼Œé…ç½® CPU èµ„æºå’Œè¿è¡Œæ—¶ç¯å¢ƒå˜é‡</span>
</span></span><span class=line><span class=cl>    <span class=n>ray</span><span class=o>.</span><span class=n>init</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>runtime_env</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;env_vars&#34;</span><span class=p>:</span> <span class=p>{</span><span class=o>...</span><span class=p>}},</span>
</span></span><span class=line><span class=cl>        <span class=n>num_cpus</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>ray_init</span><span class=o>.</span><span class=n>num_cpus</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># åˆ›å»ºè¿œç¨‹ TaskRunner å®ä¾‹</span>
</span></span><span class=line><span class=cl>    <span class=c1># TaskRunner æ˜¯ Ray ä¸­çš„ä¸€ä¸ªè¿œç¨‹ actorï¼Œå®ƒå°†åœ¨ Ray é›†ç¾¤ä¸Šå¼‚æ­¥æ‰§è¡Œä¸»è¦çš„è®­ç»ƒä»»åŠ¡</span>
</span></span><span class=line><span class=cl>    <span class=n>runner</span> <span class=o>=</span> <span class=n>TaskRunner</span><span class=o>.</span><span class=n>remote</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=c1># å¼‚æ­¥æ‰§è¡Œè¿œç¨‹ä»»åŠ¡ runner.run()ï¼Œå¹¶ç­‰å¾…å…¶å®Œæˆ</span>
</span></span><span class=line><span class=cl>    <span class=c1># é€šè¿‡ ray.get() é˜»å¡ç›´åˆ°è¿œç¨‹ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œç¡®ä¿æ•´ä¸ªåˆå§‹åŒ–æµç¨‹çš„é¡ºåºæ€§</span>
</span></span><span class=line><span class=cl>    <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>runner</span><span class=o>.</span><span class=n>run</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>config</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=actorrolloutrefworker-å’Œ-rayworkergroup-çš„ç›¸äº’å…³ç³»>ActorRolloutRefWorker å’Œ RayWorkerGroup çš„ç›¸äº’å…³ç³»<a hidden class=anchor aria-hidden=true href=#actorrolloutrefworker-å’Œ-rayworkergroup-çš„ç›¸äº’å…³ç³»>#</a></h3><p><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L64>TaskRunner</a> æ˜¯ verl ä¸­å®ç° PPO/GRPO è®­ç»ƒçš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒé€šè¿‡å°†æ•´ä¸ª RL è®­ç»ƒæµç¨‹å°è£…åœ¨ä¸€ä¸ªç‹¬ç«‹çš„ Ray Actor ä¸­ï¼Œå®ç°äº†ä»»åŠ¡çš„å°è£…ã€èµ„æºéš”ç¦»å’Œåˆ†å¸ƒå¼åè°ƒã€‚ä¸ºäº†è§£é‡Šæ¸…æ¥š <code>TaskRunner</code>ï¼Œæˆ‘ä»¬å°† verl å½“ä¸­æœ€è®©äººè´¹è§£ä¸”æœ€å¤æ‚çš„ <code>ActorRolloutRefWorker</code> å’Œ <code>RayWorkerGroup</code> è¿™ä¸¤ä¸ªç±»æå‰è§£é‡Šæ¸…æ¥šã€‚</p><p>æˆ‘ä»¬å…ˆä¸è®¨è®ºè¿™ä¸¤ä¸ªç±»åŠå…¶åŸºç±»çš„å…·ä½“æ„ä¹‰ï¼Œå…ˆè®¨è®ºæ¸…æ¥šå…¶å®ä¾‹å¯¹è±¡çš„åˆ›å»ºè¿‡ç¨‹ã€‚æˆ‘ä»¬æ³¨æ„åˆ°è¿™æ®µ <code>TaskRunner</code> çš„åˆå§‹åŒ–ä¸­å¼•å…¥ <code>ActorRolloutRefWorker</code> å’Œ <code>RayWorkerGroup</code> çš„ç›¸å…³ä»£ç ï¼š</p><details><summary>TaskRunner ä¸­å¼•å…¥ ActorRolloutRefWorker</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Define worker classes based on the actor strategy.</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;fsdp&#34;</span><span class=p>,</span> <span class=s2>&#34;fsdp2&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>config</span><span class=o>.</span><span class=n>critic</span><span class=o>.</span><span class=n>strategy</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;fsdp&#34;</span><span class=p>,</span> <span class=s2>&#34;fsdp2&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>verl.single_controller.ray</span> <span class=kn>import</span> <span class=n>RayWorkerGroup</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>verl.workers.fsdp_workers</span> <span class=kn>import</span> <span class=n>ActorRolloutRefWorker</span><span class=p>,</span> <span class=n>AsyncActorRolloutRefWorker</span><span class=p>,</span> <span class=n>CriticWorker</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>actor_rollout_cls</span> <span class=o>=</span> <span class=n>AsyncActorRolloutRefWorker</span> <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>mode</span> <span class=o>==</span> <span class=s2>&#34;async&#34;</span> <span class=k>else</span> <span class=n>ActorRolloutRefWorker</span>
</span></span><span class=line><span class=cl>    <span class=n>ray_worker_group_cls</span> <span class=o>=</span> <span class=n>RayWorkerGroup</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>elif</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span> <span class=o>==</span> <span class=s2>&#34;megatron&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span> <span class=o>==</span> <span class=n>config</span><span class=o>.</span><span class=n>critic</span><span class=o>.</span><span class=n>strategy</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>verl.single_controller.ray.megatron</span> <span class=kn>import</span> <span class=n>NVMegatronRayWorkerGroup</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>verl.workers.megatron_workers</span> <span class=kn>import</span> <span class=n>ActorRolloutRefWorker</span><span class=p>,</span> <span class=n>AsyncActorRolloutRefWorker</span><span class=p>,</span> <span class=n>CriticWorker</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>actor_rollout_cls</span> <span class=o>=</span> <span class=n>AsyncActorRolloutRefWorker</span> <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>mode</span> <span class=o>==</span> <span class=s2>&#34;async&#34;</span> <span class=k>else</span> <span class=n>ActorRolloutRefWorker</span>
</span></span><span class=line><span class=cl>    <span class=n>ray_worker_group_cls</span> <span class=o>=</span> <span class=n>NVMegatronRayWorkerGroup</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>raise</span> <span class=ne>NotImplementedError</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>verl.trainer.ppo.ray_trainer</span> <span class=kn>import</span> <span class=n>ResourcePoolManager</span><span class=p>,</span> <span class=n>Role</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Map roles to their corresponding remote worker classes.</span>
</span></span><span class=line><span class=cl><span class=n>role_worker_mapping</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>:</span> <span class=n>ray</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>actor_rollout_cls</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>Role</span><span class=o>.</span><span class=n>Critic</span><span class=p>:</span> <span class=n>ray</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>CriticWorker</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define the resource pool specification.</span>
</span></span><span class=line><span class=cl><span class=c1># Map roles to the resource pool.</span>
</span></span><span class=line><span class=cl><span class=n>global_pool_id</span> <span class=o>=</span> <span class=s2>&#34;global_pool&#34;</span>
</span></span><span class=line><span class=cl><span class=n>resource_pool_spec</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>global_pool_id</span><span class=p>:</span> <span class=p>[</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=o>.</span><span class=n>n_gpus_per_node</span><span class=p>]</span> <span class=o>*</span> <span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=o>.</span><span class=n>nnodes</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>mapping</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>:</span> <span class=n>global_pool_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>Role</span><span class=o>.</span><span class=n>Critic</span><span class=p>:</span> <span class=n>global_pool_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div></details><p>å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œåœ¨ <code>TaskRunner</code> çš„åˆå§‹åŒ–ä¸­ï¼Œä¼šæ ¹æ®å„ç±»é…ç½®å¼•å…¥å¯¹åº”çš„ <code>ActorRolloutRefWorker / AsyncActorRolloutRefWorker</code> ç±»ä»¥åŠ <code>RayWorkerGroup / NVMegatronRayWorkerGroup</code> ç±»ã€‚å¯¹äº SGLang è€Œè¨€ï¼Œä¸å­˜åœ¨ <code>AsyncActorRolloutRefWorker</code>ã€‚<code>ActorRolloutRefWorker</code> ç±»ç›´æ¥é€šè¿‡ <code>ray.remote(ActorRolloutRefWorker)</code> åˆ›å»ºä¸€ä¸ªè¿œç¨‹çš„ Ray Actorï¼Œå°†å…¶åŒ…è£…æˆä¸€ä¸ª Ray Actor ç±»ã€‚æ­¤æ—¶è¿˜è¿˜æ²¡æœ‰åˆ›å»ºä»»ä½•å®ä¾‹ï¼Œä¹Ÿæ²¡æœ‰åˆ†é…èµ„æºã€‚é‚£ä¹ˆï¼Œ<code>ActorRolloutRefWorker</code> ç±»åˆ°åº•åœ¨å“ªå„¿å®ä¾‹åŒ–å¹¶åˆ†é…èµ„æºçš„å‘¢ï¼Ÿ</p><p>å®é™…ä¸Šï¼Œåœ¨ <code>main_ppo.py</code> çš„ <a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L172>172 è¡Œ</a>ï¼Œæ„é€ äº† <code>RayPPOTrainer</code> ç±»ï¼Œéšåè°ƒç”¨äº† <code>RayPPOTrainer.init_workers()</code> æ–¹æ³•ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æŸ¥çœ‹ <code>RayPPOTrainer.init_workers()</code> æ–¹æ³•çš„<a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L715>ç›¸å…³ä»£ç </a>ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæ¯ä¸€ä¸ª RL worker ç±»ï¼ˆæ¯”å¦‚ ActorRolloutRefWorkerï¼‰éƒ½ä¼šåˆ›é€ ä¸€ä¸ª work groupï¼ˆverl ä¸­çš„å„ç§ wg å˜é‡ï¼‰ï¼Œéšåè°ƒç”¨æ¯ä¸ª worker group çš„ <code>init_model()</code> æ–¹æ³•ï¼Œè€Œè¿™äº› worker group å®é™…ä¸Šéƒ½æ˜¯ <code>RayWorkerGroup</code> çš„å®ä¾‹ã€‚<code>RayWorkerGroup</code> çš„æ ¸å¿ƒä½œç”¨æ˜¯èµ„æºè°ƒåº¦çš„æ ¸å¿ƒä¸­é—´å±‚ï¼Œç»Ÿä¸€äº†å„ç§ RL workerï¼ˆæ¯”å¦‚ ActorRolloutRefWorkerã€CriticWorkerï¼‰çš„æ¥å£ï¼Œè¿›è¡Œç»Ÿä¸€ç®¡ç†ï¼š</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># RayWorkerGroup å®ä¾‹ï¼ŒæŒ‡å®šèµ„æºæ±  å¹¶è§„å®šè§’è‰²å’Œå¯¹åº”çš„ç±»</span>
</span></span><span class=line><span class=cl><span class=n>wg_dict</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ray_worker_group_cls</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>resource_pool</span><span class=o>=</span><span class=n>resource_pool</span><span class=p>,</span>  <span class=c1># åªéœ€è¦æŒ‡å®šèµ„æºæ± </span>
</span></span><span class=line><span class=cl>    <span class=n>ray_cls_with_init</span><span class=o>=</span><span class=n>worker_dict_cls</span><span class=p>,</span>  <span class=c1># ä¸€ä¸ªåŒ…å«æ•°ä¸ªworkerçš„ç±» ï¼ˆe.g. actor_rollï¼Œ critic, refï¼‰</span>
</span></span><span class=line><span class=cl>    <span class=n>device_name</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_name</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#é€šè¿‡.spawn()è·å–è§’è‰²å¯¹Ray Actorå®ä¾‹çš„æ˜ å°„</span>
</span></span><span class=line><span class=cl><span class=n>wg_dict</span><span class=o>.</span><span class=n>spawn</span><span class=p>(</span><span class=n>prefix_set</span><span class=o>=</span><span class=n>class_dict</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># æ‰€æœ‰ worker éƒ½é€šè¿‡ç›¸åŒçš„æ¨¡å¼åˆ›å»ºï¼Œæˆ‘è¿™é‡Œè¿›è¡Œç®€åŒ–ï¼Œå®é™…ä¸Šçš„ä»£ç æ¯”è¾ƒç¹ç</span>
</span></span><span class=line><span class=cl><span class=n>actor_rollout_wg</span> <span class=o>=</span> <span class=n>RayWorkerGroup</span><span class=p>(</span><span class=n>resource_pool</span><span class=p>,</span> <span class=n>actor_rollout_cls</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>critic_wg</span> <span class=o>=</span> <span class=n>RayWorkerGroup</span><span class=p>(</span><span class=n>resource_pool</span><span class=p>,</span> <span class=n>critic_cls</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ref_policy_wg</span> <span class=o>=</span> <span class=n>RayWorkerGroup</span><span class=p>(</span><span class=n>resource_pool</span><span class=p>,</span> <span class=n>ref_policy_cls</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><details><summary>å„ç§ worker group å®é™…ä¸Šçš„åˆå§‹åŒ–</summary><p>è¿™éƒ¨åˆ†ä»£ç åœ¨ <a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L771><code>ray_trainer.py</code></a> ä¸­ï¼š</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 1. ä¸ºæ¯ä¸ªè§’è‰²ï¼ˆä¾‹å¦‚ actor_rolloutã€criticã€refï¼‰æŒ‡å®šç”¨å“ªä¸ªç±»åˆå§‹åŒ– workerï¼Œå¹¶ä¸”è¯´æ˜åœ¨å“ªä¸ªèµ„æºæ± é‡Œåˆ†é…å®ƒä»¬</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>create_resource_pool</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span> <span class=o>=</span> <span class=p>{</span><span class=n>pool</span><span class=p>:</span> <span class=p>{}</span> <span class=k>for</span> <span class=n>pool</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>resource_pool_dict</span><span class=o>.</span><span class=n>values</span><span class=p>()}</span>
</span></span><span class=line><span class=cl><span class=n>resource_pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>get_resource_pool</span><span class=p>(</span><span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>actor_rollout_cls</span> <span class=o>=</span> <span class=n>RayClassWithInitArgs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>cls</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>role</span><span class=o>=</span><span class=s2>&#34;actor_rollout&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=p>[</span><span class=n>resource_pool</span><span class=p>][</span><span class=s2>&#34;actor_rollout&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>actor_rollout_cls</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=c1># 2. æ ¹æ®èµ„æºæ± å’Œè§’è‰²ï¼Œæ‰¹é‡åˆ›å»ºå¤šä¸ª worker å®ä¾‹ï¼ˆRay Actorï¼‰å¹¶ç»Ÿä¸€ç®¡ç†å®ƒä»¬ï¼Œèµ‹äºˆå¯¹åº”çš„èŒè´£</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>resource_pool</span><span class=p>,</span> <span class=n>class_dict</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>worker_dict_cls</span> <span class=o>=</span> <span class=n>create_colocated_worker_cls</span><span class=p>(</span><span class=n>class_dict</span><span class=o>=</span><span class=n>class_dict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>wg_dict</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ray_worker_group_cls</span><span class=p>(</span><span class=n>resource_pool</span><span class=o>=</span><span class=n>resource_pool</span><span class=p>,</span> <span class=n>ray_cls_with_init</span><span class=o>=</span><span class=n>worker_dict_cls</span><span class=p>,</span> <span class=n>device_name</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_name</span><span class=p>,</span> <span class=o>**</span><span class=n>wg_kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>spawn_wg</span> <span class=o>=</span> <span class=n>wg_dict</span><span class=o>.</span><span class=n>spawn</span><span class=p>(</span><span class=n>prefix_set</span><span class=o>=</span><span class=n>class_dict</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=n>all_wg</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>spawn_wg</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=c1># 3.è°ƒç”¨ init_model() å®Œæˆæ¨¡å‹åŠ è½½</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_critic</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>critic_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;critic&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>critic_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_reference_policy</span> <span class=ow>and</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>ref_in_actor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>ref_policy_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>ref_policy_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_rm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>rm_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;rm&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>rm_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># we should create rollout at the end so that vllm can have a better estimation of kv cache memory</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>actor_rollout_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;actor_rollout&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>actor_rollout_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># create async rollout manager and request scheduler</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>async_rollout_mode</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>mode</span> <span class=o>==</span> <span class=s2>&#34;async&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>verl.workers.rollout.async_server</span> <span class=kn>import</span> <span class=n>AsyncLLMServerManager</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>async_rollout_mode</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>async_rollout_manager</span> <span class=o>=</span> <span class=n>AsyncLLMServerManager</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>worker_group</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>actor_rollout_wg</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>æ³¨æ„åˆ° <code>ray_worker_group_cls</code> å°±æ˜¯ <code>RayWorkerGroup</code> ç±»ï¼Œè€Œ <code>worker_dict_cls</code> å°±æ˜¯ <code>ActorRolloutRefWorker</code> ç±»ï¼Œæ‰€ä»¥æˆ‘çš„ç®€åŒ–æ˜¯å¾ˆåˆç†çš„ã€‚</p></details><p>å¦‚æ­¤ä»¥æ¥ï¼Œ<code>ActorRolloutRefWorker</code> å§”æ‰˜ç»™ <code>RayWorkerGroup</code> è¿›è¡Œåˆå§‹åŒ–ã€‚<code>RayWorkerGroup</code> è¿™ä¸ªç±»å°±æ˜¯ä¸“é—¨ç”¨äºèµ„æºè°ƒåº¦çš„ã€‚é€šè¿‡å…¶ç»Ÿä¸€çš„ <code>_init_with_resource_pool</code> <a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/single_controller/ray/base.py#L313>æ–¹æ³•</a>ï¼Œä¸ºæ¯ä¸ª GPU åˆ›å»ºä¸€ä¸ª workerï¼Œæœ€ç»ˆå®ä¾‹åŒ–æ¯ç§ RL worker å¹¶åˆ†é…èµ„æºã€‚</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_init_with_resource_pool</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>resource_pool</span><span class=p>,</span> <span class=n>ray_cls_with_init</span><span class=p>,</span> <span class=o>...</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># ä» Ray ç”³è¯· Placement Groups</span>
</span></span><span class=line><span class=cl>    <span class=n>pgs</span> <span class=o>=</span> <span class=n>resource_pool</span><span class=o>.</span><span class=n>get_placement_groups</span><span class=p>(</span><span class=n>strategy</span><span class=o>=</span><span class=n>strategy</span><span class=p>,</span> <span class=n>device_name</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># ä¸ºæ¯ä¸ª GPU åˆ›å»ºä¸€ä¸ª worker</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>local_rank</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>local_world_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>worker</span> <span class=o>=</span> <span class=n>ray_cls_with_init</span><span class=p>(</span><span class=n>placement_group</span><span class=o>=</span><span class=n>pg</span><span class=p>,</span> <span class=n>placement_group_bundle_idx</span><span class=o>=</span><span class=n>local_rank</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_workers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>worker</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>è¯»åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬åŸºæœ¬å¯¹ verl æœ‰äº†ä¸€äº›æ„Ÿè§‰ã€‚æ³¨æ„åˆ°ï¼Œåœ¨ verl å½“ä¸­æœ‰ä¸¤ä¸ªå¸¦æœ‰ Worker çš„ base classï¼Œä¸€ä¸ªå°±å«åš <a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/single_controller/base/worker.py#L77><code>Worker</code></a>ï¼Œå¦ä¸€ä¸ªå«åš <a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/single_controller/base/worker_group.py#L121><code>WorkerGroup</code></a>ã€‚<code>Worker</code> æ˜¯ RL é‡Œé¢çš„é€»è¾‘ç±»ï¼ˆæ¯”å¦‚ actor å’Œ criticï¼‰,å®é™…ç®¡ç† RL çš„æ•°æ®æµï¼Œè€Œ <code>WorkerGroup</code> åªç”¨äºåˆ†å¸ƒå¼ç³»ç»Ÿçš„èµ„æºè°ƒåº¦ã€‚</p><p>æ­¤å¤–ï¼Œä» <code>actor_rollout_wg</code> å’Œ <code>ref_policy_wg</code> çš„å®ä¾‹åŒ–å½“ä¸­ï¼Œä¹Ÿèƒ½çœ‹å‡ºä¸€äº›å­¦é—®ã€‚åœ¨ <code>ActorRolloutRefWorker</code> çš„è®¾è®¡å½“ä¸­ï¼ŒActor Trainingï¼ŒActor Rollout å’Œ Reference model æ˜¯ç”¨åŒä¸€ä¸ª worker class è¿›è¡Œç®¡ç†çš„ã€‚ä½†æ˜¯ï¼Œä¹‹åå§”æ‰˜ç»™ <code>RayWorkerGroup</code> åˆ›å»º worker group å¹¶ä¸”è°ƒç”¨èµ„æºçš„æ—¶å€™ï¼ŒActor Training å’Œ Actor Rollout æ˜¯ç”±åŒä¸€ç»„ <code>RayWorkerGroup</code> è¿›è¡Œèµ„æºç®¡ç†çš„ï¼ˆè¿™äºŒè€…æœ¬æ¥å°±è¦è¢«æ”¾åœ¨åŒä¸€ä¸ªèµ„æºç»„ä¸Šåš hybird engineï¼‰ï¼Œè€Œ Reference Model æ˜¯ç”±å¦ä¸€ç»„ <code>RayWorkerGroup</code> ç®¡ç†èµ„æºçš„ã€‚</p><p>æœ€åï¼Œæˆ‘å»é—®äº†ç›¸å…³å¼€å‘è€…ï¼Œä»–ä»¬ä¹Ÿè®¤ä¸ºæŠŠ Actor Rolloutï¼ŒActor Training å’Œ Reference Model æ”¾åœ¨åŒä¸€ä¸ª worker é‡Œæ˜¯ bad design ğŸ˜‚ï¼Œä¸ç”¨çº ç»“è¿™ç§è®¾è®¡æ˜¯å¦æœ‰ä»€ä¹ˆé«˜ç»è¿œç©ï¼Œå®Œå…¨æ²¡æœ‰ã€‚</p><h3 id=actorrolloutrefworker__init__><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/workers/fsdp_workers.py#L101><code>ActorRolloutRefWorker.__init__()</code></a><a hidden class=anchor aria-hidden=true href=#actorrolloutrefworker__init__>#</a></h3><p>å¦‚å‰æ–‡æ‰€è¯´ï¼Œ<code>ActorRolloutRefWorker</code> æ˜¯ verl ä¸­ç”¨äºç®¡ç† Actor Trainingï¼ŒActor Rollout å’Œ Reference Model çš„ worker classã€‚æˆ‘ä»¬å…·ä½“æ¥åˆ†æå…¶é€»è¾‘ä¸Šå®ç°çš„åŠŸèƒ½ã€‚æ³¨æ„ï¼Œæœ¬æ–‡æ¡£åªåˆ†æ FSDP backend ä¸‹çš„å®ç°ï¼Œmegatron ç•™ä½œåæ–‡ã€‚</p><ol><li>è°ƒç”¨ Worker åŸºç±»çš„æ„é€ å‡½æ•°ï¼Œå¹¶ä¿å­˜é…ç½®ã€‚</li><li>å¦‚æœ PyTorch åˆ†å¸ƒå¼ç¯å¢ƒå°šæœªåˆå§‹åŒ–ï¼Œåˆ™è¿›è¡Œåˆå§‹åŒ–ï¼ŒåŒ…æ‹¬è®¾ç½®é€šä¿¡åç«¯å’Œè¿›ç¨‹ç»„ã€‚</li><li>ä¸º FSDP åˆ›å»ºè®¾å¤‡ç½‘æ ¼ï¼Œç”¨äºæ¨¡å‹å‚æ•°çš„åˆ†ç‰‡ã€‚</li><li>å¦‚æœå¯ç”¨ Ulysses åºåˆ—å¹¶è¡Œï¼Œåˆ™åˆå§‹åŒ–å…¶è®¾å¤‡ç½‘æ ¼ã€‚</li><li>æ ¹æ®ä¼ å…¥çš„ <code>role</code> å‚æ•°è®¾ç½® Worker çš„å…·ä½“è§’è‰²ï¼ˆactor, rollout, refï¼‰ã€‚</li><li>æ ¹æ® Worker è§’è‰²é…ç½® profilerï¼Œç”¨äºæ€§èƒ½åˆ†æã€‚</li><li>é…ç½® parameter offload å’Œ optimizer offloadã€‚</li><li>ä¸º Actorï¼ŒRollout å’Œ Reference åˆ†åˆ« normalize batch sizeã€‚</li></ol><p>ç¬¬ 8 æ­¥ä¸­é…ç½®äº†éå¸¸å¤šçš„ batch sizeï¼›verl çš„ batch size å‚æ•°æ»¡å¤©é£ï¼Œè™½ç„¶æˆ‘ä¸ªäººè®¤ä¸ºåå­—åŸºæœ¬æ˜¯å‡†ç¡®çš„ï¼Œä½†æ˜¯ç”±äºåå­—å¤ªåƒäº†ï¼Œä¸€å®šè¦åšå‡ºä¸€äº›åŒºåˆ†ã€‚äº‹å®ä¸Šï¼Œå‚æ•°åˆ†ææˆ‘ä»¬æœ‰å•ç‹¬çš„æ–‡æ¡£ï¼Œæˆ‘å…ˆæŠŠä¸€éƒ¨åˆ†å†…å®¹æå‰å…¬å¸ƒäº†ã€‚</p><ol><li><code>data.train_batch_size</code>ï¼šåœ¨ä¸€æ¬¡å®Œæ•´çš„ PPO è¿­ä»£ï¼ˆä» rollout åˆ° trainï¼‰ä¸­ï¼Œä»æ•°æ®é›†ä¸­é‡‡æ ·å¹¶ç”¨äºç”Ÿæˆ experience çš„æ€»æ ·æœ¬æ•°é‡ï¼Œå†³å®šäº†æ¯æ¬¡ policy æ›´æ–°æ‰€ä¾æ®çš„æ•°æ®é‡ã€‚</li><li><code>actor_rollout_ref.actor.ppo_mini_batch_size</code>ï¼šè¿™ä¸ªå‚æ•°çš„åå­—å…¶å®æ˜¯å‡†ç¡®çš„ï¼Œå› ä¸º mini batch SGD å°±æ˜¯æ•°æ®åˆ°è¾¾äº†ä¸€ä¸ª mini batch å°±æ›´æ–°ä¸€æ¬¡æ¨¡å‹å‚æ•°ã€‚åœ¨ verl ä¸­ï¼Œæ¨¡å‹ä¼šåœ¨æ•°æ®ç´¯ç§¯åˆ°ä¸€ä¸ª mini batch åæ›´æ–°ä¸€æ¬¡å‚æ•°ã€‚</li><li><code>actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu</code>ï¼šè¿™é‡Œå…¶å®æ˜¯ gradient accumulation çš„å‚æ•°ã€‚ç”±äºä¸€ä¸ª mini batch çš„æ•°æ®é‡å¯èƒ½ä»ç„¶å¤ªå¤§ï¼Œæ— æ³•ä¸€æ¬¡æ€§å‰å‘å’Œåå‘ä¼ æ’­ï¼Œå› æ­¤éœ€è¦å°†å…¶è¿›ä¸€æ­¥æ‹†åˆ†ä¸º micro batchã€‚æ¯ä¸ª micro batch ä¼šè®¡ç®—ä¸€æ¬¡æ¢¯åº¦å¹¶ä¸”ç´¯è®¡ï¼Œä½†æ˜¯ä¸ä¼šç«‹åˆ»æ›´æ–°æ¨¡å‹å‚æ•°ã€‚å¤„ç†å®Œæ•´ä¸ª mini batch åï¼Œæ‰ç”¨ç´¯ç§¯çš„æ¢¯åº¦è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°ã€‚</li></ol><p>æ­¤å¤–ï¼Œåœ¨ verl ä¸­ï¼Œç”±äº verl å¼ºè°ƒ SPMD ç­–ç•¥ï¼Œå¯ä»¥ç†è§£ä¸ºæ¯ä¸ª RL worker æ‰€å æ®çš„æ¯ä¸ª GPU ä¸Šå¸Œæœ›è¿›è¡Œå®Œå…¨ä¸€è‡´çš„æ“ä½œï¼Œæ‰€ä»¥ verl ä¼šè¦æ±‚æ¯ä¸ª GPU çš„ micro batch size ç›¸åŒã€‚å› æ­¤ï¼Œverl ä¼šæ£€æŸ¥ train batch size / gpu æ˜¯å¦æ•´é™¤ <a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/trainer/ppo/ray_trainer.py#L363>(ref)</a>ï¼Œå¦‚æœä¸æ•´é™¤ï¼Œåˆ™æŠ¥é”™ã€‚è¿™ä¸ªè®¾å®šå…¶å®å®Œå…¨æ²¡å¿…è¦ï¼›å¯¹äº rollout è€Œè¨€ï¼ŒSGLang å®Œå…¨ä¸éœ€è¦å‘é€çš„è¯·æ±‚æ•°é‡æ•´é™¤ DP æˆ–è€… TP sizeï¼Œæ›´ä½•å†µç›´æ¥è¦æ•´é™¤ gpu æ•°é‡å‘¢ï¼Ÿä½†æ˜¯ï¼Œå› ä¸º verl ä¼šç”¨ all gather ä» rollout çš„æ¯ä¸ª worker é‡Œæ”¶é›†æ•°æ®ï¼Œè¿™å°±è¦æ±‚ rollout çš„æ¯ä¸ª worker ä¸Šåˆ†åˆ°çš„æ•°æ®ä¸€è‡´ã€‚æ›´è¿›ä¸€æ­¥ï¼Œä¸ºäº† SPMDï¼Œåˆè¦æ±‚ rollout çš„æ¯ä¸ª gpu ä¸Šåˆ†åˆ°çš„æ•°æ®ä¸€è‡´ã€‚æœ€ç»ˆï¼Œè¿™å°±å¯¼è‡´ verl çš„ train batch size å¿…é¡»æ•´é™¤ gpu æ•°é‡ï¼›åœ¨ GRPO ä¸‹æ˜¯ real train batch size éœ€è¦æ•´é™¤ n gpusï¼Œç­‰äº train batch size * sampling params ä¸­çš„ nã€‚</p><p>åŒºåˆ†å¥½ mini batch å’Œ micro batch åï¼Œæˆ‘ä¹Ÿæ˜¯æœ€è¿‘æ‰æ˜ç™½ PPO ä¸­æ˜¯å¦‚ä½•ç»´æŠ¤ on policy çš„ã€‚æˆ‘ä¹‹å‰ä¸€ç›´ä»¥ä¸ºæˆ‘ä»¬éƒ½æ˜¯åœ¨åšä¸¥æ ¼ on policy çš„è®­ç»ƒï¼Œä½†æ˜¯ä¸€ä¸ª train batch size ä¸‹æœ‰å¥½å‡ ä¸ª mini batchï¼Œä¼¼ä¹ç¬¬ä¸€ä¸ª mini batch ç»“æŸä¹‹åï¼Œç›®æ ‡ç­–ç•¥ï¼ˆtarget policyï¼Œè¢«è®­ç»ƒçš„ policyï¼‰å’Œè¡Œä¸ºç­–ç•¥ï¼ˆbehavior policyï¼Œç”¨äºåœ¨ç¯å¢ƒä¸­é‡‡æ ·çš„ policyï¼‰å°±ä¸ä¸€è‡´äº†ã€‚ä¸€æ¬¡é‡‡æ ·ä¼šè®­ç»ƒå¾ˆå¤šä¸ª mini batchï¼Œä»ç¬¬ä¸€ä¸ª mini batch ç»“æŸå°±ä¸æ˜¯ on policy äº†ã€‚äº‹å®ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œæˆ‘ä»¬æ³¨æ„åˆ° PPO çš„ loss functionï¼š</p><p>$$ L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t) \right] $$</p><p>å…¶ä¸­çš„ $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)}$ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹ä¼˜åŠ¿å‡½æ•°çš„çŸ«æ­£æ¯”ä¾‹ï¼Œè€Œ $\hat{A}<em>t$ å°±æ˜¯ advantageã€‚å¯¹äº LLM çš„ PPO è€Œè¨€ï¼Œ$\pi</em>{\theta_{old}}(a_t | s_t)$ ä»£è¡¨ç€é‡‡æ ·æ—¶ behavior policy åœ¨ç»™å®š $s_t$ æ—¶ï¼Œé€‰æ‹© $a_t$ çš„æ¦‚ç‡ï¼Œè€Œ $\pi_\theta(a_t | s_t)$ å°±æ˜¯ target policy åœ¨è®­ç»ƒä¸­çš„æ¯ä¸€æ­¥ç»™å®š $s_t$ æ—¶ï¼Œé€‰æ‹© $a_t$ çš„æ¦‚ç‡ã€‚å¯¹ LLM è€Œè¨€ï¼Œ<code>s_t</code> æ˜¯ prompt å‰ç¼€ï¼Œè€Œ <code>a_t</code> ä»…ä»…æ˜¯ prompt åçš„é‚£ä¸€ä¸ª tokenã€‚è¿™ä¸€æ¦‚ç‡å…¶å®å°±æ˜¯ inference å¾—åˆ°çš„ log probsï¼›æˆ‘ä»¬å°†æ”¶é›†å¾—åˆ°çš„ (prompt, action) åˆ†åˆ«ç»è¿‡ target policy å’Œ behaviour policy å¾—åˆ° log probsï¼Œç„¶åäºŒè€… log probs ç›¸å‡å†å–å¯¹æ•°ï¼Œå°±æ˜¯çŸ«æ­£é¡¹çš„å€¼ã€‚ä»è€Œï¼Œå³ä¾¿ç¬¬ä¸€ä¸ª mini batch ä¹‹å target policy å°±å·²ç»å’Œ behaviour policy ä¸ä¸€è‡´äº†ï¼Œä»ç„¶å¯ä»¥é€šè¿‡ log probs è¿›è¡ŒçŸ«æ­£ï¼Œä¹Ÿå³ importance samplingã€‚</p><p>è¿™æ ·ä¸€æ¥ï¼Œåˆæœ‰äº†ä¸¤ä¸ªé—®é¢˜ï¼šlog probs åº”è¯¥å¦‚ä½•å¾—åˆ°ï¼Ÿå®é™…ä¸Šæ¯æ¬¡é‡‡æ ·æ—¶éƒ½æ˜¯å‘é€ç»™ rollout å›ºå®šæ•°é‡çš„ requestsï¼Œå¦‚æœæ¯ä¸ª (prompt, action) å¯¹éƒ½ä¼šè®¡ç®—ä¸€æ¬¡ loss çš„è¯ï¼Œå²‚ä¸æ˜¯æ›´é•¿çš„ sequence ä¼šè®¡ç®—æ›´å¤šæ¬¡ï¼Ÿ</p><p>å¯¹äºç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œè¿™åˆæ˜¯ç»å…¸çš„<a href=https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md#introduction>ç²¾åº¦é—®é¢˜</a>ã€‚å¦‚åŒæˆ‘åœ¨é“¾æ¥åˆ°çš„æ–‡ç« ä¸­æ‰€è¯´çš„ï¼Œrollout engine ç›®å‰åªæœ‰é‡‡æ ·å¾—åˆ°çš„ token èƒ½ç”¨ï¼Œè€Œå¾—åˆ°çš„ log probs ä»¥åŠ reward ç²¾åº¦éƒ½ä¸å¤Ÿï¼Œä¸èƒ½ç”¨äºè®­ç»ƒã€‚behaviour policy å’Œ target policy ä¸ºäº†åš importance sampling æ‰€éœ€çš„ log probs éƒ½å¾—ç”¨ training engine é‡ç®—ã€‚ä¸è¿‡è¦ç®—èµ·æ¥ä¹Ÿä¸éº»çƒ¦ï¼Œåœ¨ç¬¬ä¸€ä¸ª mini batch å¯åŠ¨å‰ï¼Œè¿™æ—¶å€™ target behaviour æ˜¯ä¸€è‡´çš„ï¼Œé‡ç®— log probs å¹¶ä¸”å­˜ä¸‹æ¥å³å¯ã€‚</p><p>å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œçš„ç¡®å¦‚æ­¤ã€‚ä¸€æ¡å¾ˆé•¿çš„ prompt + answer åºåˆ—ç¡®å®ä¼šäº§ç”Ÿéå¸¸å¤šçš„ (prompt, action) å¯¹ï¼Œå…¶ä¸­æ¯ä¸ªå¯¹éƒ½å¯ä»¥çœ‹ä½œä¸€ä¸ª (state, action) å¯¹ã€‚è€Œä¸”ç†è®ºä¸Šæ¯ä¸ªè¿™æ ·çš„ (prompt, action) å¯¹éƒ½ä¼šå‚ä¸ Loss çš„è®¡ç®—ã€‚è¿™ç¡®å®å¯èƒ½å¯¼è‡´é•¿åºåˆ—ä¸­çš„ token ä¼šåœ¨ Loss è®¡ç®—ä¸­å æ®æ›´å¤§çš„æ¯”ä¾‹ï¼Œè®©æ¨¡å‹è¿‡åº¦å…³æ³¨é•¿åºåˆ—çš„ä¼˜åŒ–ï¼Œè€Œå¯¹çŸ­åºåˆ—çš„ä¼˜åŒ–ä¸è¶³ã€‚ä¸è¿‡ï¼Œverl çš„ rollout engine ä¼šè‡ªåŠ¨å¯¹æ¯ä¸ª (prompt, action) å¯¹è¿›è¡ŒåŠ æƒï¼Œä»è€Œè®©é•¿åºåˆ—å’ŒçŸ­åºåˆ—çš„ token åœ¨ Loss è®¡ç®—ä¸­å æ®ç›¸åŒçš„æƒé‡ã€‚ä¸ºäº†ç¼“è§£è¿™ç§æƒ…å†µï¼Œæœ‰å¾ˆå¤šç›¸å…³æ–¹æ³•ï¼š</p><details><summary>æ ·æœ¬åŠ æƒæ–¹æ³•</summary><p>åºåˆ—çº§åˆ«åŠ æƒï¼š ä¸€ç§ç›´æ¥çš„æ–¹æ³•æ˜¯åœ¨è®¡ç®— Loss æ—¶ï¼Œç»™æ¥è‡ªä¸åŒåºåˆ—çš„æ ·æœ¬èµ‹äºˆä¸åŒçš„æƒé‡ã€‚ä¾‹å¦‚ï¼Œç»™æ¯ä¸ªå®Œæ•´åºåˆ—ä¸€ä¸ªå›ºå®šçš„æƒé‡ï¼ˆæ¯”å¦‚ 1ï¼‰ï¼Œç„¶åå°†è¿™ä¸ªæƒé‡å‡åŒ€åˆ†é…ç»™è¯¥åºåˆ—ä¸­çš„æ¯ä¸ª (prompt, action) å¯¹ã€‚è¿™æ ·ï¼Œæ— è®ºåºåˆ—å¤šé•¿ï¼Œå®ƒå¯¹æ€» Loss çš„è´¡çŒ®éƒ½ç›¸åŒã€‚å¦‚æœä¸€ä¸ªåºåˆ—æœ‰ N ä¸ª tokenï¼Œé‚£ä¹ˆæ¯ä¸ª (prompt, action) å¯¹çš„æƒé‡å°±æ˜¯ 1/Nã€‚</p><p>æŒ‰é•¿åº¦åˆ†æ¡¶ï¼š åœ¨æ•°æ®æ”¶é›†åï¼Œå¯ä»¥æ ¹æ®åºåˆ—é•¿åº¦å¯¹æ ·æœ¬è¿›è¡Œæ’åºï¼Œå¹¶å°è¯•å°†ç›¸ä¼¼é•¿åº¦çš„åºåˆ—æ”¾å…¥åŒä¸€ä¸ª mini-batchã€‚è¿™æœ‰åŠ©äºæé«˜è®¡ç®—æ•ˆç‡ï¼Œå› ä¸ºå¯ä»¥å‡å°‘ paddingï¼Œä½†å¯¹äºè§£å†³ Loss è´¡çŒ®ä¸å‡è¡¡çš„ä½œç”¨æœ‰é™ã€‚</p><p>å›ºå®š Token æ•°é‡çš„æ‰¹æ¬¡ï¼š æœ€å¸¸è§ä¸”æœ‰æ•ˆçš„æ–¹æ³•æ˜¯æ„å»ºæ‰¹æ¬¡æ—¶ï¼Œä¸å›ºå®šæ ·æœ¬æ•°é‡ï¼Œè€Œæ˜¯å›ºå®šæ‰¹æ¬¡ä¸­çš„æ€» token æ•°é‡ã€‚è¿™æ ·ï¼Œä¸€ä¸ª mini-batch å¯èƒ½åŒ…å« 4 æ¡é•¿åºåˆ—ï¼Œä¹Ÿå¯èƒ½åŒ…å« 40 æ¡çŸ­åºåˆ—ï¼Œç¡®ä¿æ¯æ¬¡æ›´æ–°æ—¶å¤„ç†çš„æ€»è®¡ç®—é‡å’Œæ¢¯åº¦æ¥æºçš„æ€» token æ•°æ˜¯æ’å®šçš„ï¼Œä»è€Œç¼“è§£é•¿çŸ­åºåˆ—çš„ä¸å‡è¡¡é—®é¢˜ã€‚</p><p>Loss å½’ä¸€åŒ–ï¼šåœ¨è®¡ç®—æ¯ä¸ª mini-batch çš„ Loss æ—¶ï¼Œå¯ä»¥å°†å…¶é™¤ä»¥è¯¥ mini-batch ä¸­å®é™…çš„ token æ•°é‡ã€‚è¿™ç¡®ä¿äº† Loss å€¼ä¸ä¼šä»…ä»…å› ä¸ºæ‰¹æ¬¡ä¸­åŒ…å«äº†æ›´å¤š token è€Œå¢å¤§ï¼Œä»è€Œä¸ºä¸åŒå¤§å°çš„ mini-batchesï¼ˆå¦‚æœä¸æ˜¯æŒ‰å›ºå®š token æ•°æ„å»ºï¼‰æä¾›ä¸€ä¸ªå…¬å¹³çš„æ¯”è¾ƒåŸºç¡€ã€‚</p><p>æˆªæ–­ï¼šè®¾å®šä¸€ä¸ª max_length å‚æ•°ï¼Œé™åˆ¶æ¨¡å‹ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡ã€‚è™½ç„¶è¿™ä¸ç›´æ¥è§£å†³å·²æœ‰é•¿åºåˆ—çš„æƒé‡é—®é¢˜ï¼Œä½†å¯ä»¥é˜²æ­¢ç”Ÿæˆè¿‡é•¿çš„åºåˆ—ï¼Œä»è€Œé™åˆ¶æç«¯ä¸å‡è¡¡çš„å‘ç”Ÿã€‚</p></details><p>whateverï¼Œè§£é‡Šäº†è¿™ä¹ˆå¤šï¼Œé¡ºç€ç†è§£ verl çš„æ¡†æ¶è¿›ä¸€æ­¥å­¦ä¹ äº† RL ç®—æ³•å’Œç³»ç»Ÿï¼Œè¿™é‡Œå…¶å®å’Œ multi-turn éƒ½è¿˜æ²¡æœ‰å…³ç³»ï¼Œæˆ‘ä»¬è¿˜æ˜¯å›åˆ° <code>ActorRolloutRefWorker</code> çš„æºç ä¸Šã€‚</p><details><summary>ActorRolloutRefWorker.__init__ æºç </summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>DictConfig</span><span class=p>,</span> <span class=n>role</span><span class=p>:</span> <span class=nb>str</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># åˆå§‹åŒ– Worker åŸºç±»</span>
</span></span><span class=line><span class=cl>        <span class=n>Worker</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># å­˜å‚¨é…ç½®ä¿¡æ¯</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>config</span>
</span></span><span class=line><span class=cl>        <span class=kn>import</span> <span class=nn>torch.distributed</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># å¦‚æœåˆ†å¸ƒå¼ç¯å¢ƒå°šæœªåˆå§‹åŒ–ï¼Œåˆ™è¿›è¡Œåˆå§‹åŒ–</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>is_initialized</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>rank</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;RANK&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>world_size</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;WORLD_SIZE&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>init_process_group</span><span class=p>(</span><span class=n>backend</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;cpu:gloo,</span><span class=si>{</span><span class=n>get_device_name</span><span class=p>()</span><span class=si>}</span><span class=s2>:</span><span class=si>{</span><span class=n>get_nccl_backend</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>rank</span><span class=o>=</span><span class=n>rank</span><span class=p>,</span> <span class=n>world_size</span><span class=o>=</span><span class=n>world_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># ä¸º FSDP æ„å»ºè®¾å¤‡ç½‘æ ¼</span>
</span></span><span class=line><span class=cl>        <span class=n>world_size</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>get_world_size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span> <span class=o>=</span> <span class=n>create_device_mesh</span><span class=p>(</span><span class=n>world_size</span><span class=o>=</span><span class=n>world_size</span><span class=p>,</span> <span class=n>fsdp_size</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>fsdp_config</span><span class=o>.</span><span class=n>fsdp_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># ä¸º Ulysses åºåˆ—å¹¶è¡Œæ„å»ºè®¾å¤‡ç½‘æ ¼</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_device_mesh</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;ulysses_sequence_parallel_size&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>dp</span> <span class=o>=</span> <span class=n>world_size</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_device_mesh</span> <span class=o>=</span> <span class=n>init_device_mesh</span><span class=p>(</span><span class=n>device_name</span><span class=p>,</span> <span class=n>mesh_shape</span><span class=o>=</span><span class=p>(</span><span class=n>dp</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span><span class=p>),</span> <span class=n>mesh_dim_names</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;dp&#34;</span><span class=p>,</span> <span class=s2>&#34;sp&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># åˆå§‹åŒ– Ulysses åˆ†ç‰‡ç®¡ç†å™¨</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sharding_manager</span> <span class=o>=</span> <span class=n>FSDPUlyssesShardingManager</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ulysses_device_mesh</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># è·å– LoRA rank å’Œæ˜¯å¦ä½¿ç”¨ LoRA çš„æ ‡å¿—</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_lora_rank</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;lora_rank&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_is_lora</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_lora_rank</span> <span class=o>&gt;</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># è®¾ç½® Worker è§’è‰²å’Œç›¸å…³æ ‡å¿—</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>role</span> <span class=o>=</span> <span class=n>role</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>role</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;actor&#34;</span><span class=p>,</span> <span class=s2>&#34;rollout&#34;</span><span class=p>,</span> <span class=s2>&#34;ref&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout_ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_is_actor</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>role</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;actor&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout_ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_is_rollout</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>role</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;rollout&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout_ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_is_ref</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>role</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;ref&#34;</span><span class=p>,</span> <span class=s2>&#34;actor_rollout_ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>profiler_config</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>ProfilerConfig</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=c1># æ ¹æ®è§’è‰²è·å–æ€§èƒ½åˆ†æé…ç½®</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_actor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>profiler_config</span> <span class=o>=</span> <span class=n>omega_conf_to_dataclass</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;profiler&#34;</span><span class=p>,</span> <span class=p>{}),</span> <span class=n>ProfilerConfig</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_rollout</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>profiler_config</span> <span class=o>=</span> <span class=n>omega_conf_to_dataclass</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;profiler&#34;</span><span class=p>,</span> <span class=p>{}),</span> <span class=n>ProfilerConfig</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_ref</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>profiler_config</span> <span class=o>=</span> <span class=n>omega_conf_to_dataclass</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>ref</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;profiler&#34;</span><span class=p>,</span> <span class=p>{}),</span> <span class=n>ProfilerConfig</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># åˆå§‹åŒ–åˆ†å¸ƒå¼æ€§èƒ½åˆ†æå™¨</span>
</span></span><span class=line><span class=cl>        <span class=n>DistProfilerExtension</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>DistProfiler</span><span class=p>(</span><span class=n>rank</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>rank</span><span class=p>,</span> <span class=n>config</span><span class=o>=</span><span class=n>profiler_config</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># è®¾ç½®å‚æ•°å’Œä¼˜åŒ–å™¨å¸è½½æ ‡å¿—</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_param</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_optimizer</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_actor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_param</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>fsdp_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;param_offload&#34;</span><span class=p>,</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_optimizer</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>fsdp_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;optimizer_offload&#34;</span><span class=p>,</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_ref</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_param</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>ref</span><span class=o>.</span><span class=n>fsdp_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;param_offload&#34;</span><span class=p>,</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># è§„èŒƒåŒ– actor ç›¸å…³é…ç½®</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_actor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span> <span class=o>*=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>n</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span> <span class=o>//=</span> <span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span>
</span></span><span class=line><span class=cl>            <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;ppo_mini_batch_size </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span><span class=si>}</span><span class=s2> should be larger than 0 after normalization&#34;</span>
</span></span><span class=line><span class=cl>            <span class=c1># micro bsz</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size</span> <span class=o>//=</span> <span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size_per_gpu</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size_per_gpu</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span> <span class=o>%</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size_per_gpu</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;normalized ppo_mini_batch_size </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span><span class=si>}</span><span class=s2> should be divisible by ppo_micro_batch_size_per_gpu </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size_per_gpu</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>                <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size_per_gpu</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;normalized ppo_mini_batch_size </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_mini_batch_size</span><span class=si>}</span><span class=s2> should be larger than ppo_micro_batch_size_per_gpu </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>ppo_micro_batch_size_per_gpu</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># è§„èŒƒåŒ– rollout ç›¸å…³é…ç½®</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_rollout</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>log_prob_micro_batch_size</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>log_prob_micro_batch_size</span> <span class=o>//=</span> <span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>log_prob_micro_batch_size_per_gpu</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>log_prob_micro_batch_size</span>
</span></span><span class=line><span class=cl>        <span class=c1># è§„èŒƒåŒ– ref ç›¸å…³é…ç½®</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_ref</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>ref</span><span class=o>.</span><span class=n>log_prob_micro_batch_size</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>ref</span><span class=o>.</span><span class=n>log_prob_micro_batch_size</span> <span class=o>//=</span> <span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>ref</span><span class=o>.</span><span class=n>log_prob_micro_batch_size_per_gpu</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>ref</span><span class=o>.</span><span class=n>log_prob_micro_batch_size</span>
</span></span></code></pre></td></tr></table></div></div></details><h3 id=actorrolloutrefworker_build_model_optimizer><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/fsdp_workers.py#L177><code>ActorRolloutRefWorker._build_model_optimizer()</code></a><a hidden class=anchor aria-hidden=true href=#actorrolloutrefworker_build_model_optimizer>#</a></h3><p>è¿™éƒ¨åˆ†æºç å’Œç±»å†™çš„è¿˜æ˜¯å¾ˆç›´ç™½çš„ï¼Œä¸ç”¨å¤ªå¤šè§£é‡Šï¼š</p><ol><li>åˆå§‹åŒ– Hugging Face é…ç½®ï¼Œè·å– Generation Configï¼Œå¹¶è®¾ç½®æ¨¡å‹çš„æ•°æ®ç±»å‹ï¼ˆActor ä½¿ç”¨ fp32ï¼ŒReference ä½¿ç”¨ bf16ï¼‰ã€‚</li><li>ä½¿ç”¨ Hugging Face çš„ <code>AutoModelForCausalLM</code> æˆ– <code>AutoModelForVision2Seq</code> ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½åŸºç¡€æ¨¡å‹ã€‚</li><li>åº”ç”¨å„ç§ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬ Liger kernelã€èåˆ kernelã€æ¢¯åº¦æ£€æŸ¥ç‚¹ã€LoRA ç­‰ã€‚</li><li>æ ¹æ®é…ç½®é€‰æ‹© FSDP æˆ– FSDP2 ç­–ç•¥ï¼Œå°†æ¨¡å‹å°è£…åˆ°åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ä¸­ï¼Œæ”¯æŒå‚æ•°åˆ†ç‰‡å’Œæ··åˆç²¾åº¦è®­ç»ƒã€‚</li><li>å¦‚æœå½“å‰ Worker æ˜¯ Actor è§’è‰²ï¼Œåˆ™åˆå§‹åŒ– AdamW ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚</li></ol><details><summary>ActorRolloutRefWorker._build_model_optimizer æºç </summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span><span class=lnt>116
</span><span class=lnt>117
</span><span class=lnt>118
</span><span class=lnt>119
</span><span class=lnt>120
</span><span class=lnt>121
</span><span class=lnt>122
</span><span class=lnt>123
</span><span class=lnt>124
</span><span class=lnt>125
</span><span class=lnt>126
</span><span class=lnt>127
</span><span class=lnt>128
</span><span class=lnt>129
</span><span class=lnt>130
</span><span class=lnt>131
</span><span class=lnt>132
</span><span class=lnt>133
</span><span class=lnt>134
</span><span class=lnt>135
</span><span class=lnt>136
</span><span class=lnt>137
</span><span class=lnt>138
</span><span class=lnt>139
</span><span class=lnt>140
</span><span class=lnt>141
</span><span class=lnt>142
</span><span class=lnt>143
</span><span class=lnt>144
</span><span class=lnt>145
</span><span class=lnt>146
</span><span class=lnt>147
</span><span class=lnt>148
</span><span class=lnt>149
</span><span class=lnt>150
</span><span class=lnt>151
</span><span class=lnt>152
</span><span class=lnt>153
</span><span class=lnt>154
</span><span class=lnt>155
</span><span class=lnt>156
</span><span class=lnt>157
</span><span class=lnt>158
</span><span class=lnt>159
</span><span class=lnt>160
</span><span class=lnt>161
</span><span class=lnt>162
</span><span class=lnt>163
</span><span class=lnt>164
</span><span class=lnt>165
</span><span class=lnt>166
</span><span class=lnt>167
</span><span class=lnt>168
</span><span class=lnt>169
</span><span class=lnt>170
</span><span class=lnt>171
</span><span class=lnt>172
</span><span class=lnt>173
</span><span class=lnt>174
</span><span class=lnt>175
</span><span class=lnt>176
</span><span class=lnt>177
</span><span class=lnt>178
</span><span class=lnt>179
</span><span class=lnt>180
</span><span class=lnt>181
</span><span class=lnt>182
</span><span class=lnt>183
</span><span class=lnt>184
</span><span class=lnt>185
</span><span class=lnt>186
</span><span class=lnt>187
</span><span class=lnt>188
</span><span class=lnt>189
</span><span class=lnt>190
</span><span class=lnt>191
</span><span class=lnt>192
</span><span class=lnt>193
</span><span class=lnt>194
</span><span class=lnt>195
</span><span class=lnt>196
</span><span class=lnt>197
</span><span class=lnt>198
</span><span class=lnt>199
</span><span class=lnt>200
</span><span class=lnt>201
</span><span class=lnt>202
</span><span class=lnt>203
</span><span class=lnt>204
</span><span class=lnt>205
</span><span class=lnt>206
</span><span class=lnt>207
</span><span class=lnt>208
</span><span class=lnt>209
</span><span class=lnt>210
</span><span class=lnt>211
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_build_model_optimizer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>model_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>fsdp_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>optim_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>override_model_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>use_remove_padding</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>use_fused_kernels</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>enable_gradient_checkpointing</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>use_liger</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>role</span><span class=o>=</span><span class=s2>&#34;actor&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>enable_activation_offload</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>optim</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>torch.distributed.fsdp</span> <span class=kn>import</span> <span class=n>CPUOffload</span><span class=p>,</span> <span class=n>MixedPrecision</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoConfig</span><span class=p>,</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoModelForVision2Seq</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.utils.model</span> <span class=kn>import</span> <span class=n>get_generation_config</span><span class=p>,</span> <span class=n>print_model_size</span><span class=p>,</span> <span class=n>update_model_config</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.utils.torch_dtypes</span> <span class=kn>import</span> <span class=n>PrecisionType</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>role</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;actor&#34;</span><span class=p>,</span> <span class=s2>&#34;ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Before init </span><span class=si>{</span><span class=n>role</span><span class=si>}</span><span class=s2> from HF AutoModel&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>local_path</span> <span class=o>=</span> <span class=n>model_path</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>hf_tokenizer</span><span class=p>(</span><span class=n>local_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>processor</span> <span class=o>=</span> <span class=n>hf_processor</span><span class=p>(</span><span class=n>local_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>torch_dtype</span> <span class=o>=</span> <span class=n>fsdp_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;model_dtype&#34;</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>torch_dtype</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>torch_dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>float32</span> <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_actor</span> <span class=k>else</span> <span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>torch_dtype</span> <span class=o>=</span> <span class=n>PrecisionType</span><span class=o>.</span><span class=n>to_dtype</span><span class=p>(</span><span class=n>torch_dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>actor_model_config</span> <span class=o>=</span> <span class=n>AutoConfig</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>local_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>,</span> <span class=n>attn_implementation</span><span class=o>=</span><span class=s2>&#34;flash_attention_2&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>getattr</span><span class=p>(</span><span class=n>actor_model_config</span><span class=p>,</span> <span class=s2>&#34;model_type&#34;</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span> <span class=o>==</span> <span class=s2>&#34;kimi_vl&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_model_config</span><span class=o>.</span><span class=n>text_config</span><span class=o>.</span><span class=n>topk_method</span> <span class=o>=</span> <span class=s2>&#34;greedy&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>generation_config</span> <span class=o>=</span> <span class=n>get_generation_config</span><span class=p>(</span><span class=n>local_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>override_config_kwargs</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;bos_token_id&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>bos_token_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;eos_token_id&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;pad_token_id&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>override_config_kwargs</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>override_model_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>update_model_config</span><span class=p>(</span><span class=n>actor_model_config</span><span class=p>,</span> <span class=n>override_config_kwargs</span><span class=o>=</span><span class=n>override_config_kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># å¦‚æœæ˜¯ rank 0 è¿›ç¨‹ï¼Œæ‰“å°æ›´æ–°åçš„æ¨¡å‹é…ç½®</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Model config after override: </span><span class=si>{</span><span class=n>actor_model_config</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>init_context</span> <span class=o>=</span> <span class=n>get_init_weight_context_manager</span><span class=p>(</span><span class=n>use_meta_tensor</span><span class=o>=</span><span class=ow>not</span> <span class=n>actor_model_config</span><span class=o>.</span><span class=n>tie_word_embeddings</span><span class=p>,</span> <span class=n>mesh</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>init_context</span><span class=p>(),</span> <span class=n>warnings</span><span class=o>.</span><span class=n>catch_warnings</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>warnings</span><span class=o>.</span><span class=n>simplefilter</span><span class=p>(</span><span class=s2>&#34;ignore&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=nb>type</span><span class=p>(</span><span class=n>actor_model_config</span><span class=p>)</span> <span class=ow>in</span> <span class=n>AutoModelForVision2Seq</span><span class=o>.</span><span class=n>_model_mapping</span><span class=o>.</span><span class=n>keys</span><span class=p>():</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module_class</span> <span class=o>=</span> <span class=n>AutoModelForVision2Seq</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module_class</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>actor_module</span> <span class=o>=</span> <span class=n>actor_module_class</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>pretrained_model_name_or_path</span><span class=o>=</span><span class=n>local_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch_dtype</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>config</span><span class=o>=</span><span class=n>actor_model_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>use_liger</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=kn>from</span> <span class=nn>liger_kernel.transformers.monkey_patch</span> <span class=kn>import</span> <span class=n>_apply_liger_kernel_to_instance</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>_apply_liger_kernel_to_instance</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>actor_module</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>fused_kernel_options</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;fused_kernel_options&#34;</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>fused_kernels_backend</span> <span class=o>=</span> <span class=n>fused_kernel_options</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;impl_backend&#34;</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span> <span class=k>if</span> <span class=n>fused_kernel_options</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>apply_monkey_patch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>model</span><span class=o>=</span><span class=n>actor_module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>use_remove_padding</span><span class=o>=</span><span class=n>use_remove_padding</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>ulysses_sp_size</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>ulysses_sequence_parallel_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>use_fused_kernels</span><span class=o>=</span><span class=n>use_fused_kernels</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>fused_kernels_backend</span><span class=o>=</span><span class=n>fused_kernels_backend</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>actor_module</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>torch_dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>enable_gradient_checkpointing</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module</span><span class=o>.</span><span class=n>gradient_checkpointing_enable</span><span class=p>(</span><span class=n>gradient_checkpointing_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;use_reentrant&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>})</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_lora</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Applying LoRA to actor module&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module</span><span class=o>.</span><span class=n>enable_input_require_grads</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>lora_config</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;task_type&#34;</span><span class=p>:</span> <span class=n>TaskType</span><span class=o>.</span><span class=n>CAUSAL_LM</span><span class=p>,</span> <span class=s2>&#34;r&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>lora_rank</span><span class=p>,</span> <span class=s2>&#34;lora_alpha&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>lora_alpha</span><span class=p>,</span> <span class=s2>&#34;target_modules&#34;</span><span class=p>:</span> <span class=n>convert_to_regular_types</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>target_modules</span><span class=p>),</span> <span class=s2>&#34;bias&#34;</span><span class=p>:</span> <span class=s2>&#34;none&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module</span> <span class=o>=</span> <span class=n>get_peft_model</span><span class=p>(</span><span class=n>actor_module</span><span class=p>,</span> <span class=n>LoraConfig</span><span class=p>(</span><span class=o>**</span><span class=n>lora_config</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>barrier</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>print_model_size</span><span class=p>(</span><span class=n>actor_module</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;After init </span><span class=si>{</span><span class=n>role</span><span class=si>}</span><span class=s2> from HF AutoModel&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>mixed_precision_config</span> <span class=o>=</span> <span class=n>fsdp_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;mixed_precision&#34;</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>mixed_precision_config</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>param_dtype</span> <span class=o>=</span> <span class=n>PrecisionType</span><span class=o>.</span><span class=n>to_dtype</span><span class=p>(</span><span class=n>mixed_precision_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;param_dtype&#34;</span><span class=p>,</span> <span class=s2>&#34;bf16&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>reduce_dtype</span> <span class=o>=</span> <span class=n>PrecisionType</span><span class=o>.</span><span class=n>to_dtype</span><span class=p>(</span><span class=n>mixed_precision_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;reduce_dtype&#34;</span><span class=p>,</span> <span class=s2>&#34;fp32&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>buffer_dtype</span> <span class=o>=</span> <span class=n>PrecisionType</span><span class=o>.</span><span class=n>to_dtype</span><span class=p>(</span><span class=n>mixed_precision_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;buffer_dtype&#34;</span><span class=p>,</span> <span class=s2>&#34;fp32&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>param_dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span>
</span></span><span class=line><span class=cl>            <span class=n>reduce_dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>float32</span>
</span></span><span class=line><span class=cl>            <span class=n>buffer_dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>float32</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>mixed_precision</span> <span class=o>=</span> <span class=n>MixedPrecision</span><span class=p>(</span><span class=n>param_dtype</span><span class=o>=</span><span class=n>param_dtype</span><span class=p>,</span> <span class=n>reduce_dtype</span><span class=o>=</span><span class=n>reduce_dtype</span><span class=p>,</span> <span class=n>buffer_dtype</span><span class=o>=</span><span class=n>buffer_dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>auto_wrap_policy</span> <span class=o>=</span> <span class=n>get_fsdp_wrap_policy</span><span class=p>(</span><span class=n>module</span><span class=o>=</span><span class=n>actor_module</span><span class=p>,</span> <span class=n>config</span><span class=o>=</span><span class=n>fsdp_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;wrap_policy&#34;</span><span class=p>,</span> <span class=kc>None</span><span class=p>),</span> <span class=n>is_lora</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;lora_rank&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># TODO(zhangchi.usc1992, shengguangming) fix me. Current, auto_wrap_policy causes HFRollout to hang in Gemma</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_is_rollout</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>name</span> <span class=o>==</span> <span class=s2>&#34;hf&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>auto_wrap_policy</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># å¦‚æœæ˜¯ rank 0 è¿›ç¨‹ï¼Œæ‰“å°åŒ…è£…ç­–ç•¥</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;wrap_policy: </span><span class=si>{</span><span class=n>auto_wrap_policy</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>fsdp_mesh</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span>
</span></span><span class=line><span class=cl>        <span class=n>sharding_strategy</span> <span class=o>=</span> <span class=n>get_sharding_strategy</span><span class=p>(</span><span class=n>fsdp_mesh</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># TODO: æ·»åŠ  transformer ç­–ç•¥</span>
</span></span><span class=line><span class=cl>        <span class=c1># æˆ‘ä»¬å¼ºåˆ¶ reference policy ä½¿ç”¨ CPUOffload æ¥èŠ‚çœå†…å­˜</span>
</span></span><span class=line><span class=cl>        <span class=c1># æˆ‘ä»¬å¼ºåˆ¶å…³é—­ actor çš„ CPUOffloadï¼Œå› ä¸ºå®ƒåœ¨ä½¿ç”¨ grad accumulation æ—¶ä¼šå¯¼è‡´ä¸æ­£ç¡®çš„ç»“æœ</span>
</span></span><span class=line><span class=cl>        <span class=n>cpu_offload</span> <span class=o>=</span> <span class=kc>None</span> <span class=k>if</span> <span class=n>role</span> <span class=o>==</span> <span class=s2>&#34;actor&#34;</span> <span class=k>else</span> <span class=n>CPUOffload</span><span class=p>(</span><span class=n>offload_params</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># æ ¹æ®é…ç½®çš„ç­–ç•¥ï¼Œå°†æ¨¡å‹å°è£…åˆ° FSDP ä¸­</span>
</span></span><span class=line><span class=cl>        <span class=n>fsdp_strategy</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>fsdp_strategy</span> <span class=o>==</span> <span class=s2>&#34;fsdp&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_module_fsdp</span> <span class=o>=</span> <span class=n>FSDP</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>cpu_offload</span><span class=o>=</span><span class=n>cpu_offload</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>param_init_fn</span><span class=o>=</span><span class=n>init_fn</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>use_orig_params</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>auto_wrap_policy</span><span class=o>=</span><span class=n>auto_wrap_policy</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>device_id</span><span class=o>=</span><span class=n>get_device_id</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                <span class=n>sharding_strategy</span><span class=o>=</span><span class=n>sharding_strategy</span><span class=p>,</span>  <span class=c1># zero3</span>
</span></span><span class=line><span class=cl>                <span class=n>mixed_precision</span><span class=o>=</span><span class=n>mixed_precision</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>sync_module_states</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>device_mesh</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_mesh</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>forward_prefetch</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>fsdp_config</span><span class=o>.</span><span class=n>forward_prefetch</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>fsdp_strategy</span> <span class=o>==</span> <span class=s2>&#34;fsdp2&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>assert</span> <span class=n>CPUOffloadPolicy</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>,</span> <span class=s2>&#34;PyTorch version &gt;= 2.4 is required for using fully_shard API (FSDP2)&#34;</span>
</span></span><span class=line><span class=cl>            <span class=n>mp_policy</span> <span class=o>=</span> <span class=n>MixedPrecisionPolicy</span><span class=p>(</span><span class=n>param_dtype</span><span class=o>=</span><span class=n>param_dtype</span><span class=p>,</span> <span class=n>reduce_dtype</span><span class=o>=</span><span class=n>reduce_dtype</span><span class=p>,</span> <span class=n>cast_forward_inputs</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>role</span> <span class=o>==</span> <span class=s2>&#34;actor&#34;</span> <span class=ow>and</span> <span class=n>fsdp_config</span><span class=o>.</span><span class=n>offload_policy</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>cpu_offload</span> <span class=o>=</span> <span class=n>CPUOffloadPolicy</span><span class=p>(</span><span class=n>pin_memory</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_param</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_optimizer</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>cpu_offload</span> <span class=o>=</span> <span class=kc>None</span> <span class=k>if</span> <span class=n>role</span> <span class=o>==</span> <span class=s2>&#34;actor&#34;</span> <span class=k>else</span> <span class=n>CPUOffloadPolicy</span><span class=p>(</span><span class=n>pin_memory</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>fsdp_kwargs</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;mesh&#34;</span><span class=p>:</span> <span class=n>fsdp_mesh</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;mp_policy&#34;</span><span class=p>:</span> <span class=n>mp_policy</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;offload_policy&#34;</span><span class=p>:</span> <span class=n>cpu_offload</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;reshard_after_forward&#34;</span><span class=p>:</span> <span class=n>fsdp_config</span><span class=o>.</span><span class=n>reshard_after_forward</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=n>full_state</span> <span class=o>=</span> <span class=n>actor_module</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>apply_fsdp2</span><span class=p>(</span><span class=n>actor_module</span><span class=p>,</span> <span class=n>fsdp_kwargs</span><span class=p>,</span> <span class=n>fsdp_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>fsdp2_load_full_state_dict</span><span class=p>(</span><span class=n>actor_module</span><span class=p>,</span> <span class=n>full_state</span><span class=p>,</span> <span class=n>fsdp_mesh</span><span class=p>,</span> <span class=n>cpu_offload</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_module_fsdp</span> <span class=o>=</span> <span class=n>actor_module</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>NotImplementedError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;not implement </span><span class=si>{</span><span class=n>fsdp_strategy</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># å¦‚æœå¯ç”¨äº†æ¿€æ´»å¸è½½ï¼Œåˆ™å¯ç”¨å®ƒ</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>enable_activation_offload</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>enable_activation_offloading</span><span class=p>(</span><span class=n>actor_module_fsdp</span><span class=p>,</span> <span class=n>fsdp_strategy</span><span class=p>,</span> <span class=n>enable_gradient_checkpointing</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># è®°å½• FSDP åˆå§‹åŒ–ä¹‹åçš„ GPU å†…å­˜ä½¿ç”¨æƒ…å†µ</span>
</span></span><span class=line><span class=cl>        <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;After </span><span class=si>{</span><span class=n>role</span><span class=si>}</span><span class=s2> FSDP init&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># TODO: add more optimizer args into config</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>role</span> <span class=o>==</span> <span class=s2>&#34;actor&#34;</span> <span class=ow>and</span> <span class=n>optim_config</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=kn>from</span> <span class=nn>verl.utils.torch_functional</span> <span class=kn>import</span> <span class=n>get_constant_schedule_with_warmup</span><span class=p>,</span> <span class=n>get_cosine_schedule_with_warmup</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>actor_optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_module_fsdp</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                <span class=n>lr</span><span class=o>=</span><span class=n>optim_config</span><span class=o>.</span><span class=n>lr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>betas</span><span class=o>=</span><span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;betas&#34;</span><span class=p>,</span> <span class=p>(</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>                <span class=n>weight_decay</span><span class=o>=</span><span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;weight_decay&#34;</span><span class=p>,</span> <span class=mf>1e-2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>total_steps</span> <span class=o>=</span> <span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;total_training_steps&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>num_warmup_steps</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;lr_warmup_steps&#34;</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>warmup_style</span> <span class=o>=</span> <span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;warmup_style&#34;</span><span class=p>,</span> <span class=s2>&#34;constant&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>min_lr_ratio</span> <span class=o>=</span> <span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;min_lr_ratio&#34;</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>num_cycles</span> <span class=o>=</span> <span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;num_cycles&#34;</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>num_warmup_steps</span> <span class=o>&lt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>num_warmup_steps_ratio</span> <span class=o>=</span> <span class=n>optim_config</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;lr_warmup_steps_ratio&#34;</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>num_warmup_steps</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>num_warmup_steps_ratio</span> <span class=o>*</span> <span class=n>total_steps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Total steps: </span><span class=si>{</span><span class=n>total_steps</span><span class=si>}</span><span class=s2>, num_warmup_steps: </span><span class=si>{</span><span class=n>num_warmup_steps</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>warmup_style</span> <span class=o>==</span> <span class=s2>&#34;constant&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_lr_scheduler</span> <span class=o>=</span> <span class=n>get_constant_schedule_with_warmup</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=n>actor_optimizer</span><span class=p>,</span> <span class=n>num_warmup_steps</span><span class=o>=</span><span class=n>num_warmup_steps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>elif</span> <span class=n>warmup_style</span> <span class=o>==</span> <span class=s2>&#34;cosine&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>actor_lr_scheduler</span> <span class=o>=</span> <span class=n>get_cosine_schedule_with_warmup</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=n>actor_optimizer</span><span class=p>,</span> <span class=n>num_warmup_steps</span><span class=o>=</span><span class=n>num_warmup_steps</span><span class=p>,</span> <span class=n>num_training_steps</span><span class=o>=</span><span class=n>total_steps</span><span class=p>,</span> <span class=n>min_lr_ratio</span><span class=o>=</span><span class=n>min_lr_ratio</span><span class=p>,</span> <span class=n>num_cycles</span><span class=o>=</span><span class=n>num_cycles</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>raise</span> <span class=ne>NotImplementedError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Warmup style </span><span class=si>{</span><span class=n>warmup_style</span><span class=si>}</span><span class=s2> is not supported&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;After </span><span class=si>{</span><span class=n>role</span><span class=si>}</span><span class=s2> optimizer init&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_optimizer</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_lr_scheduler</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>actor_module_fsdp</span><span class=p>,</span> <span class=n>actor_optimizer</span><span class=p>,</span> <span class=n>actor_lr_scheduler</span><span class=p>,</span> <span class=n>actor_model_config</span>
</span></span></code></pre></td></tr></table></div></div></details><p>è¿™é‡Œä»£ç å¾ˆç›´ç™½ã€‚æœ‰ä¸€ä¸ªç‚¹å€¼å¾—å•ç‹¬æ‹å‡ºæ¥è®²ä¸€ä¸‹ï¼šä»”ç»†è§‚å¯Ÿ <code>actor_module</code> çš„ dtypeï¼Œç›´è§‰å‘Šè¯‰æˆ‘ï¼Œ<code>actor_module</code> çš„ dtype åº”è¯¥æ˜¯ bf16 çš„ï¼Œè€Œ gradient å’Œ optimizer çš„ dtype æ˜¯ fp32 çš„ã€‚å¯æ˜¯ <code>actor_module</code> çš„ default dtype è¢«è®¾ä¸ºäº† fp32ï¼Œç„¶åä» fp32 load äº†æ¨¡å‹ã€‚å®é™…ä¸Šè¿™æ˜¯å› ä¸º pytorch çš„å„ç§ optimizer éƒ½æ˜¯ç›´æ¥å’Œ parameter ç»‘å®šçš„ï¼Œç”¨ bf16 çš„ parameter åˆå§‹åŒ–çš„ optimizer ä¹Ÿæ˜¯ bf16ã€‚æ‰€ä»¥ model å…ˆ load äº† fp32ï¼Œç„¶ååˆå§‹åŒ– optimizer ä½œä¸ºæ··åˆç²¾åº¦ï¼Œæœ€åæŠŠ model è½¬æˆ bf16ã€‚</p><h3 id=actorrolloutrefworker_build_rollout><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/fsdp_workers.py#L394><code>ActorRolloutRefWorker._build_rollout()</code></a><a hidden class=anchor aria-hidden=true href=#actorrolloutrefworker_build_rollout>#</a></h3><p>è¿™æ˜¯å¯¹æˆ‘è€Œè¨€æœ€æ¸…æ™°çš„åœ°æ–¹ï¼Œå®é™…ä¸Šä¹Ÿæ˜¯æœ€ç†Ÿæ‚‰çš„ã€‚åœ¨è¿™é‡Œç»ˆäºå¼•å…¥äº† SGLangï¼š</p><ol><li><strong>è®¾å¤‡ç½‘æ ¼åˆ›å»º</strong>ï¼šä¸º Rollout åˆ›å»ºæ¨ç†å¼ é‡å¹¶è¡Œï¼ˆ<code>infer_tp</code>ï¼‰è®¾å¤‡ç½‘æ ¼ã€‚</li><li><strong>SGLang Rollout æ„å»º</strong>ï¼šå¯¼å…¥å¹¶å®ä¾‹åŒ– <code>SGLangRollout</code> å’Œ <code>FSDPSGLangShardingManager</code>ã€‚<code>FSDPSGLangShardingManager</code> è´Ÿè´£åœ¨ FSDP è®­ç»ƒæ ¼å¼å’Œ SGLang æ¨ç†æ ¼å¼ä¹‹é—´è½¬æ¢æ¨¡å‹æƒé‡ã€‚</li></ol><details><summary>ActorRolloutRefWorker._build_rollout éƒ¨åˆ†æºç </summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_build_rollout</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>torch.distributed.device_mesh</span> <span class=kn>import</span> <span class=n>init_device_mesh</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>infer_tp</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>tensor_model_parallel_size</span>
</span></span><span class=line><span class=cl>    <span class=n>dp</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>world_size</span> <span class=o>//</span> <span class=n>infer_tp</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>world_size</span> <span class=o>%</span> <span class=n>infer_tp</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;rollout world_size: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>world_size</span><span class=si>}</span><span class=s2> is not divisible by infer_tp: </span><span class=si>{</span><span class=n>infer_tp</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>rollout_device_mesh</span> <span class=o>=</span> <span class=n>init_device_mesh</span><span class=p>(</span><span class=n>device_name</span><span class=p>,</span> <span class=n>mesh_shape</span><span class=o>=</span><span class=p>(</span><span class=n>dp</span><span class=p>,</span> <span class=n>infer_tp</span><span class=p>),</span> <span class=n>mesh_dim_names</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;dp&#34;</span><span class=p>,</span> <span class=s2>&#34;infer_tp&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>rollout_name</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>name</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>rollout_name</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;sglang&#34;</span><span class=p>,</span> <span class=s2>&#34;sglang_async&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>rollout_name</span> <span class=o>==</span> <span class=s2>&#34;sglang_async&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>warnings</span><span class=o>.</span><span class=n>warn</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;&#39;sglang_async&#39; has been deprecated and merged into &#39;sglang&#39;. Please use &#39;sglang&#39; going forward.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=ne>DeprecationWarning</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>stacklevel</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.workers.rollout.sglang_rollout</span> <span class=kn>import</span> <span class=n>SGLangRollout</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.workers.sharding_manager.fsdp_sglang</span> <span class=kn>import</span> <span class=n>FSDPSGLangShardingManager</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>local_path</span> <span class=o>=</span> <span class=n>copy_to_local</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Before building </span><span class=si>{</span><span class=n>rollout_name</span><span class=si>}</span><span class=s2> rollout&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>rollout</span> <span class=o>=</span> <span class=n>SGLangRollout</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_module</span><span class=o>=</span><span class=n>local_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>tokenizer</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>model_hf_config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>actor_model_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;After building </span><span class=si>{</span><span class=n>rollout_name</span><span class=si>}</span><span class=s2> rollout&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>get_world_size</span><span class=p>()</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>load_format</span> <span class=o>=</span> <span class=s2>&#34;dummy_hf&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>rollout_sharding_manager</span> <span class=o>=</span> <span class=n>FSDPSGLangShardingManager</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>module</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>actor_module_fsdp</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>inference_engine</span><span class=o>=</span><span class=n>rollout</span><span class=o>.</span><span class=n>_engine</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>model_config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>actor_model_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>full_params</span><span class=o>=</span><span class=s2>&#34;hf&#34;</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>load_format</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>device_mesh</span><span class=o>=</span><span class=n>rollout_device_mesh</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>offload_param</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_is_offload_param</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>log_gpu_memory_usage</span><span class=p>(</span><span class=s2>&#34;After building sharding manager&#34;</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>logger</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=ne>NotImplementedError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Rollout name: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>name</span><span class=si>}</span><span class=s2> is not supported&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>rollout</span><span class=p>,</span> <span class=n>rollout_sharding_manager</span>
</span></span></code></pre></td></tr></table></div></div></details><h3 id=sglangrollout__init__><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L208><code>SGLangRollout.__init__()</code></a><a hidden class=anchor aria-hidden=true href=#sglangrollout__init__>#</a></h3><p>äº‹å·²è‡³æ­¤ï¼Œå†å¾€ä¸‹çœ‹ä¸€å±‚ SGLang å…·ä½“çš„åˆå§‹åŒ–ï¼š</p><ol><li>è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°å¹¶è®¾ç½®é…ç½®å’Œè®¾å¤‡ç½‘æ ¼ã€‚</li><li>é€šè¿‡ <code>_initialize_tools()</code> åˆå§‹åŒ–å·¥å…· schemasã€map å’Œè§£æå™¨ï¼Œæ”¯æŒ Multi-turn å¯¹è¯ä¸­çš„å·¥å…·ä½¿ç”¨ã€‚</li><li>åˆå§‹åŒ– SGLang æ¨ç†æ‰€éœ€çš„åˆ†å¸ƒå¼ç¯å¢ƒã€‚</li><li>é€šè¿‡ <code>_verify_config()</code> éªŒè¯æ¨¡å‹é…ç½®ã€‚</li><li>é€šè¿‡ <code>_init_inference_engine()</code> åˆå§‹åŒ– SGLang æ¨ç†å¼•æ“ã€‚</li><li>é€šè¿‡ <code>_init_sampling_params()</code> åˆå§‹åŒ–ç”Ÿæˆåºåˆ—çš„é‡‡æ ·å‚æ•°ã€‚</li><li>è®¾ç½® Tokenizer å’Œ padding token IDã€‚</li></ol><details><summary>SGLangRollout.__init__ éƒ¨åˆ†æºç </summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>SGLangRollout</span><span class=p>(</span><span class=n>BaseRollout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>actor_module</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=p>:</span> <span class=n>DictConfig</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>model_hf_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>port</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>trust_remote_code</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>device_mesh</span><span class=p>:</span> <span class=n>DeviceMesh</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=o>**</span><span class=n>kwargs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Synchronized SGLang rollout engine.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            actor_module: Huggingface model name or path to the model. The
</span></span></span><span class=line><span class=cl><span class=s2>                model should be supported by SGLang.
</span></span></span><span class=line><span class=cl><span class=s2>            config: A DictConfig object containing SGLang-specific operational
</span></span></span><span class=line><span class=cl><span class=s2>                parameters and rollout settings.
</span></span></span><span class=line><span class=cl><span class=s2>                Refer to https://docs.sglang.ai/backend/server_arguments.html
</span></span></span><span class=line><span class=cl><span class=s2>            tokenizer: The tokenizer instance compatible with the actor_module.
</span></span></span><span class=line><span class=cl><span class=s2>            model_hf_config: The Hugging Face model&#39;s configuration (e.g.,
</span></span></span><span class=line><span class=cl><span class=s2>                `transformers.PretrainedConfig`). It provides architectural
</span></span></span><span class=line><span class=cl><span class=s2>                details and hyperparameters like `max_position_embeddings`,
</span></span></span><span class=line><span class=cl><span class=s2>                used by SGLang for correct model initialization. This is
</span></span></span><span class=line><span class=cl><span class=s2>                the model&#39;s inherent design, not SGLang&#39;s runtime behavior.
</span></span></span><span class=line><span class=cl><span class=s2>            port: Optional port for multi-node initialization when nnodes &gt; 1.
</span></span></span><span class=line><span class=cl><span class=s2>            trust_remote_code: Whether or not to allow for custom models
</span></span></span><span class=line><span class=cl><span class=s2>                defined on the Hub in their own modeling files.
</span></span></span><span class=line><span class=cl><span class=s2>            device_mesh: Optional `DeviceMesh` object for distributed setup.
</span></span></span><span class=line><span class=cl><span class=s2>            **kwargs: Additional keyword arguments, primarily `train_tp` for
</span></span></span><span class=line><span class=cl><span class=s2>                Megatron Backend integration to initialize hybrid engine
</span></span></span><span class=line><span class=cl><span class=s2>                process groups.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>config</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_device_mesh_cpu</span> <span class=o>=</span> <span class=n>device_mesh</span>
</span></span><span class=line><span class=cl>        <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>setdefault</span><span class=p>(</span><span class=s2>&#34;SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_tool_schemas</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_tool_map</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_tool_call_parser_type</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_sgl_tools</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_function_call_parser</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_initialize_tools</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>interaction</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>BaseInteraction</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_intitalize_interaction</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># If turn on `free_cache_engine`, SGLang engine&#39;s KV cache</span>
</span></span><span class=line><span class=cl>        <span class=c1># will be freed after each `generate_sequences` call.</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=ow>not</span> <span class=p>(</span><span class=ow>not</span> <span class=n>config</span><span class=o>.</span><span class=n>enforce_eager</span> <span class=ow>and</span> <span class=n>config</span><span class=o>.</span><span class=n>free_cache_engine</span><span class=p>),</span> <span class=s2>&#34;disable CUDA graph (enforce_eager = False) if free cache engine&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;tool_schemas: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_tool_schemas</span><span class=si>}</span><span class=s2>, tool_map: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_tool_map</span><span class=si>}</span><span class=s2>, tool_call_parser_type: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_tool_call_parser_type</span><span class=si>}</span><span class=s2>, sgl_tools: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_sgl_tools</span><span class=si>}</span><span class=s2>, function_call_parser: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>_function_call_parser</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_init_distributed_env</span><span class=p>(</span><span class=n>device_mesh_cpu</span><span class=o>=</span><span class=n>device_mesh</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_verify_config</span><span class=p>(</span><span class=n>model_hf_config</span><span class=o>=</span><span class=n>model_hf_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># initialize the inference engine</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_init_inference_engine</span><span class=p>(</span><span class=n>trust_remote_code</span><span class=p>,</span> <span class=n>actor_module</span><span class=p>,</span> <span class=n>port</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_init_sampling_params</span><span class=p>(</span><span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>tokenizer</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pad_token_id</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token_id</span>
</span></span></code></pre></td></tr></table></div></div></details><h3 id=sglangrolloutasyncengine><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L124><code>SGLangRollout.AsyncEngine</code></a><a hidden class=anchor aria-hidden=true href=#sglangrolloutasyncengine>#</a></h3><p>å…³äº <code>SGLangRollout</code> è°ƒç”¨ tool çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬åœ¨ä¸‹æ–‡çš„è®­ç»ƒå¾ªç¯ä¸­å†å±•å¼€ï¼Œè¿™é‡Œå…ˆè®¨è®ºå®Œ SGLang çš„åˆå§‹åŒ–ã€‚ä¸ºäº†è°ƒç”¨ SGLang engine çš„æ¥å£ï¼Œverl è¿›è¡Œäº†ä¸€å±‚å°è£…ï¼Œå®ç°äº†æˆ‘ä»¬å¯¹ SGLang é™¤å¼€ rollout ä¹‹å¤–çš„æ‰€æœ‰æ¥å£ï¼š</p><ol><li>release and resume memory occupationï¼šåœ¨è®­ç»ƒæ—¶é‡Šæ”¾æ‰æ˜¾å­˜å ç”¨å¹¶åœ¨è®­ç»ƒåæ¢å¤ã€‚</li><li>update weights from tensorï¼šè®­ç»ƒç»“æŸåæ›´æ–°æ¨¡å‹æƒé‡ã€‚</li><li>flush cacheï¼šæ¨¡å‹å‚æ•°æ›´æ–°ååˆ·æ–° KV cacheï¼Œå› ä¸ºä¹‹å‰çš„ KV cache å·²ç»å¤±æ•ˆäº†ã€‚</li></ol><p>è¿™é‡Œæ¶‰åŠåˆ°äº†éå¸¸æ·±å…¥çš„å†…å­˜ç®¡ç†é—®é¢˜ï¼Œè¯»è€…å¯¹ SGLang engine åœ¨ verl é‡Œçš„æ˜¾å­˜ç®¡ç†æ„Ÿå…´è¶£ï¼Œæ¬¢è¿é˜…è¯»æ ‡å“¥çš„åšå®¢ <a href=https://hebiao064.github.io/rl-memory-management>optimizing Memory Usage in verl</a>ï¼Œå†™çš„éå¸¸æ·±å…¥æµ…å‡ºã€‚</p><details><summary>SGLangRollout ä½•æ—¶éœ€è¦ flush cache</summary><p>è¿™ä¸€éƒ¨åˆ†å†…å®¹éœ€è¦å•ç‹¬æ‹å‡ºæ¥è®²è®²ã€‚SGLang engine çš„ release å’Œ resume éœ€è¦ä¿ç•™ CUDA Graphï¼Œå¦åˆ™ rollout æ•ˆç‡ä¼šå¤§å¹…é™ä½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åŸºäº tom çš„ <a href=https://github.com/fzyzcjy/torch_memory_saver>torch_memory_saver</a> å®ç°äº†ç‹¬ç«‹çš„æ˜¾å­˜ç®¡ç†ã€‚ç®€å•æ¥è¯´ï¼Œæˆ‘ä»¬æœ‰ï¼š</p><ol><li><code>pause</code>ï¼›ä¿ç•™ mem savor ä½œç”¨åŸŸå†…æŒ‡å®š tensor çš„ virtual addressï¼Œä½†æ˜¯å°†å…¶ physical memory é‡Šæ”¾å›æ˜¾å­˜ç®¡ç†å™¨ã€‚</li><li><code>resume</code>ï¼›å°†å…ˆå‰ <code>pause</code> çš„ tensor é‡æ–°ç”³è¯·ä¸€å— physical memoryï¼Œå¹¶å°†å…¶ virtual address æ˜ å°„åˆ°æ–°çš„ physical memoryã€‚</li></ol><p>æ³¨æ„ï¼Œæ•´ä¸ª pause å’Œ resume çš„è¿‡ç¨‹ä¸­ï¼Œtensor çš„ virtual address ä¸ä¼šå‘ç”Ÿå˜åŒ–ï¼Œåªæ˜¯è¿™å— virtual address æ˜ å°„åˆ°çš„ physical memory æ”¹å˜äº†ã€‚å› æ­¤ï¼ŒCUDA Graph å¹¶æ²¡æœ‰å¤±æ•ˆï¼Œä¸å˜çš„ virtual address è®©è®¡ç®—æµä»æ—§å¯ä»¥æ­£å¸¸æ‰§è¡Œã€‚</p><p>verl å†…çš„ <code>release_memory_occupation</code> å’Œ <code>resume_memory_occupation</code> å°±æ˜¯åŸºäº <code>pause</code> å’Œ <code>resume</code> å®ç°çš„ã€‚å¬ä¸Šå»æ˜¯ä¸ªå®Œç¾çš„æ•…äº‹ï¼Œæˆ‘ä»¬ç”šè‡³å®ç°äº† <a href=https://github.com/fzyzcjy/torch_memory_saver/pull/20>mutli-stage çš„æ˜¾å­˜ç®¡ç†</a>ï¼Œèƒ½å¤Ÿç‹¬ç«‹ release å’Œ resume kv cache å’Œ model weightsã€‚</p><p>ä¸è¿‡ï¼Œå¯¹äº kv cache è€Œè¨€ï¼Œåœ¨ kv cache è¢« release æ‰ä¹‹åï¼Œå®é™…ä¸Š kv cache çš„ tensor ä»æ—§ä¿ç•™ï¼Œåªæ˜¯å…¶ virtual address æ˜ å°„åˆ°çš„ physical memory è¢«é‡Šæ”¾äº†ã€‚ä¸æ­¤åŒæ—¶ï¼Œradix tree ä»æ—§ç´¢å¼•ç€æ•´ä¸ª kv cacheã€‚å½“ kv cache è¢« resume ä¹‹åï¼Œä¸€æ–¹é¢ä¹‹å‰ç‰©ç†å†…å­˜ä¸Šä¹‹å‰çš„ kv cache å·²ç»ä¸å¤å­˜åœ¨äº†ï¼Œå¦ä¸€æ–¹é¢æ¨¡å‹çš„å‚æ•°ä¹Ÿè¢«æ›´æ–°ã€‚å‡ºäºè¿™ä¸¤ç‚¹ï¼Œæˆ‘ä»¬ä¸€å®šè¦ä½¿ç”¨ flush cache æ¥å£æ¥åˆ·æ–° kv cache çš„ç´¢å¼•ï¼ˆradix treeï¼‰ã€‚</p><p>è¿™é‡Œåˆæœ‰ä¸ªéå¸¸æœ‰è¶£çš„è®¾è®¡ã€‚ä¹ä¸€æƒ³ kv cache çš„ç®¡ç†è¿™ä¹ˆéº»çƒ¦ï¼Œè¿˜è¦ flushï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥ delete kv cache ä»¥åŠ delete model weights å†é‡æ–°åˆå§‹åŒ–å‘¢ï¼Ÿæ˜¾ç„¶ï¼Œè¿™æ ·æ²¡æ³•åˆ©ç”¨å·²æœ‰çš„ cuda graphï¼Œéå¸¸æ¶ˆè€—æ—¶é—´ã€‚ä¿ç•™ virtual address ä¸å˜ä½†æ˜¯æ›´æ¢ physical memory çš„æ–¹æ¡ˆï¼Œè®© verl èƒ½å¤ŸæŒç»­åˆ©ç”¨å·²å»ºå¥½çš„ cuda graphã€‚</p><p>æœ€åä¸€ä¸ªé—®é¢˜ï¼Œä¸€å…±è¦å‡ æ¬¡ flush cache å‘¢ï¼Ÿæˆ‘ä¸ªäººç†è§£ï¼Œåœ¨ä¸€æ•´ä¸ª training engine è¢« pauseï¼Œresume ç„¶å update weights çš„è¿‡ç¨‹ä¸­ï¼Œå¿…é¡»è¦æœ‰ä¸€æ¬¡ flush cache æ¥åˆ·æ–° kv cache çš„ç´¢å¼•ï¼Œåªæ˜¯ verl å½“ä¸­ä¸ºäº†ä¿é™©ï¼Œåˆ·æ–°äº†å¾ˆå¤šæ¬¡ç½¢äº†ã€‚</p></details><details><summary>SGLangRollout.AsyncEngine æºç </summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>AsyncEngine</span><span class=p>(</span><span class=n>sglang</span><span class=o>.</span><span class=n>srt</span><span class=o>.</span><span class=n>entrypoints</span><span class=o>.</span><span class=n>engine</span><span class=o>.</span><span class=n>Engine</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># default to use dummy load format, which need to reload weights in first time</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_need_reload</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>async</span> <span class=k>def</span> <span class=nf>release_memory_occupation</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Release GPU occupation temporarily.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>obj</span> <span class=o>=</span> <span class=n>ReleaseMemoryOccupationReqInput</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=k>await</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer_manager</span><span class=o>.</span><span class=n>release_memory_occupation</span><span class=p>(</span><span class=n>obj</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>async</span> <span class=k>def</span> <span class=nf>resume_memory_occupation</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=k>await</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer_manager</span><span class=o>.</span><span class=n>resume_memory_occupation</span><span class=p>(</span><span class=n>obj</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>async</span> <span class=k>def</span> <span class=nf>update_weights_from_tensor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>named_tensors</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]],</span>  <span class=c1># noqa: UP006</span>
</span></span><span class=line><span class=cl>        <span class=n>load_format</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>flush_cache</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Update weights from distributed source. If there are going to be more updates, set `flush_cache` to be false
</span></span></span><span class=line><span class=cl><span class=s2>        to avoid duplicated cache cleaning operation.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>obj</span> <span class=o>=</span> <span class=n>UpdateWeightsFromTensorReqInput</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>serialized_named_tensors</span><span class=o>=</span><span class=p>[</span><span class=n>MultiprocessingSerializer</span><span class=o>.</span><span class=n>serialize</span><span class=p>(</span><span class=n>named_tensors</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>server_args</span><span class=o>.</span><span class=n>tp_size</span><span class=p>)],</span>
</span></span><span class=line><span class=cl>            <span class=n>load_format</span><span class=o>=</span><span class=n>load_format</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>flush_cache</span><span class=o>=</span><span class=n>flush_cache</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=k>await</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer_manager</span><span class=o>.</span><span class=n>update_weights_from_tensor</span><span class=p>(</span><span class=n>obj</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>async</span> <span class=k>def</span> <span class=nf>flush_cache</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=k>await</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer_manager</span><span class=o>.</span><span class=n>flush_cache</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div></details><h3 id=sglangrollout_init_inference_engine><a href=https://github.com/volcengine/verl/blob/e67ee86f8b94bfa141da95402a254966733cba08/verl/workers/rollout/sglang_rollout/sglang_rollout.py#L325><code>SGLangRollout._init_inference_engine()</code></a><a hidden class=anchor aria-hidden=true href=#sglangrollout_init_inference_engine>#</a></h3><p><code>SGLangRollout._init_inference_engine()</code> åˆå§‹åŒ–äº†å°è£…çš„ <code>AsyncEngine</code>ã€‚</p><details><summary>SGLangRollout._init_inference_engine æºç </summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_init_inference_engine</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=p>,</span> <span class=n>actor_module</span><span class=p>,</span> <span class=n>port</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># initialize the inference engine</span>
</span></span><span class=line><span class=cl>    <span class=n>nnodes</span> <span class=o>=</span> <span class=o>-</span><span class=p>(</span><span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>_tp_size</span> <span class=o>//</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>visible_devices_set</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>nnodes</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>ip</span> <span class=o>=</span> <span class=n>get_ip</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>port</span> <span class=o>=</span> <span class=n>get_open_port</span><span class=p>()</span> <span class=k>if</span> <span class=n>port</span> <span class=ow>is</span> <span class=kc>None</span> <span class=k>else</span> <span class=n>port</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=n>ip</span><span class=p>,</span> <span class=n>port</span><span class=p>]</span> <span class=o>=</span> <span class=n>broadcast_pyobj</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=n>ip</span><span class=p>,</span> <span class=n>port</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=n>rank</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_rank</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>dist_group</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_device_mesh_cpu</span><span class=o>.</span><span class=n>get_group</span><span class=p>(</span><span class=s2>&#34;tp&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>src</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_device_mesh_cpu</span><span class=p>[</span><span class=s2>&#34;tp&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>mesh</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>force_cpu_device</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>dist_init_addr</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;[</span><span class=si>{</span><span class=n>ip</span><span class=si>}</span><span class=s2>]:</span><span class=si>{</span><span class=n>port</span><span class=si>}</span><span class=s2>&#34;</span> <span class=k>if</span> <span class=n>is_ipv6</span><span class=p>(</span><span class=n>ip</span><span class=p>)</span> <span class=k>else</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>ip</span><span class=si>}</span><span class=s2>:</span><span class=si>{</span><span class=n>port</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dist_init_addr</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>load_format</span> <span class=o>=</span> <span class=s2>&#34;dummy&#34;</span> <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>load_format</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s2>&#34;dummy&#34;</span><span class=p>)</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>load_format</span>
</span></span><span class=line><span class=cl>    <span class=n>tp_size_per_node</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_tp_size</span> <span class=o>//</span> <span class=n>nnodes</span>
</span></span><span class=line><span class=cl>    <span class=n>node_rank</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_tp_rank</span> <span class=o>//</span> <span class=n>tp_size_per_node</span>
</span></span><span class=line><span class=cl>    <span class=n>first_rank_in_node</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_tp_rank</span> <span class=o>%</span> <span class=n>tp_size_per_node</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>first_rank_in_node</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>rank</span> <span class=o>=</span> <span class=n>dist</span><span class=o>.</span><span class=n>get_rank</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;SGLANG_BLOCK_NONZERO_RANK_CHILDREN&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&#34;0&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_engine</span> <span class=o>=</span> <span class=n>AsyncEngine</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>model_path</span><span class=o>=</span><span class=n>actor_module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>dtype</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>dtype</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>mem_fraction_static</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>gpu_memory_utilization</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>enable_memory_saver</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>base_gpu_id</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>gpu_id_step</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>tp_size</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_tp_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>node_rank</span><span class=o>=</span><span class=n>node_rank</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>load_format</span><span class=o>=</span><span class=n>load_format</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>dist_init_addr</span><span class=o>=</span><span class=n>dist_init_addr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>nnodes</span><span class=o>=</span><span class=n>nnodes</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=c1># NOTE(linjunrong): add rank to prevent SGLang generate same port inside PortArgs.init_new</span>
</span></span><span class=line><span class=cl>            <span class=c1># when random.seed is being set during training</span>
</span></span><span class=line><span class=cl>            <span class=n>port</span><span class=o>=</span><span class=mi>30000</span> <span class=o>+</span> <span class=n>rank</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=c1># NOTE(Chenyang): if you want to debug the SGLang engine output</span>
</span></span><span class=line><span class=cl>            <span class=c1># please set the following parameters</span>
</span></span><span class=line><span class=cl>            <span class=c1># Otherwise, it will make the engine run too slow</span>
</span></span><span class=line><span class=cl>            <span class=c1># log_level=&#34;INFO&#34;,</span>
</span></span><span class=line><span class=cl>            <span class=c1># log_requests=True,</span>
</span></span><span class=line><span class=cl>            <span class=c1># log_requests_level=2,</span>
</span></span><span class=line><span class=cl>            <span class=c1># max_running_requests=1,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_engine</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>sharding_manager</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>is_sleep</span> <span class=o>=</span> <span class=kc>True</span>
</span></span></code></pre></td></tr></table></div></div></details><p>è¿™é‡Œæœ€å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒSGLang engine å¹¶æ²¡æœ‰ä¸¥æ ¼å®ç° verl æ‰€å¸Œæœ›çš„ SPMD æ¨¡å¼ï¼ˆæ¯ä¸ª GPU ä¸Šçš„è¿›ç¨‹å®Œå…¨ä¸€æ ·ï¼‰ï¼Œè€Œæ˜¯é‡‡ç”¨äº† mock çš„ SPMDã€‚ä¸¾ä¾‹æ¥è¯´ï¼Œå‡è®¾ tp size = 4ï¼ŒæŒ‰ç…§ verl çš„è®¾è®¡ï¼Œåº”è¯¥è¦ 4 å¼  GPU ä¸Šæ¯ä¸ªéƒ½è¿è¡Œä¸€ä¸ªç›¸åŒçš„ SGLang engineã€‚å®é™…ä¸Šçš„å®ç°æ˜¯åœ¨ GPU 0 ä¸Šå¯åŠ¨ä¸€ä¸ªè¿›ç¨‹å æ®å…¨éƒ¨ GPUï¼Œè€Œ GPU 1 2 3 ä¸Šä»…ä»…ä¿ç•™ä¸€ä¸ªç©ºè¿›ç¨‹ <code>None</code>ã€‚è™½ç„¶ verl team èµ·åˆè®¾å®šä¸­è®¤ä¸ºä¸¥æ ¼çš„ SPMD æ„ä¹‰å·¨å¤§ï¼Œä½†å®é™…ä½¿ç”¨ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸º mock çš„ SPMD å·²ç»è¶³å¤Ÿæ»¡è¶³æ€§èƒ½éœ€æ±‚ã€‚</p><p>ã€TODOã€‘ è¿™ä¹ˆæè¿°å¯èƒ½ä¸ä¸¥è°¨ã€‚</p><h3 id=taskrunnerrun><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/main_ppo.py#L64><code>TaskRunner.run()</code></a><a hidden class=anchor aria-hidden=true href=#taskrunnerrun>#</a></h3><p>å¾€ä¸‹èµ°äº†è¿™ä¹ˆå¤šå±‚ï¼Œæˆ‘ä»¬ç»ˆäºèƒ½å¤Ÿç»§ç»­å›åˆ° <code>TaskRunner</code> ç±»ã€‚ğŸ˜­</p><p>ã€TODOã€‘ä¸Šæ–‡å…¶å®ä¸»è¦æ˜¯ Actor Rolloutï¼Œè¿˜æ²¡æœ‰å…·ä½“è¯´ Actor çš„ training forward and backwardã€‚ä»¥åŠ Referenceï¼Œreward å’Œ critic çš„ training forward and backwardã€‚</p><ol><li>åŠ è½½ã€è§£æå’ŒéªŒè¯è®­ç»ƒä»»åŠ¡çš„é…ç½®ï¼ˆä½¿ç”¨ <code>OmegaConf</code>ï¼‰ï¼Œç¡®ä¿æ‰€æœ‰å‚æ•°çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚</li><li>å°†æ¨¡å‹æ–‡ä»¶ä»è¿œç¨‹è·¯å¾„å¤åˆ¶åˆ°æœ¬åœ°ï¼Œç¡®ä¿æ‰€æœ‰ Worker éƒ½å¯ä»¥è®¿é—®ã€‚</li><li>ç»„ä»¶åˆå§‹åŒ–ï¼š<ul><li>åˆå§‹åŒ– Tokenizer å’Œ Processorï¼Œç”¨äºæ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®çš„å¤„ç†ã€‚</li><li>æ ¹æ®é…ç½®ä¸­æŒ‡å®šçš„ Actor ç­–ç•¥ï¼ˆå¦‚ <code>fsdp</code> æˆ– <code>megatron</code>ï¼‰ï¼ŒåŠ¨æ€é€‰æ‹©ç›¸åº”çš„ Worker ç±»ï¼ˆä¾‹å¦‚ <code>ActorRolloutRefWorker</code> å’Œ <code>CriticWorker</code>ï¼‰ï¼Œå¹¶ç¡®å®šä½¿ç”¨çš„ <code>RayWorkerGroup</code> ç±»å‹ã€‚</li><li>å®šä¹‰ Ray èµ„æºæ± çš„è§„æ ¼å’Œè§’è‰²åˆ°èµ„æºæ± çš„æ˜ å°„ï¼Œç”¨äº GPU èµ„æºçš„åˆ†é…å’Œç®¡ç†ã€‚</li><li>åŠ è½½ç”¨äºè®­ç»ƒå’ŒéªŒè¯çš„å¥–åŠ±æ¨¡å‹ã€‚</li><li>åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼Œä»¥åŠè®­ç»ƒæ•°æ®é‡‡æ ·å™¨ã€‚</li></ul></li><li>åˆ›å»º <code>RayPPOTrainer</code> å®ä¾‹ï¼Œå®ƒæ˜¯ç®¡ç†æ‰€æœ‰è®¡ç®—èµ„æºå’Œè®­ç»ƒæµç¨‹çš„ä¸­å¤®åè°ƒå™¨ã€‚</li><li>è°ƒç”¨ <code>RayPPOTrainer</code> çš„ <code>init_workers()</code> æ–¹æ³•ï¼Œå°†é…ç½®çš„ Worker ç±»å®ä¾‹åŒ–åˆ° Ray é›†ç¾¤çš„ GPU ä¸Šï¼Œä¸ºå®é™…è®¡ç®—åšå‡†å¤‡ã€‚</li><li>è°ƒç”¨ <code>RayPPOTrainer</code> çš„ <code>fit()</code> æ–¹æ³•ï¼Œå¯åŠ¨ PPO è®­ç»ƒå¾ªç¯ã€‚</li></ol><details><summary>TaskRunner.run æºç </summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@ray.remote</span><span class=p>(</span><span class=n>num_cpus</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TaskRunner</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>run</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>pprint</span> <span class=kn>import</span> <span class=n>pprint</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>omegaconf</span> <span class=kn>import</span> <span class=n>OmegaConf</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.utils.fs</span> <span class=kn>import</span> <span class=n>copy_to_local</span>
</span></span><span class=line><span class=cl>        <span class=kn>import</span> <span class=nn>socket</span>
</span></span><span class=line><span class=cl>        <span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;TaskRunner hostname: </span><span class=si>{</span><span class=n>socket</span><span class=o>.</span><span class=n>gethostname</span><span class=p>()</span><span class=si>}</span><span class=s2>, PID: </span><span class=si>{</span><span class=n>os</span><span class=o>.</span><span class=n>getpid</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pprint</span><span class=p>(</span><span class=n>OmegaConf</span><span class=o>.</span><span class=n>to_container</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=n>resolve</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>OmegaConf</span><span class=o>.</span><span class=n>resolve</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># æ¨¡å‹ä¸‹è½½</span>
</span></span><span class=line><span class=cl>        <span class=n>local_path</span> <span class=o>=</span> <span class=n>copy_to_local</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>path</span><span class=p>,</span> <span class=n>use_shm</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;use_shm&#34;</span><span class=p>,</span> <span class=kc>False</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Tokenizer å’Œ Processor åˆå§‹åŒ–</span>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.utils</span> <span class=kn>import</span> <span class=n>hf_processor</span><span class=p>,</span> <span class=n>hf_tokenizer</span>
</span></span><span class=line><span class=cl>        <span class=n>trust_remote_code</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;trust_remote_code&#34;</span><span class=p>,</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>hf_tokenizer</span><span class=p>(</span><span class=n>local_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>processor</span> <span class=o>=</span> <span class=n>hf_processor</span><span class=p>(</span><span class=n>local_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=n>trust_remote_code</span><span class=p>,</span> <span class=n>use_fast</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Worker ç±»å‹é€‰æ‹©</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;fsdp&#34;</span><span class=p>,</span> <span class=s2>&#34;fsdp2&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=kn>from</span> <span class=nn>verl.single_controller.ray</span> <span class=kn>import</span> <span class=n>RayWorkerGroup</span>
</span></span><span class=line><span class=cl>            <span class=kn>from</span> <span class=nn>verl.workers.fsdp_workers</span> <span class=kn>import</span> <span class=n>ActorRolloutRefWorker</span><span class=p>,</span> <span class=n>AsyncActorRolloutRefWorker</span><span class=p>,</span> <span class=n>CriticWorker</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_rollout_cls</span> <span class=o>=</span> <span class=n>AsyncActorRolloutRefWorker</span> <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>mode</span> <span class=o>==</span> <span class=s2>&#34;async&#34;</span> <span class=k>else</span> <span class=n>ActorRolloutRefWorker</span>
</span></span><span class=line><span class=cl>            <span class=n>ray_worker_group_cls</span> <span class=o>=</span> <span class=n>RayWorkerGroup</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span> <span class=o>==</span> <span class=s2>&#34;megatron&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>assert</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>strategy</span> <span class=o>==</span> <span class=n>config</span><span class=o>.</span><span class=n>critic</span><span class=o>.</span><span class=n>strategy</span>
</span></span><span class=line><span class=cl>            <span class=kn>from</span> <span class=nn>verl.single_controller.ray.megatron</span> <span class=kn>import</span> <span class=n>NVMegatronRayWorkerGroup</span>
</span></span><span class=line><span class=cl>            <span class=kn>from</span> <span class=nn>verl.workers.megatron_workers</span> <span class=kn>import</span> <span class=n>ActorRolloutRefWorker</span><span class=p>,</span> <span class=n>AsyncActorRolloutRefWorker</span><span class=p>,</span> <span class=n>CriticWorker</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_rollout_cls</span> <span class=o>=</span> <span class=n>AsyncActorRolloutRefWorker</span> <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>mode</span> <span class=o>==</span> <span class=s2>&#34;async&#34;</span> <span class=k>else</span> <span class=n>ActorRolloutRefWorker</span>
</span></span><span class=line><span class=cl>            <span class=n>ray_worker_group_cls</span> <span class=o>=</span> <span class=n>NVMegatronRayWorkerGroup</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>NotImplementedError</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.trainer.ppo.ray_trainer</span> <span class=kn>import</span> <span class=n>ResourcePoolManager</span><span class=p>,</span> <span class=n>Role</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># è§’è‰²åˆ° Worker ç±»çš„æ˜ å°„</span>
</span></span><span class=line><span class=cl>        <span class=n>role_worker_mapping</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>:</span> <span class=n>ray</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>actor_rollout_cls</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>Role</span><span class=o>.</span><span class=n>Critic</span><span class=p>:</span> <span class=n>ray</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>CriticWorker</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># èµ„æºæ± è§„æ ¼å’Œè§’è‰²æ˜ å°„</span>
</span></span><span class=line><span class=cl>        <span class=n>global_pool_id</span> <span class=o>=</span> <span class=s2>&#34;global_pool&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>resource_pool_spec</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>global_pool_id</span><span class=p>:</span> <span class=p>[</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=o>.</span><span class=n>n_gpus_per_node</span><span class=p>]</span> <span class=o>*</span> <span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=o>.</span><span class=n>nnodes</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>mapping</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>:</span> <span class=n>global_pool_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>Role</span><span class=o>.</span><span class=n>Critic</span><span class=p>:</span> <span class=n>global_pool_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Reward Model Worker çš„åˆå§‹åŒ–</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>reward_model</span><span class=o>.</span><span class=n>enable</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>reward_model</span><span class=o>.</span><span class=n>strategy</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;fsdp&#34;</span><span class=p>,</span> <span class=s2>&#34;fsdp2&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>                <span class=kn>from</span> <span class=nn>verl.workers.fsdp_workers</span> <span class=kn>import</span> <span class=n>RewardModelWorker</span>
</span></span><span class=line><span class=cl>            <span class=k>elif</span> <span class=n>config</span><span class=o>.</span><span class=n>reward_model</span><span class=o>.</span><span class=n>strategy</span> <span class=o>==</span> <span class=s2>&#34;megatron&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=kn>from</span> <span class=nn>verl.workers.megatron_workers</span> <span class=kn>import</span> <span class=n>RewardModelWorker</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>raise</span> <span class=ne>NotImplementedError</span>
</span></span><span class=line><span class=cl>            <span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>RewardModel</span><span class=p>]</span> <span class=o>=</span> <span class=n>ray</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>RewardModelWorker</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>RewardModel</span><span class=p>]</span> <span class=o>=</span> <span class=n>global_pool_id</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Reference Policy Worker çš„åˆå§‹åŒ–</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>algorithm</span><span class=o>.</span><span class=n>use_kl_in_reward</span> <span class=ow>or</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>actor</span><span class=o>.</span><span class=n>use_kl_loss</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>RefPolicy</span><span class=p>]</span> <span class=o>=</span> <span class=n>ray</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>ActorRolloutRefWorker</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>RefPolicy</span><span class=p>]</span> <span class=o>=</span> <span class=n>global_pool_id</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># åŠ è½½å¥–åŠ±ç®¡ç†å™¨</span>
</span></span><span class=line><span class=cl>        <span class=n>reward_fn</span> <span class=o>=</span> <span class=n>load_reward_manager</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>num_examine</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=o>**</span><span class=n>config</span><span class=o>.</span><span class=n>reward_model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;reward_kwargs&#34;</span><span class=p>,</span> <span class=p>{}))</span>
</span></span><span class=line><span class=cl>        <span class=n>val_reward_fn</span> <span class=o>=</span> <span class=n>load_reward_manager</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>num_examine</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=o>**</span><span class=n>config</span><span class=o>.</span><span class=n>reward_model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;reward_kwargs&#34;</span><span class=p>,</span> <span class=p>{}))</span>
</span></span><span class=line><span class=cl>        <span class=n>resource_pool_manager</span> <span class=o>=</span> <span class=n>ResourcePoolManager</span><span class=p>(</span><span class=n>resource_pool_spec</span><span class=o>=</span><span class=n>resource_pool_spec</span><span class=p>,</span> <span class=n>mapping</span><span class=o>=</span><span class=n>mapping</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>verl.utils.dataset.rl_dataset</span> <span class=kn>import</span> <span class=n>collate_fn</span><span class=p>,</span> <span class=n>create_rl_dataset</span><span class=p>,</span> <span class=n>create_rl_sampler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†</span>
</span></span><span class=line><span class=cl>        <span class=n>train_dataset</span> <span class=o>=</span> <span class=n>create_rl_dataset</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>train_files</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>processor</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>val_dataset</span> <span class=o>=</span> <span class=n>create_rl_dataset</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>val_files</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>processor</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>train_sampler</span> <span class=o>=</span> <span class=n>create_rl_sampler</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>train_dataset</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># åˆå§‹åŒ– PPO è®­ç»ƒå™¨</span>
</span></span><span class=line><span class=cl>        <span class=n>trainer</span> <span class=o>=</span> <span class=n>RayPPOTrainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>config</span><span class=o>=</span><span class=n>config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>processor</span><span class=o>=</span><span class=n>processor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>role_worker_mapping</span><span class=o>=</span><span class=n>role_worker_mapping</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>resource_pool_manager</span><span class=o>=</span><span class=n>resource_pool_manager</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>ray_worker_group_cls</span><span class=o>=</span><span class=n>ray_worker_group_cls</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>reward_fn</span><span class=o>=</span><span class=n>reward_fn</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>val_reward_fn</span><span class=o>=</span><span class=n>val_reward_fn</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>train_dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>val_dataset</span><span class=o>=</span><span class=n>val_dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>collate_fn</span><span class=o>=</span><span class=n>collate_fn</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>train_sampler</span><span class=o>=</span><span class=n>train_sampler</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>device_name</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=o>.</span><span class=n>device</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># åˆå§‹åŒ–è®­ç»ƒå™¨çš„ Workers</span>
</span></span><span class=line><span class=cl>        <span class=n>trainer</span><span class=o>.</span><span class=n>init_workers</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># å¯åŠ¨è®­ç»ƒè¿‡ç¨‹</span>
</span></span><span class=line><span class=cl>        <span class=n>trainer</span><span class=o>.</span><span class=n>fit</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div></details><h3 id=rayppotrainer__init__><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L277><code>RayPPOTrainer.__init__()</code></a><a hidden class=anchor aria-hidden=true href=#rayppotrainer__init__>#</a></h3><ol><li>ä¿å­˜ä¼ å…¥çš„é…ç½®å¯¹è±¡ã€tokenizerã€processorã€è§’è‰²åˆ° Worker çš„æ˜ å°„ã€èµ„æºæ± ç®¡ç†å™¨ä»¥åŠ WorkerGroup ç±»ã€‚</li><li>æ ¹æ®é…ç½®å¯ç”¨æˆ–ç¦ç”¨ Criticã€Reference Policyã€Reward Model å’Œ Hybrid Engine ç­‰åŠŸèƒ½ç»„ä»¶ã€‚</li><li>è°ƒç”¨ <code>_validate_config()</code> æ–¹æ³•éªŒè¯é…ç½®çš„åˆç†æ€§ã€‚</li><li>å­˜å‚¨è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€collate å‡½æ•°å’Œè®­ç»ƒæ•°æ®é‡‡æ ·å™¨ã€‚</li></ol><details><summary>RayPPOTrainer æºç </summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>RayPPOTrainer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># TODO: support each role have individual ray_worker_group_cls,</span>
</span></span><span class=line><span class=cl>    <span class=c1># i.e., support different backend of different role</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>role_worker_mapping</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=n>Role</span><span class=p>,</span> <span class=n>WorkerType</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>resource_pool_manager</span><span class=p>:</span> <span class=n>ResourcePoolManager</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>ray_worker_group_cls</span><span class=p>:</span> <span class=n>RayWorkerGroup</span> <span class=o>=</span> <span class=n>RayWorkerGroup</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>processor</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>reward_fn</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>val_reward_fn</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>train_dataset</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Dataset</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>val_dataset</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Dataset</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>collate_fn</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>train_sampler</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Sampler</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>device_name</span><span class=o>=</span><span class=s2>&#34;cuda&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Initialize distributed PPO trainer with Ray backend.
</span></span></span><span class=line><span class=cl><span class=s2>        Note that this trainer runs on the driver process on a single CPU/GPU node.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            config: Configuration object containing training parameters.
</span></span></span><span class=line><span class=cl><span class=s2>            tokenizer: Tokenizer used for encoding and decoding text.
</span></span></span><span class=line><span class=cl><span class=s2>            role_worker_mapping (dict[Role, WorkerType]): Mapping from roles to worker classes.
</span></span></span><span class=line><span class=cl><span class=s2>            resource_pool_manager (ResourcePoolManager): Manager for Ray resource pools.
</span></span></span><span class=line><span class=cl><span class=s2>            ray_worker_group_cls (RayWorkerGroup, optional): Class for Ray worker groups. Defaults to RayWorkerGroup.
</span></span></span><span class=line><span class=cl><span class=s2>            processor: Optional data processor, used for multimodal data.
</span></span></span><span class=line><span class=cl><span class=s2>            reward_fn: Function for computing rewards during training.
</span></span></span><span class=line><span class=cl><span class=s2>            val_reward_fn: Function for computing rewards during validation.
</span></span></span><span class=line><span class=cl><span class=s2>            train_dataset (Optional[Dataset], optional): Training dataset. Defaults to None.
</span></span></span><span class=line><span class=cl><span class=s2>            val_dataset (Optional[Dataset], optional): Validation dataset. Defaults to None.
</span></span></span><span class=line><span class=cl><span class=s2>            collate_fn: Function to collate data samples into batches.
</span></span></span><span class=line><span class=cl><span class=s2>            train_sampler (Optional[Sampler], optional): Sampler for the training dataset. Defaults to None.
</span></span></span><span class=line><span class=cl><span class=s2>            device_name (str, optional): Device name for training (e.g., &#34;cuda&#34;, &#34;cpu&#34;). Defaults to &#34;cuda&#34;.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Store the tokenizer for text processing</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>tokenizer</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>processor</span> <span class=o>=</span> <span class=n>processor</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>config</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>reward_fn</span> <span class=o>=</span> <span class=n>reward_fn</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>val_reward_fn</span> <span class=o>=</span> <span class=n>val_reward_fn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hybrid_engine</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>hybrid_engine</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>hybrid_engine</span><span class=p>,</span> <span class=s2>&#34;Currently, only support hybrid engine&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>hybrid_engine</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>assert</span> <span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span> <span class=ow>in</span> <span class=n>role_worker_mapping</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>role_worker_mapping</span><span class=o>.</span><span class=n>keys</span><span class=p>()</span><span class=si>=}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>role_worker_mapping</span> <span class=o>=</span> <span class=n>role_worker_mapping</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span> <span class=o>=</span> <span class=n>resource_pool_manager</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>use_reference_policy</span> <span class=o>=</span> <span class=n>Role</span><span class=o>.</span><span class=n>RefPolicy</span> <span class=ow>in</span> <span class=n>role_worker_mapping</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>use_rm</span> <span class=o>=</span> <span class=n>Role</span><span class=o>.</span><span class=n>RewardModel</span> <span class=ow>in</span> <span class=n>role_worker_mapping</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ray_worker_group_cls</span> <span class=o>=</span> <span class=n>ray_worker_group_cls</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>device_name</span> <span class=o>=</span> <span class=n>device_name</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>validation_generations_logger</span> <span class=o>=</span> <span class=n>ValidationGenerationsLogger</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># if ref_in_actor is True, the reference policy will be actor without lora applied</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ref_in_actor</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;lora_rank&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># define in-reward KL control</span>
</span></span><span class=line><span class=cl>        <span class=c1># kl loss control currently not suppoorted</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>algorithm</span><span class=o>.</span><span class=n>use_kl_in_reward</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>kl_ctrl_in_reward</span> <span class=o>=</span> <span class=n>core_algos</span><span class=o>.</span><span class=n>get_kl_controller</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>algorithm</span><span class=o>.</span><span class=n>kl_ctrl</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>algorithm</span><span class=o>.</span><span class=n>adv_estimator</span> <span class=o>==</span> <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>GAE</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>use_critic</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>algorithm</span><span class=o>.</span><span class=n>adv_estimator</span> <span class=ow>in</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>GRPO</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>GRPO_PASSK</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>REINFORCE_PLUS_PLUS</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>REMAX</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>RLOO</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>OPO</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>AdvantageEstimator</span><span class=o>.</span><span class=n>REINFORCE_PLUS_PLUS_BASELINE</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>use_critic</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>NotImplementedError</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_validate_config</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_create_dataloader</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span> <span class=n>val_dataset</span><span class=p>,</span> <span class=n>collate_fn</span><span class=p>,</span> <span class=n>train_sampler</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></details><h3 id=rayppotrainerinit_workers><a href=https://github.com/volcengine/verl/blob/76f63cffa5081564d8fea93a1cb3ce8bd5bdcc39/verl/trainer/ppo/ray_trainer.py#L715><code>RayPPOTrainer.init_workers()</code></a><a hidden class=anchor aria-hidden=true href=#rayppotrainerinit_workers>#</a></h3><p><code>init_workers()</code> å‡½æ•°è´Ÿè´£åœ¨ Ray é›†ç¾¤ä¸Šå®ä¾‹åŒ–å’Œåˆå§‹åŒ– ActorRolloutã€Criticã€Reference Policy å’Œ Reward Model Workersã€‚</p><ol><li><strong>åˆ›å»ºèµ„æºæ± </strong>ï¼šé€šè¿‡ <code>ResourcePoolManager</code> åˆ›å»º Ray èµ„æºæ± ã€‚</li><li><strong>åˆå§‹åŒ–èµ„æºæ± åˆ°ç±»çš„æ˜ å°„</strong>ï¼šä¸ºæ¯ä¸ªèµ„æºæ± åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨ä¸åŒè§’è‰² Worker çš„ <code>RayClassWithInitArgs</code> åŒ…è£…å™¨ã€‚<code>RayClassWithInitArgs</code> ç”¨äºå»¶è¿Ÿåˆå§‹åŒ– Workerï¼Œå­˜å‚¨äº† Worker çš„ç±»å’Œåˆå§‹åŒ–å‚æ•°ã€‚</li><li><strong>åˆ›å»ºä¸åŒè§’è‰²çš„ Worker çš„ <code>RayClassWithInitArgs</code> å®ä¾‹</strong>ï¼šæ ¹æ®é…ç½®å¯ç”¨æƒ…å†µï¼Œä¸º ActorRolloutã€Criticã€Reference Policy å’Œ Reward Model åˆ›å»ºå¯¹åº”çš„ <code>RayClassWithInitArgs</code> å®ä¾‹ã€‚</li><li><strong>åˆå§‹åŒ– WorkerGroup</strong>ï¼šéå†æ‰€æœ‰èµ„æºæ± ï¼Œå°†åŒä¸€èµ„æºæ± ä¸­çš„å¤šä¸ª Worker ç±»é€šè¿‡ <code>create_colocated_worker_cls</code> ç»„åˆæˆä¸€ä¸ªå…±ç½®ç±»ï¼Œç„¶åå®ä¾‹åŒ– <code>RayWorkerGroup</code>ã€‚<code>RayWorkerGroup</code> è´Ÿè´£åœ¨å¤šä¸ª GPU ä¸Šå¯åŠ¨å¤šä¸ª Worker å®ä¾‹ã€‚æœ€åè°ƒç”¨ <code>spawn()</code> æ–¹æ³•åœ¨ Ray ä¸­å®é™…åˆ›å»º Worker å®ä¾‹ã€‚</li><li><strong>åˆå§‹åŒ–å„ä¸ª Worker</strong>ï¼šæ ¹æ®è§’è‰²ä»åˆ›å»ºçš„ WorkerGroup å­—å…¸ä¸­è·å–å¯¹åº”çš„ WorkerGroupï¼Œå¹¶è°ƒç”¨å…¶ <code>init_model()</code> æ–¹æ³•ï¼ŒæŒ‰ç…§ä¾èµ–å…³ç³»ä¾æ¬¡åˆå§‹åŒ–ä¸åŒçš„ Worker æ¨¡å—ã€‚ActorRollout Worker é€šå¸¸æœ€ååˆå§‹åŒ–ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€‚</li></ol><details><summary>RayPPOTrainer.init_workers æºç </summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span><span class=lnt>86
</span><span class=lnt>87
</span><span class=lnt>88
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>init_workers</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Initialize distributed training workers using Ray backend.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Creates:
</span></span></span><span class=line><span class=cl><span class=s2>        1. Ray resource pools from configuration
</span></span></span><span class=line><span class=cl><span class=s2>        2. Worker groups for each role (actor, critic, etc.)
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>create_resource_pool</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span> <span class=o>=</span> <span class=p>{</span><span class=n>pool</span><span class=p>:</span> <span class=p>{}</span> <span class=k>for</span> <span class=n>pool</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>resource_pool_dict</span><span class=o>.</span><span class=n>values</span><span class=p>()}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create actor and rollout</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>hybrid_engine</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>resource_pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>get_resource_pool</span><span class=p>(</span><span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>actor_rollout_cls</span> <span class=o>=</span> <span class=n>RayClassWithInitArgs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=bp>cls</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>ActorRollout</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>role</span><span class=o>=</span><span class=s2>&#34;actor_rollout&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=p>[</span><span class=n>resource_pool</span><span class=p>][</span><span class=s2>&#34;actor_rollout&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>actor_rollout_cls</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>NotImplementedError</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create critic</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_critic</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>resource_pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>get_resource_pool</span><span class=p>(</span><span class=n>Role</span><span class=o>.</span><span class=n>Critic</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>critic_cls</span> <span class=o>=</span> <span class=n>RayClassWithInitArgs</span><span class=p>(</span><span class=bp>cls</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>Critic</span><span class=p>],</span> <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>critic</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=p>[</span><span class=n>resource_pool</span><span class=p>][</span><span class=s2>&#34;critic&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>critic_cls</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create reference policy if needed</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_reference_policy</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>resource_pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>get_resource_pool</span><span class=p>(</span><span class=n>Role</span><span class=o>.</span><span class=n>RefPolicy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>ref_policy_cls</span> <span class=o>=</span> <span class=n>RayClassWithInitArgs</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>RefPolicy</span><span class=p>],</span> <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=p>,</span> <span class=n>role</span><span class=o>=</span><span class=s2>&#34;ref&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=p>[</span><span class=n>resource_pool</span><span class=p>][</span><span class=s2>&#34;ref&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>ref_policy_cls</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create a reward model if reward_fn is None</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_rm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># we create a RM here</span>
</span></span><span class=line><span class=cl>            <span class=n>resource_pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_manager</span><span class=o>.</span><span class=n>get_resource_pool</span><span class=p>(</span><span class=n>Role</span><span class=o>.</span><span class=n>RewardModel</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>rm_cls</span> <span class=o>=</span> <span class=n>RayClassWithInitArgs</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>role_worker_mapping</span><span class=p>[</span><span class=n>Role</span><span class=o>.</span><span class=n>RewardModel</span><span class=p>],</span> <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>reward_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=p>[</span><span class=n>resource_pool</span><span class=p>][</span><span class=s2>&#34;rm&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>rm_cls</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># initialize WorkerGroup</span>
</span></span><span class=line><span class=cl>        <span class=c1># NOTE: if you want to use a different resource pool for each role, which can support different parallel size,</span>
</span></span><span class=line><span class=cl>        <span class=c1># you should not use `create_colocated_worker_cls`.</span>
</span></span><span class=line><span class=cl>        <span class=c1># Instead, directly pass different resource pool to different worker groups.</span>
</span></span><span class=line><span class=cl>        <span class=c1># See https://github.com/volcengine/verl/blob/master/examples/ray/tutorial.ipynb for more information.</span>
</span></span><span class=line><span class=cl>        <span class=n>all_wg</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>        <span class=n>wg_kwargs</span> <span class=o>=</span> <span class=p>{}</span>  <span class=c1># Setting up kwargs for RayWorkerGroup</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>OmegaConf</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=p>,</span> <span class=s2>&#34;ray_wait_register_center_timeout&#34;</span><span class=p>)</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>wg_kwargs</span><span class=p>[</span><span class=s2>&#34;ray_wait_register_center_timeout&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=o>.</span><span class=n>ray_wait_register_center_timeout</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>OmegaConf</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=p>,</span> <span class=s2>&#34;profile_steps&#34;</span><span class=p>)</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>wg_kwargs</span><span class=p>[</span><span class=s2>&#34;profile_steps&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>OmegaConf</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=p>,</span> <span class=s2>&#34;profile_steps&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>assert</span> <span class=n>OmegaConf</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=p>,</span> <span class=s2>&#34;worker_nsight_options&#34;</span><span class=p>)</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>,</span> <span class=s2>&#34;worker_nsight_options must be set when profile_steps is set&#34;</span>
</span></span><span class=line><span class=cl>            <span class=n>wg_kwargs</span><span class=p>[</span><span class=s2>&#34;worker_nsight_options&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>OmegaConf</span><span class=o>.</span><span class=n>to_container</span><span class=p>(</span><span class=n>OmegaConf</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>trainer</span><span class=p>,</span> <span class=s2>&#34;worker_nsight_options&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>resource_pool</span><span class=p>,</span> <span class=n>class_dict</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>resource_pool_to_cls</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>worker_dict_cls</span> <span class=o>=</span> <span class=n>create_colocated_worker_cls</span><span class=p>(</span><span class=n>class_dict</span><span class=o>=</span><span class=n>class_dict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>wg_dict</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ray_worker_group_cls</span><span class=p>(</span><span class=n>resource_pool</span><span class=o>=</span><span class=n>resource_pool</span><span class=p>,</span> <span class=n>ray_cls_with_init</span><span class=o>=</span><span class=n>worker_dict_cls</span><span class=p>,</span> <span class=n>device_name</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_name</span><span class=p>,</span> <span class=o>**</span><span class=n>wg_kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>spawn_wg</span> <span class=o>=</span> <span class=n>wg_dict</span><span class=o>.</span><span class=n>spawn</span><span class=p>(</span><span class=n>prefix_set</span><span class=o>=</span><span class=n>class_dict</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
</span></span><span class=line><span class=cl>            <span class=n>all_wg</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>spawn_wg</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_critic</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>critic_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;critic&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>critic_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_reference_policy</span> <span class=ow>and</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>ref_in_actor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ref_policy_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;ref&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ref_policy_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_rm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>rm_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;rm&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>rm_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># we should create rollout at the end so that vllm can have a better estimation of kv cache memory</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>actor_rollout_wg</span> <span class=o>=</span> <span class=n>all_wg</span><span class=p>[</span><span class=s2>&#34;actor_rollout&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>actor_rollout_wg</span><span class=o>.</span><span class=n>init_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create async rollout manager and request scheduler</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>async_rollout_mode</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>actor_rollout_ref</span><span class=o>.</span><span class=n>rollout</span><span class=o>.</span><span class=n>mode</span> <span class=o>==</span> <span class=s2>&#34;async&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=kn>from</span> <span class=nn>verl.workers.rollout.async_server</span> <span class=kn>import</span> <span class=n>AsyncLLMServerManager</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>async_rollout_mode</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>async_rollout_manager</span> <span class=o>=</span> <span class=n>AsyncLLMServerManager</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>worker_group</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>actor_rollout_wg</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><details></div><footer class=post-footer><ul class=post-tags><li><a href=https://pillumina.github.io/tags/framework/>Framework</a></li><li><a href=https://pillumina.github.io/tags/verl/>Verl</a></li><li><a href=https://pillumina.github.io/tags/sglang/>Sglang</a></li></ul><nav class=paginav><a class=prev href=https://pillumina.github.io/posts/aiinfra/08-verl-multiturn-2/><span class=title>Â« Prev</span><br><span>[VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ2ï¼‰</span>
</a><a class=next href=https://pillumina.github.io/posts/aiinfra/04-aiinfra-thinking/><span class=title>Next Â»</span><br><span>AI Infraï¼šé¢ è¦†æ€§åˆ›æ–°ï¼Œè¿˜æ˜¯ç»å…¸å·¥ç¨‹èŒƒå¼çš„åä¸½è½¬èº«ï¼Ÿ</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰ on x" href="https://x.com/intent/tweet/?text=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f&amp;hashtags=framework%2cverl%2csglang"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰ on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f&amp;title=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89&amp;summary=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89&amp;source=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰ on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f&title=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰ on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰ on whatsapp" href="https://api.whatsapp.com/send?text=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89%20-%20https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰ on telegram" href="https://telegram.me/share/url?text=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ1ï¼‰ on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5bVeRL%5d%20Multi-Turn%20RL%e8%ae%ad%e7%bb%83%e6%ba%90%e7%a0%81%e8%b5%b0%e8%af%bb%ef%bc%881%ef%bc%89&u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f07-verl-multiturn-1%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul><div class=related-posts><div class=related-series><h3>åŒç³»åˆ—æ–‡ç« </h3><ul><li><a href=/posts/aiinfra/12-verl-sglang-memory/>[VeRL,SGLang] RLè®­æ¨æ˜¾å­˜ç®¡ç†ä¼˜åŒ–</a>
<span class=meta>2025-09-17
Â· 2 min read</span></li><li><a href=/posts/aiinfra/10-verl-dataproto/>[VeRL] DataProtoä»‹ç»</a>
<span class=meta>2025-08-25
Â· 17 min read</span></li><li><a href=/posts/aiinfra/09-verl-agentloop/>[VeRL] AgentLoopæºç èµ°è¯»</a>
<span class=meta>2025-08-14
Â· 15 min read</span></li><li><a href=/posts/aiinfra/05-verl-params/>[VeRL] å‚æ•°é€Ÿè§ˆ</a>
<span class=meta>2025-08-14
Â· 6 min read</span></li><li><a href=/posts/aiinfra/08-verl-multiturn-2/>[VeRL] Multi-Turn RLè®­ç»ƒæºç èµ°è¯»ï¼ˆ2ï¼‰</a>
<span class=meta>2025-08-03
Â· 27 min read</span></li></ul></div><div class=related-tags><h3>ç›¸å…³æ–‡ç« </h3><ul><li><a href=/posts/aiinfra/12-verl-sglang-memory/>[VeRL,SGLang] RLè®­æ¨æ˜¾å­˜ç®¡ç†ä¼˜åŒ–</a>
<span class=meta>2025-09-17
Â· 2 min read
Â· Tags: sglang, verl</span></li><li><a href=/posts/aiinfra/10-verl-dataproto/>[VeRL] DataProtoä»‹ç»</a>
<span class=meta>2025-08-25
Â· 17 min read
Â· Tags: framework, verl</span></li><li><a href=/posts/aiinfra/09-verl-agentloop/>[VeRL] AgentLoopæºç èµ°è¯»</a>
<span class=meta>2025-08-14
Â· 15 min read
Â· Tags: framework, verl, sglang</span></li><li><a href=/posts/aiinfra/05-verl-params/>[VeRL] å‚æ•°é€Ÿè§ˆ</a>
<span class=meta>2025-08-14
Â· 6 min read
Â· Tags: framework, verl</span></li><li><a href=/posts/aiinfra/06-sglang-backend/>[SGLang] åç«¯ä»£ç é€Ÿè§ˆ</a>
<span class=meta>2025-08-13
Â· 5 min read
Â· Tags: inference, sglang</span></li></ul></div></div></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://pillumina.github.io/>CctoctoFX</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div class=reading-progress-bar></div><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelector(".reading-progress-bar");if(!t)return;const n=document.querySelector(".post-single");if(!n)return;function s(){const e=n.getBoundingClientRect(),s=e.height,o=window.innerHeight,i=window.scrollY||window.pageYOffset,a=i/(s-o)*100;t.style.width=`${Math.min(100,Math.max(0,a))}%`}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){s(),e=!1}),e=!0)}),s()}),document.addEventListener("DOMContentLoaded",function(){mediumZoom("article img:not(.nozoom)",{margin:24,background:"var(--theme)",scrollOffset:0})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>