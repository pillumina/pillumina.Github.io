<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[VeRL] 参数速览 | CctoctoFX</title><meta name=keywords content="framework,LLM,RL"><meta name=description content="
VeRL框架的参数众多，基于当前（2025.8.5）主线分支整理，附带了相关的理解，一些描述不一定完全正确，供学习参考。
Batch Size

  
      
          参数名称
          详细解释
      
  
  
      
          data.train_batch_size
          作用：定义了单次训练发送给 Rollout Engine 的样本数量，也即这是在每个 PPO 迭代开始时，从训练数据集中采样的提示 （Prompt）数量。详细解释：这个值是 RL 训练中的基本样本数量。例如，设置为 1024 意味着在一次迭代中会：1. 从数据集中随机抽取 1024 个 prompt。 2. 将这 1024 个 prompt 发送给当前的 Rollout Engine 中，从而得到 1024 组完整的 trajectories（prompt, response）。3. 接下来，这 1024 个 trajectories 进行经验计算（make experience），后续用于 Actor 和 Critic 模型的更新。影响与权衡：影响总共训练的样本量。
      
      
          data.val_batch_size （Deprecated)
          作用：在 Validation 阶段使用的批次大小。详细解释：这与 train_batch_size 类似，但仅用于评估模型性能，不参与训练。如果设置为 null，会使用验证集的大小作为默认值。Note: 已经deprecated，推荐设置为 null。此时，整个 validation dataset 一次性发给 SGLang engines，自行进行内存管理。
      
      
          actor_rollout_ref.actor.ppo_mini_batch_size  critic.ppo_mini_batch_size
          作用：定义了 PPO 训练更新中的 mini-batch 大小。详细解释：data.train_batch_size 收集到的全部经验数据将被分割成多个 mini-batch，每块的大小就是 ppo_mini_batch_size。模型每处理完一个 mini-batch，才会进行一次参数更新。例如，如果 train_batch_size = 1024，ppo_mini_batch_size = 256，那么在一个 PPO Epoch 中，模型会进行 1024 / 256 = 4 次参数更新。影响与权衡：增大 mini-batch，单次更新的梯度更稳定，但更新频率更低，更新次数减少。
      
      
          actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu  critic.ppo_micro_batch_size_per_gpu
          作用：定义了在单个 GPU 上进行一次 forward/backward 的数据大小。详细解释：这是实现梯度累积的核心参数。mini-batch 会被再次切分为若干个 micro-batch。例如，在单卡上，ppo_mini_batch_size = 256，ppo_micro_batch_size_per_gpu = 32，那么梯度累积的步数就是 256 / 32 = 8。这意味着模型会运行 8 次 forward 得到 loss，然后 backward 的到 gradient。每次处理 32 个样本，直到累积完整个 mini-batch 计算出的梯度。此时，使用累积的总梯度，对模型参数进行一次更新（optimizer.step()）。这个值必须根据显存大小来严格调整，是防止 OOM 的关键。影响与权衡：增大此值，减少了梯度累积的次数，可以提高训练的吞吐量，增大显存消耗。
      
      
          actor_rollout_ref.actor.ppo_micro_batch_size  critic.ppo_micro_batch_size（Deprecated)
          作用：已弃用，被 per_gpu 版本取代，因为它能更好地适应分布式训练环境。
      
  

Dynamic Batch Size
当样本长度差异很大时，按样本数量划分批次可能导致不同批次的计算量极不均衡，而基于 token 总数来控制 batch size 是一种平衡每个 batch 训练时间的方案。"><meta name=author content="Me"><link rel=canonical href=https://pillumina.github.io/posts/aiinfra/05-verl-params/><link crossorigin=anonymous href=/assets/css/stylesheet.9d388901283682bb45dd422fcaa0d0a2054a3c8ff47c9cc6b2baab15508b1b90.css integrity="sha256-nTiJASg2grtF3UIvyqDQogVKPI/0fJzGsrqrFVCLG5A=" rel="preload stylesheet" as=style><link rel=icon href=https://pillumina.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://pillumina.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://pillumina.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://pillumina.github.io/apple-touch-icon.png><link rel=mask-icon href=https://pillumina.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://pillumina.github.io/posts/aiinfra/05-verl-params/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>(function(){function t(){return document.querySelector(".post-content")||document.querySelector(".post-single")||document.body}function n(e){return/\$\$[\s\S]+?\$\$|\\\(|\\\)|\\\[|\\\]/.test(e)}function s(e){if(window.__mathjaxLoaded)return;window.__mathjaxLoaded=!0,window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code","tt"],ignoreHtmlClass:"no-math"}};var t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js",t.defer=!0,t.onload=function(){window.MathJax&&window.MathJax.typesetPromise&&window.MathJax.typesetPromise([e]).catch(function(e){console.warn("MathJax typeset error",e)})},document.head.appendChild(t)}function e(){try{if(typeof renderMathInElement=="function"){const e=t();renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,strict:!1,trust:!0,ignoredTags:["script","noscript","style","textarea","pre","code","tt"],ignoredClasses:["no-math"],macros:{"\\boldsymbol":"\\mathbf{#1}","\\bm":"\\mathbf{#1}"}}),setTimeout(function(){n(e.innerHTML)&&s(e)},200)}}catch(e){console.warn("KaTeX render error:",e)}}document.addEventListener("DOMContentLoaded",function(){e(),setTimeout(e,200)}),window.addEventListener("load",function(){setTimeout(e,0)})})()</script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>const config={startOnLoad:!0,theme:"neutral",themeVariables:{lineColor:"#0f0f0f"},flowchart:{useMaxWidth:!1,htmlLabels:!0}};mermaid.initialize(config),window.onload=()=>{window.mermaid.init(0[0],document.querySelectorAll(".language-mermaid"))}</script><link rel=stylesheet href=/css/custom.min.bda7229c4269a242639e058fb11a4782f02f8d77071ba16609befee67cc41c49.css integrity="sha256-vacinEJpokJjngWPsRpHgvAvjXcHG6FmCb7+5nzEHEk="><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]"),n=document.querySelectorAll(".toc a");if(t.length===0||n.length===0)return;const s={};t.forEach(e=>{s[e.id]=e.offsetTop});function i(){const t=window.scrollY+100;let e="";for(const[n,o]of Object.entries(s))if(t>=o)e=n;else break;return e}function o(){const e=i();if(n.forEach(e=>{e.classList.remove("active")}),e){const t=document.querySelector(`.toc a[href="#${e}"]`);t&&t.classList.add("active")}}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){o(),e=!1}),e=!0)}),o()})</script><meta property="og:url" content="https://pillumina.github.io/posts/aiinfra/05-verl-params/"><meta property="og:site_name" content="CctoctoFX"><meta property="og:title" content="[VeRL] 参数速览"><meta property="og:description" content=" VeRL框架的参数众多，基于当前（2025.8.5）主线分支整理，附带了相关的理解，一些描述不一定完全正确，供学习参考。
Batch Size 参数名称 详细解释 data.train_batch_size 作用：定义了单次训练发送给 Rollout Engine 的样本数量，也即这是在每个 PPO 迭代开始时，从训练数据集中采样的提示 （Prompt）数量。
详细解释：这个值是 RL 训练中的基本样本数量。例如，设置为 1024 意味着在一次迭代中会：
1. 从数据集中随机抽取 1024 个 prompt。
2. 将这 1024 个 prompt 发送给当前的 Rollout Engine 中，从而得到 1024 组完整的 trajectories（prompt, response）。
3. 接下来，这 1024 个 trajectories 进行经验计算（make experience），后续用于 Actor 和 Critic 模型的更新。
影响与权衡：影响总共训练的样本量。 data.val_batch_size （Deprecated) 作用：在 Validation 阶段使用的批次大小。
详细解释：这与 train_batch_size 类似，但仅用于评估模型性能，不参与训练。如果设置为 null，会使用验证集的大小作为默认值。Note: 已经deprecated，推荐设置为 null。此时，整个 validation dataset 一次性发给 SGLang engines，自行进行内存管理。 actor_rollout_ref.actor.ppo_mini_batch_size critic.ppo_mini_batch_size 作用：定义了 PPO 训练更新中的 mini-batch 大小。
详细解释：data.train_batch_size 收集到的全部经验数据将被分割成多个 mini-batch，每块的大小就是 ppo_mini_batch_size。模型每处理完一个 mini-batch，才会进行一次参数更新。
例如，如果 train_batch_size = 1024，ppo_mini_batch_size = 256，那么在一个 PPO Epoch 中，模型会进行 1024 / 256 = 4 次参数更新。
影响与权衡：增大 mini-batch，单次更新的梯度更稳定，但更新频率更低，更新次数减少。 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu critic.ppo_micro_batch_size_per_gpu 作用：定义了在单个 GPU 上进行一次 forward/backward 的数据大小。
详细解释：这是实现梯度累积的核心参数。mini-batch 会被再次切分为若干个 micro-batch。例如，在单卡上，ppo_mini_batch_size = 256，ppo_micro_batch_size_per_gpu = 32，那么梯度累积的步数就是 256 / 32 = 8。这意味着模型会运行 8 次 forward 得到 loss，然后 backward 的到 gradient。每次处理 32 个样本，直到累积完整个 mini-batch 计算出的梯度。此时，使用累积的总梯度，对模型参数进行一次更新（optimizer.step()）。这个值必须根据显存大小来严格调整，是防止 OOM 的关键。
影响与权衡：增大此值，减少了梯度累积的次数，可以提高训练的吞吐量，增大显存消耗。 actor_rollout_ref.actor.ppo_micro_batch_size critic.ppo_micro_batch_size（Deprecated) 作用：已弃用，被 per_gpu 版本取代，因为它能更好地适应分布式训练环境。 Dynamic Batch Size 当样本长度差异很大时，按样本数量划分批次可能导致不同批次的计算量极不均衡，而基于 token 总数来控制 batch size 是一种平衡每个 batch 训练时间的方案。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-14T10:20:12+08:00"><meta property="article:modified_time" content="2025-08-14T10:20:12+08:00"><meta property="article:tag" content="Framework"><meta property="article:tag" content="LLM"><meta property="article:tag" content="RL"><meta property="og:image" content="https://pillumina.github.io/imgs/icon_head.png"><meta property="og:see_also" content="https://pillumina.github.io/posts/aiinfra/02-slime/"><meta property="og:see_also" content="https://pillumina.github.io/posts/aiinfra/03-areal/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://pillumina.github.io/imgs/icon_head.png"><meta name=twitter:title content="[VeRL] 参数速览"><meta name=twitter:description content="
VeRL框架的参数众多，基于当前（2025.8.5）主线分支整理，附带了相关的理解，一些描述不一定完全正确，供学习参考。
Batch Size

  
      
          参数名称
          详细解释
      
  
  
      
          data.train_batch_size
          作用：定义了单次训练发送给 Rollout Engine 的样本数量，也即这是在每个 PPO 迭代开始时，从训练数据集中采样的提示 （Prompt）数量。详细解释：这个值是 RL 训练中的基本样本数量。例如，设置为 1024 意味着在一次迭代中会：1. 从数据集中随机抽取 1024 个 prompt。 2. 将这 1024 个 prompt 发送给当前的 Rollout Engine 中，从而得到 1024 组完整的 trajectories（prompt, response）。3. 接下来，这 1024 个 trajectories 进行经验计算（make experience），后续用于 Actor 和 Critic 模型的更新。影响与权衡：影响总共训练的样本量。
      
      
          data.val_batch_size （Deprecated)
          作用：在 Validation 阶段使用的批次大小。详细解释：这与 train_batch_size 类似，但仅用于评估模型性能，不参与训练。如果设置为 null，会使用验证集的大小作为默认值。Note: 已经deprecated，推荐设置为 null。此时，整个 validation dataset 一次性发给 SGLang engines，自行进行内存管理。
      
      
          actor_rollout_ref.actor.ppo_mini_batch_size  critic.ppo_mini_batch_size
          作用：定义了 PPO 训练更新中的 mini-batch 大小。详细解释：data.train_batch_size 收集到的全部经验数据将被分割成多个 mini-batch，每块的大小就是 ppo_mini_batch_size。模型每处理完一个 mini-batch，才会进行一次参数更新。例如，如果 train_batch_size = 1024，ppo_mini_batch_size = 256，那么在一个 PPO Epoch 中，模型会进行 1024 / 256 = 4 次参数更新。影响与权衡：增大 mini-batch，单次更新的梯度更稳定，但更新频率更低，更新次数减少。
      
      
          actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu  critic.ppo_micro_batch_size_per_gpu
          作用：定义了在单个 GPU 上进行一次 forward/backward 的数据大小。详细解释：这是实现梯度累积的核心参数。mini-batch 会被再次切分为若干个 micro-batch。例如，在单卡上，ppo_mini_batch_size = 256，ppo_micro_batch_size_per_gpu = 32，那么梯度累积的步数就是 256 / 32 = 8。这意味着模型会运行 8 次 forward 得到 loss，然后 backward 的到 gradient。每次处理 32 个样本，直到累积完整个 mini-batch 计算出的梯度。此时，使用累积的总梯度，对模型参数进行一次更新（optimizer.step()）。这个值必须根据显存大小来严格调整，是防止 OOM 的关键。影响与权衡：增大此值，减少了梯度累积的次数，可以提高训练的吞吐量，增大显存消耗。
      
      
          actor_rollout_ref.actor.ppo_micro_batch_size  critic.ppo_micro_batch_size（Deprecated)
          作用：已弃用，被 per_gpu 版本取代，因为它能更好地适应分布式训练环境。
      
  

Dynamic Batch Size
当样本长度差异很大时，按样本数量划分批次可能导致不同批次的计算量极不均衡，而基于 token 总数来控制 batch size 是一种平衡每个 batch 训练时间的方案。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://pillumina.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI Infra","item":"https://pillumina.github.io/posts/aiinfra/"},{"@type":"ListItem","position":3,"name":"[VeRL] 参数速览","item":"https://pillumina.github.io/posts/aiinfra/05-verl-params/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[VeRL] 参数速览","name":"[VeRL] 参数速览","description":" VeRL框架的参数众多，基于当前（2025.8.5）主线分支整理，附带了相关的理解，一些描述不一定完全正确，供学习参考。\nBatch Size 参数名称 详细解释 data.train_batch_size 作用：定义了单次训练发送给 Rollout Engine 的样本数量，也即这是在每个 PPO 迭代开始时，从训练数据集中采样的提示 （Prompt）数量。\n详细解释：这个值是 RL 训练中的基本样本数量。例如，设置为 1024 意味着在一次迭代中会：\n1. 从数据集中随机抽取 1024 个 prompt。\n2. 将这 1024 个 prompt 发送给当前的 Rollout Engine 中，从而得到 1024 组完整的 trajectories（prompt, response）。\n3. 接下来，这 1024 个 trajectories 进行经验计算（make experience），后续用于 Actor 和 Critic 模型的更新。\n影响与权衡：影响总共训练的样本量。 data.val_batch_size （Deprecated) 作用：在 Validation 阶段使用的批次大小。\n详细解释：这与 train_batch_size 类似，但仅用于评估模型性能，不参与训练。如果设置为 null，会使用验证集的大小作为默认值。Note: 已经deprecated，推荐设置为 null。此时，整个 validation dataset 一次性发给 SGLang engines，自行进行内存管理。 actor_rollout_ref.actor.ppo_mini_batch_size critic.ppo_mini_batch_size 作用：定义了 PPO 训练更新中的 mini-batch 大小。\n详细解释：data.train_batch_size 收集到的全部经验数据将被分割成多个 mini-batch，每块的大小就是 ppo_mini_batch_size。模型每处理完一个 mini-batch，才会进行一次参数更新。\n例如，如果 train_batch_size = 1024，ppo_mini_batch_size = 256，那么在一个 PPO Epoch 中，模型会进行 1024 / 256 = 4 次参数更新。\n影响与权衡：增大 mini-batch，单次更新的梯度更稳定，但更新频率更低，更新次数减少。 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu critic.ppo_micro_batch_size_per_gpu 作用：定义了在单个 GPU 上进行一次 forward/backward 的数据大小。\n详细解释：这是实现梯度累积的核心参数。mini-batch 会被再次切分为若干个 micro-batch。例如，在单卡上，ppo_mini_batch_size = 256，ppo_micro_batch_size_per_gpu = 32，那么梯度累积的步数就是 256 / 32 = 8。这意味着模型会运行 8 次 forward 得到 loss，然后 backward 的到 gradient。每次处理 32 个样本，直到累积完整个 mini-batch 计算出的梯度。此时，使用累积的总梯度，对模型参数进行一次更新（optimizer.step()）。这个值必须根据显存大小来严格调整，是防止 OOM 的关键。\n影响与权衡：增大此值，减少了梯度累积的次数，可以提高训练的吞吐量，增大显存消耗。 actor_rollout_ref.actor.ppo_micro_batch_size critic.ppo_micro_batch_size（Deprecated) 作用：已弃用，被 per_gpu 版本取代，因为它能更好地适应分布式训练环境。 Dynamic Batch Size 当样本长度差异很大时，按样本数量划分批次可能导致不同批次的计算量极不均衡，而基于 token 总数来控制 batch size 是一种平衡每个 batch 训练时间的方案。\n","keywords":["framework","LLM","RL"],"articleBody":" VeRL框架的参数众多，基于当前（2025.8.5）主线分支整理，附带了相关的理解，一些描述不一定完全正确，供学习参考。\nBatch Size 参数名称 详细解释 data.train_batch_size 作用：定义了单次训练发送给 Rollout Engine 的样本数量，也即这是在每个 PPO 迭代开始时，从训练数据集中采样的提示 （Prompt）数量。\n详细解释：这个值是 RL 训练中的基本样本数量。例如，设置为 1024 意味着在一次迭代中会：\n1. 从数据集中随机抽取 1024 个 prompt。\n2. 将这 1024 个 prompt 发送给当前的 Rollout Engine 中，从而得到 1024 组完整的 trajectories（prompt, response）。\n3. 接下来，这 1024 个 trajectories 进行经验计算（make experience），后续用于 Actor 和 Critic 模型的更新。\n影响与权衡：影响总共训练的样本量。 data.val_batch_size （Deprecated) 作用：在 Validation 阶段使用的批次大小。\n详细解释：这与 train_batch_size 类似，但仅用于评估模型性能，不参与训练。如果设置为 null，会使用验证集的大小作为默认值。Note: 已经deprecated，推荐设置为 null。此时，整个 validation dataset 一次性发给 SGLang engines，自行进行内存管理。 actor_rollout_ref.actor.ppo_mini_batch_size critic.ppo_mini_batch_size 作用：定义了 PPO 训练更新中的 mini-batch 大小。\n详细解释：data.train_batch_size 收集到的全部经验数据将被分割成多个 mini-batch，每块的大小就是 ppo_mini_batch_size。模型每处理完一个 mini-batch，才会进行一次参数更新。\n例如，如果 train_batch_size = 1024，ppo_mini_batch_size = 256，那么在一个 PPO Epoch 中，模型会进行 1024 / 256 = 4 次参数更新。\n影响与权衡：增大 mini-batch，单次更新的梯度更稳定，但更新频率更低，更新次数减少。 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu critic.ppo_micro_batch_size_per_gpu 作用：定义了在单个 GPU 上进行一次 forward/backward 的数据大小。\n详细解释：这是实现梯度累积的核心参数。mini-batch 会被再次切分为若干个 micro-batch。例如，在单卡上，ppo_mini_batch_size = 256，ppo_micro_batch_size_per_gpu = 32，那么梯度累积的步数就是 256 / 32 = 8。这意味着模型会运行 8 次 forward 得到 loss，然后 backward 的到 gradient。每次处理 32 个样本，直到累积完整个 mini-batch 计算出的梯度。此时，使用累积的总梯度，对模型参数进行一次更新（optimizer.step()）。这个值必须根据显存大小来严格调整，是防止 OOM 的关键。\n影响与权衡：增大此值，减少了梯度累积的次数，可以提高训练的吞吐量，增大显存消耗。 actor_rollout_ref.actor.ppo_micro_batch_size critic.ppo_micro_batch_size（Deprecated) 作用：已弃用，被 per_gpu 版本取代，因为它能更好地适应分布式训练环境。 Dynamic Batch Size 当样本长度差异很大时，按样本数量划分批次可能导致不同批次的计算量极不均衡，而基于 token 总数来控制 batch size 是一种平衡每个 batch 训练时间的方案。\n参数名称 详细解释 actor_rollout_ref.actor.ppo_max_token_len_per_gpu critic.ppo_max_token_len_per_gpu 作用：定义了一个 PPO micro batch size 中，单个 GPU 能处理的最大 Token 总数。\n详细解释：这是 ppo_micro_batch_size_per_gpu 的替代方案，与 use_dynamic_bsz 配合使用。系统会自动打包样本，直到总 Token 量（prompt_len + response_len）接近这个阈值，形成一个动态的 micro batch size，从而稳定计算效率；无论长短样本，每个微批次的计算量都相对恒定。\n例如，设置为 actor_rollout_ref.actor.ppo_max_token_len_per_gpu = 16384，系统可能会打包 16 个长度为 1024 的样本（16 * 1024 = 16384）或者 64个长度为 256 的样本（64 * 256 = 16384）。\n影响与权衡：通常比固定样本数的微批次效率更高，能更好地利用计算资源，减少 GPU 不稳定性。通常设置为 n * ({data.max_prompt_length} + {data.max_response_length}) reward_model.forward_max_token_len_per_gpu critic.forward_max_token_len_per_gpu actor_rollout_ref.ref.log_prob_max_token_len_per_gpu 作用：只进行 forward 计算的 Model 的一个 micro-batch 的 token 最大数量。\n详细解释：一些模型（Reward Model, Critic 求 value, Reference Model 求 log probs）在 make experience 阶段只有 forward 计算，此时 rollout engine 已经 offload 了，而 training engine 还没启动，显存占用是很少的。因此，可以为它们设置一个更大的 batch size 以加速计算。这些参数同样是 use_dynamic_bsz 的一部分，用于优化这些特定任务的执行效率。 critic.forward_micro_batch_size_per_gpu reward_model.micro_batch_size_per_gpu actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu 作用：同样为只进行 forward 计算的 model 设置 micro-batch size。\n详细解释：同上一行参数。 actor_rollout_ref.actor.use_dynamic_bsz critic.use_dynamic_bsz reward_model.use_dynamic_bsz 作用：是否启用 Dynamic Batch Size。\n详细解释：当此项为 True 时，系统会忽略基于样本数的 micro_batch_size_per_gpu 参数，转而使用基于 Token 数的 max_token_len_per_gpu 参数来构建 batch。 trainer.balance_batch 作用：是否在分布式训练的各个 dp rank 间平衡 batch size。\n详细解释：在 single controller 上将 data 重新排序使得每个 dp rank 获得相似数目的 token。 Rollout Sampling Parameters 参数名称 作用与解释 actor_rollout_ref.rollout.temperature temperature 值越高，概率分布越平滑，生成结果更多样、更随机；值越低，分布越尖锐，生成结果更倾向于高概率词元，更确定、更保守。temperature=0 通常等同于 Greedy Decoding。 actor_rollout_ref.rollout.top_k 在每一步生成时，只考虑概率最高的 K 个 token 进行采样。例如，top_k=50 表示只从概率前 50 的 token 中选择。\n- 禁用时：在 Hugging Face 中设置为 0 或 None，在 SGLang 中设置为 -1（此时从整个词汇表采样）。 actor_rollout_ref.rollout.top_p 从概率最高的 token 开始累加，直到它们的总概率达到 P，然后从这个 nucleus token 集合中进行采样。是一种动态选择采样范围的方法。top_p=1.0 表示不限制。 actor_rollout_ref.rollout.use_fire_sampling 是否使用 Fire Sampling，来自字节的论文。 actor_rollout_ref.rollout.n 为每个 prompt 生成的 response 数量，也即 GRPO 中的 group size。 actor_rollout_ref.rollout.ignore_eos 是否忽略 EOS (End-of-Sentence) 标记。如果为 True，即使模型生成了 EOS 标记，也会继续生成直到达到 max_response_length。 Performance and Resource Management 参数名称 作用与解释 actor_rollout_ref.rollout.prompt_length 最大的 prompt 长度，过长则被截断。 actor_rollout_ref.rollout.response_length 最大的 response 长度，到达最大长度时 SGLang engine 会直接返回。 actor_rollout_ref.rollout.dtype 模型数据类型。例如 bfloat16, float16，需要与训练阶段的模型类型对齐，否则更新模型参数的时候还需要做量化。 actor_rollout_ref.rollout.gpu_memory_utilization SGLang 中模型参数和 KV Cache占显存的比例，如果使用 0.4.8.post1 以上版本 SGLang，则可以设置到 0.85，使用以下版本的 SGLang 则需要设置到 0.5 左右。 actor_rollout_ref.rollout.free_cache_engine Rollout 后是否释放引擎缓存；SGLang 中启用此选项将触发 flush_cache() 操作：清空 kv cache pool，将所有 slots 标记为可用。通过释放 KV Cache 的逻辑占用，但是不释放物理显存。为什么需要 flush kv cache 可以参考此处。 actor_rollout_ref.rollout.load_format 模型权重加载模式。例如 dummy_dtensor（随机初始化权重，用于快速调试）、hf、safetensors（推荐，安全且高效）。 actor_rollout_ref.rollout.tensor_model_parallel_size (TP_SIZE) 张量并行大小，表示用多少个 GPU 来共同运行一个 SGLang engine。例如，TP_SIZE=4 表示将一个大模型的权重切成 4 份，由 4 个 GPU 协同完成推理。 actor_rollout_ref.rollout.max_model_len 模型能处理的最大总长度（prompt + response）；如果未设置，通常由模型配置决定。 actor_rollout_ref.rollout.max_num_seqs 引擎能同时处理的最大请求量，或者说同时推理的最多 prompts 数量。 actor_rollout_ref.rollout.enable_chunked_prefill 是否启用 Chunked Prefill，对于非常长的 Prompt，可以将其分块处理，减少显存峰值，但是降低吞吐量。 actor_rollout_ref.rollout.disable_log_stats 是否禁用推理引擎的统计日志，以减少控制台输出。 SGLang 配置 参数名称 作用与解释 actor_rollout_ref.rollout.engine_kwargs.sglang.attention_backend SGLang 使用的注意力后端。可以选择如 flashinfer, triton, flashmla, null 几种实现，以适应自身显卡。 multi-turn tool calling 这部分参数主要用于需要多轮交互的场景，如工具调用、连续对话等，由 SGLang Engine 支持。\n参数名称 作用与解释 actor_rollout_ref.rollout.multi_turn.enable 是否启用多轮对话模式。 actor_rollout_ref.rollout.multi_turn.max_turns 最多进行 tool calling 的轮次，null 时会默认设置成 max_model_len // 3 来避免无限对话。 actor_rollout_ref.rollout.multi_turn.tool_config_path 工具配置文件路径，定义模型可以调用的外部工具。 actor_rollout_ref.rollout.multi_turn.completion_callback 自定义 callback function，在每轮生成后可以执行自定义逻辑。 actor_rollout_ref.rollout.multi_turn.use_inference_chat_template 是否使用模型在 inference 阶段的 chat template。True 表示遵循 inference 阶段的模板格式。False 表示使用预训练中的模板，可能包含额外思考过程的完整 Token 序列。对于任何模型，一定要保证在 post training 和后续 inference 进行测试的阶段采用一致的模板。 actor_rollout_ref.rollout.multi_turn.enable_tokenization_sanity_check 是否进行 tokenization 安全性检查，检查逐轮 tokenize 的结果与一次 tokenize 整个 chat history 的结果一致。 验证阶段配置 参数名称 作用与解释 actor_rollout_ref.rollout.val_kwargs.* 验证阶段的 sampling parameters，这允许我们在 post training 和 validation 时使用不同的 sampling parameters。例如，验证时通常设置 temperature=0 和 do_sample=False 来进行贪心解码，以获得更稳定的评估结果。 Dataset 参数名称 作用与解释 data.tokenizer Tokenizer 的类或路径。如果为 null，将从模型中自动推断。 data.use_shm 是否使用共享内存（shared memory）来加载数据。 data.train_files 训练集 parquet 文件。可以是列表或单个文件；路径可以是本地路径或 HDFS 路径。 data.val_files 验证集 parquet 文件。可以是列表或单个文件。 data.prompt_key 数据集中 prompt 的字段。默认为 prompt。 data.reward_fn_key 用于选择奖励函数（如果每个样本使用不同奖励函数）的字段。 data.max_prompt_length 最大提示长度。所有提示将向左填充到此长度。 data.return_raw_input_ids 是否返回未添加聊天模板的原始 input_ids;当 reward model 的 chat template 与 policy model 不同时使用。 data.return_raw_chat 是否返回未应用聊天模板的原始 response。 data.return_full_prompt 是否返回带有聊天模板的完整 prompt。 data.shuffle 是否在 DataLoader 中打乱数据。 data.validation_shuffle 是否打乱验证集。 data.filter_overlong_prompts 是否过滤超长的 prompt。 data.filter_overlong_prompts_workers 过滤超长 prompt 的工作进程数。对于大型数据集，使用多进程加速。默认为 1。 data.truncation 如果 input_ids 或 prompt 超过最大长度，则进行截断。 data.image_key 多模态数据集中表示图像的字段。默认为 images。 data.video_key 多模态数据集中表示视频的字段。 data.trust_remote_code 是否信任本地的的 huggingface cache；注意，这个 remote 是相对 huggingface 而言的，所以这个参数考虑的是“是否信任本地”。 data.custom_cls.path 包含自定义数据集类的文件路径。如果未指定，将使用预实现的默认数据集。 data.custom_cls.name 指定文件中的数据集类名。 Actor, Rollout \u0026 Reference Worker 配置 Critic 和 Actor 的参数是非常一致的，不再赘述。\n参数名称 描述 actor_rollout_ref.hybrid_engine 目前只支持 hybird engine，将 actor 和 rollout 模型放在同一资源组上。 actor_rollout_ref.model.path Huggingface 模型路径。可以是本地路径或 HDFS 路径。 actor_rollout_ref.model.use_shm 是否使用共享内存（SHM）来加速模型权重的加载。 actor_rollout_ref.model.external_lib 用于注册 Huggingface 模型/分词器的额外 Python 包。 actor_rollout_ref.model.override_config 用于覆盖模型原始配置，主要用于 dropout。 actor_rollout_ref.model.enable_gradient_checkpointing actor 训练过程是否重算梯度，以时间换空间。 actor_rollout_ref.model.enable_activation_offload actor 训练是否将 activation offload 到 CPU。 actor_rollout_ref.model.use_remove_padding 训练期间是否移除输入中的 padding元。 actor_rollout_ref.model.use_liger 是否使用 Liger kernel 进行线性层融合。 actor_rollout_ref.model.use_fused_kernels 是否使用自定义 fused kernel（如 FlashAttention, fused MLP）。 actor_rollout_ref.model.fused_kernel_options.impl_backend 融合核的实现后端，triton 或 torch。需要和 use_fused_kernels 配合使用 actor_rollout_ref.model.trust_remote_code 是否信任本地的的 huggingface cache；注意，这个 remote 是相对 huggingface 而言的，所以这个参数考虑的是“是否信任本地”。 actor_rollout_ref.actor.strategy 训练 backend fsdp, fsdp2 或 megatron。 actor_rollout_ref.actor.grad_clip Actor 更新的梯度裁剪。 actor_rollout_ref.actor.clip_ratio PPO 裁剪比率。 actor_rollout_ref.actor.clip_ratio_low 非对称裁剪的下界（用于 dual-clip PPO）。 actor_rollout_ref.actor.clip_ratio_high 非对称裁剪的上界（用于 dual-clip PPO）。 actor_rollout_ref.actor.clip_ratio_c Dual-clip PPO 中的常数 C；当优势 \u003c -C 时进行裁剪。 actor_rollout_ref.actor.loss_agg_mode 损失聚合模式：token-mean, seq-mean-token-sum, 或 seq-mean-token-mean。 actor_rollout_ref.actor.entropy_coeff PPO 损失中的熵正则化系数。 actor_rollout_ref.actor.use_kl_loss 是否使用 KL 损失代替 KL 奖励惩罚。对于 GRPO 为 True。 actor_rollout_ref.actor.use_torch_compile 是否使用 torch.compile()。 actor_rollout_ref.actor.kl_loss_coef 启用 use_kl_loss 时的 KL 损失系数，用于 GRPO。 actor_rollout_ref.actor.kl_loss_type KL 散度损失的类型。选项：kl, abs, mse, low_var_kl, full。 actor_rollout_ref.actor.ppo_epochs PPO 轮数。 actor_rollout_ref.actor.shuffle 打乱训练数据。 actor_rollout_ref.actor.ulysses_sequence_parallel_size Ulysses 类的 sequence parallel 大小。 actor_rollout_ref.actor.entropy_from_logits_with_chunking 通过分块计算熵以减少显存峰值。 actor_rollout_ref.actor.entropy_checkpointing 是否将 entropy 通过 checkpoint 存下来。 actor_rollout_ref.actor.checkpoint.save_contents 保存的检查点中包含的内容。 actor_rollout_ref.actor.checkpoint.load_contents 从检查点加载时指定的内容。 actor_rollout_ref.actor.optim.lr 学习率。 actor_rollout_ref.actor.optim.lr_warmup_steps 预热步数；负值则由 lr_warmup_steps_ratio 决定。 actor_rollout_ref.actor.optim.lr_warmup_steps_ratio 预热步数比例（当 lr_warmup_steps 为负时使用）。 actor_rollout_ref.actor.optim.min_lr_ratio 余弦调度器的最小学习率比例。 actor_rollout_ref.actor.optim.num_cycles 学习率调度中的余弦周期数。 actor_rollout_ref.actor.optim.warmup_style 学习率预热风格：constant 或 cosine。 actor_rollout_ref.actor.optim.total_training_steps 总训练步数。 actor_rollout_ref.actor.optim.weight_decay 权重衰减系数，控制训练过程中对权重施加的 L2 正则化的强度。 actor_rollout_ref.actor.fsdp_config.wrap_policy.min_num_params 触发 FSDP 包装一个层的最小参数数量。 actor_rollout_ref.actor.fsdp_config.param_offload 是否将模型参数卸载到 CPU（以速度换内存）。 actor_rollout_ref.actor.fsdp_config.optimizer_offload 是否将优化器状态卸载到 CPU。 actor_rollout_ref.actor.fsdp_config.offload_policy 仅用于 FSDP2：训练期间卸载参数/梯度/优化器。 actor_rollout_ref.actor.fsdp_config.reshard_after_forward 仅用于 FSDP2：前向传播后重新分片以减少内存占用。 actor_rollout_ref.actor.fsdp_config.fsdp_size 每个 FSDP 分片组中的 GPU 数量；-1 表示自动。 actor_rollout_ref.actor.fsdp_config.forward_prefetch 仅用于 FSDP1：在前向计算完成前预取下一次前向传播的 all-gather。 actor_rollout_ref.actor.profiler.discrete True 表示每个任务有自己的数据库，False 表示所有任务共享一个。 actor_rollout_ref.actor.profiler.all_ranks 是否对所有 rank 进行性能分析。 actor_rollout_ref.actor.profiler.ranks 将被分析的 rank。null 或 [0,1,…]。 actor_rollout_ref.ref.strategy Reference 模型的 FSDP 配置，与 actor 相同。 actor_rollout_ref.ref.fsdp_config.param_offload FSDP 中是否卸载参数。 actor_rollout_ref.ref.fsdp_config.reshard_after_forward 仅用于 FSDP2：是否在模型前向传播后重新分片以节省内存。 actor_rollout_ref.ref.fsdp_config.forward_prefetch 仅用于 FSDP1：在前向计算完成前预取下一次前向传播的 all-gather。 actor_rollout_ref.ref.fsdp_config.wrap_policy.min_num_params FSDP 包装模块中的最小参数量。 actor_rollout_ref.ref.profiler.discrete True 表示每个任务有自己的数据库，False 表示所有任务共享一个。 actor_rollout_ref.ref.profiler.all_ranks 是否对所有 rank 进行性能分析。 actor_rollout_ref.ref.profiler.ranks 将被分析的 rank。null 或 [0,1,…]。 Reward Model 参数名称 描述 reward_model.enable 是否启用奖励模型。如果为 False，则仅使用用户定义的奖励函数计算奖励。 reward_model.strategy FSDP 策略：fsdp 或 fsdp2或megatron。 reward_model.model.input_tokenizer 输入分词器。如果奖励模型的聊天模板与策略不一致，则需要此项。 reward_model.model.path RM 的 HDFS 路径或本地路径。仅支持 AutoModelForSequenceClassification。 reward_model.model.use_shm 是否使用共享内存加载模型。 reward_model.model.external_lib 外部模型实现（可选）。 reward_model.model.use_remove_padding 使用移除填充优化（节省计算）。 reward_model.model.use_fused_kernels 是否使用融合的奖励核以加速。 reward_model.model.trust_remote_code 是否允许加载远程代码模型，默认为 False。 reward_model.model.fsdp_config.wrap_policy.min_num_params 触发 FSDP 包装的最小参数数量。 reward_model.model.fsdp_config.param_offload 是否将模型参数卸载到 CPU。 reward_model.model.fsdp_config.reshard_after_forward 仅用于 FSDP2：前向传播后重新分片以减少内存占用。 reward_model.model.fsdp_config.fsdp_size 每个 FSDP 分片组中的 GPU 数量；-1 表示自动。 reward_model.model.fsdp_config.forward_prefetch 仅用于 FSDP1：在前向计算完成前预取下一次前向传播的 all-gather。 reward_model.reward_manager 定义计算基于规则的奖励和处理不同奖励源的机制。 reward_model.launch_reward_fn_async 是否在 log_prob 期间异步启动自定义奖励函数。 reward_model.sandbox_fusion.url 用于远程 reward 函数的 URL。 reward_model.sandbox_fusion.max_concurrent 允许到沙箱的最大并发请求数。 reward_model.profiler.discrete True 表示每个任务有自己的数据库，False 表示所有任务共享一个。 Custom Reward Function 参数名称 描述 custom_reward_function.path 包含自定义奖励函数的文件路径。 custom_reward_function.name 指定文件中的奖励函数名称。默认为 compute_score。 Algorithm 参数名称 描述 algorithm.gamma 未来奖励的折扣因子。 algorithm.lam GAE 估计器中偏差和方差的权衡。 algorithm.adv_estimator 优势估计器类型：gae, grpo, reinforce_plus_plus 等。 algorithm.norm_adv_by_std_in_grpo 是否在 GRPO 中按标准差归一化优势。 algorithm.use_kl_in_reward 是否在奖励中启用 KL 惩罚。 algorithm.kl_penalty 如何估计 KL 散度：kl, abs, mse, low_var_kl, 或 full。 algorithm.kl_ctrl.type KL 控制类型：fixed 或 adaptive。 algorithm.kl_ctrl.kl_coef KL 惩罚的初始系数。 algorithm.kl_ctrl.horizon 自适应控制器的 horizon 值（如果启用）。 algorithm.kl_ctrl.target_kl 目标 KL 散度（用于自适应控制器）。 algorithm.use_pf_ppo 是否启用偏好反馈 PPO。 algorithm.pf_ppo.reweight_method 样本重加权方法：pow, max_min, 或 max_random。 algorithm.pf_ppo.weight_pow pow 方法中用于权重缩放的幂。 Trainer 参数名称 描述 trainer.balance_batch 是否在分布式工作节点间平衡批次大小。 trainer.total_epochs 训练的总轮数。 trainer.total_training_steps 总训练步数（可显式设置或从轮数派生）。 trainer.profile_steps 将被分析的步骤。null 表示不进行分析。 trainer.controller_nsight_options.trace 对于controller进程，选择要追踪的 API（比如cuda，nvtx，cublas，etc）。 trainer.controller_nsight_options.cuda-memory-usage 对于controller进程，是否profile CUDA 内存使用情况。必须是字符串 \"true\" 或 \"false\"。 trainer.controller_nsight_options.cuda-graph-trace 对于controller进程，是否将CUDA graphs 将被作为一个整体进行追踪。 trainer.worker_nsight_options.trace 对于worker进程，选择要追踪的 API。 trainer.worker_nsight_options.cuda-memory-usage 对于worker进程，是否profile CUDA 内存使用情况。必须是字符串 \"true\" 或 \"false\"。 trainer.worker_nsight_options.cuda-graph-trace 对于worker进程，是否CUDA graphs 将被作为一个整体进行追踪。 trainer.worker_nsight_options.capture-range 仅在 torch.cuda.profiler.start 和 stop 范围内进行分析。默认值为cudaProfilerApi，不要更改此配置。 trainer.worker_nsight_options.capture-range-end 指定捕获范围结束时的期望行为。 trainer.worker_nsight_options.kill 向目标应用程序的进程组发送信号。我们让程序自行退出。 trainer.project_name 用于实验跟踪（如 wandb）的项目名称。 trainer.experiment_name 用于在跟踪工具中识别运行的实验名称。 trainer.logger 使用的日志后端：console, wandb 等。 trainer.log_val_generations 验证期间要记录的生成数量。 trainer.rollout_data_dir 用于记录 rollout 数据的目录；如果为 null 则不转储。 trainer.validation_data_dir 用于记录验证数据的目录；如果为 null 则不转储。 trainer.nnodes 训练中使用的节点数。 trainer.n_gpus_per_node 每个节点的 GPU 数量。 trainer.save_freq 模型检查点的保存频率（按迭代次数）。 trainer.resume_mode 恢复模式：auto, disable, 或 resume_path。 trainer.resume_from_path 从该路径恢复训练（仅当 resume_mode 为 resume_path 时使用）。 trainer.val_before_train 是否在训练开始前运行验证。 trainer.val_only 是否只运行验证。 trainer.test_freq 验证频率（以训练迭代次数计）。 trainer.critic_warmup 在更新策略之前预热 critic 的迭代次数。 trainer.default_hdfs_dir 用于保存检查点的默认分布式文件系统路径。 trainer.del_local_ckpt_after_load 加载后是否删除本地检查点。 trainer.default_local_dir 用于保存检查点的默认本地目录。 trainer.max_actor_ckpt_to_keep 保留的 actor 检查点的最大数量。 trainer.max_critic_ckpt_to_keep 保留的 critic 检查点的最大数量。 trainer.ray_wait_register_center_timeout Ray worker 等待注册的超时时间（秒）。 trainer.device 运行训练的设备（如 cuda, cpu）。 Ray Init 参数名称 描述 ray_init.num_cpus Ray 使用的 CPU 数量。使用 SLURM 时应使用固定数字而不是 null。 ray_init.timeline_json_file 保存 Ray 时间线 JSON 文件以进行性能分析的路径。 ","wordCount":"1133","inLanguage":"en","image":"https://pillumina.github.io/imgs/icon_head.png","datePublished":"2025-08-14T10:20:12+08:00","dateModified":"2025-08-14T10:20:12+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://pillumina.github.io/posts/aiinfra/05-verl-params/"},"publisher":{"@type":"Organization","name":"CctoctoFX","logo":{"@type":"ImageObject","url":"https://pillumina.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://pillumina.github.io/ accesskey=h title="CctoctoFX (Alt + H)"><img src=https://pillumina.github.io/apple-touch-icon.png alt aria-label=logo height=30>CctoctoFX</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://pillumina.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://pillumina.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://pillumina.github.io/posts/aiinfra/ title="AI Infra"><span>AI Infra</span></a></li><li><a href=https://pillumina.github.io/posts/llmtheory/ title=Thoery><span>Thoery</span></a></li><li><a href=https://pillumina.github.io/posts/programming/ title=Programming><span>Programming</span></a></li><li><a href=https://pillumina.github.io/social/ title=Social><span>Social</span></a></li><li><a href=https://pillumina.github.io/open_courses/ title=Study><span>Study</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://pillumina.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/aiinfra/>AI Infra</a></div><h1 class="post-title entry-hint-parent">[VeRL] 参数速览</h1><div class=post-meta><span title='2025-08-14 10:20:12 +0800 CST'>August 14, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1133 words&nbsp;·&nbsp;Me</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#batch-size>Batch Size</a></li><li><a href=#dynamic-batch-size>Dynamic Batch Size</a></li><li><a href=#rollout-sampling-parameters>Rollout Sampling Parameters</a></li><li><a href=#performance-and-resource-management>Performance and Resource Management</a><ul><li><a href=#sglang-配置>SGLang 配置</a></li><li><a href=#multi-turn-tool-calling>multi-turn tool calling</a></li><li><a href=#验证阶段配置>验证阶段配置</a></li><li><a href=#dataset>Dataset</a></li><li><a href=#actor-rollout--reference-worker-配置>Actor, Rollout & Reference Worker 配置</a></li><li><a href=#reward-model>Reward Model</a></li><li><a href=#custom-reward-function>Custom Reward Function</a></li><li><a href=#algorithm>Algorithm</a></li><li><a href=#trainer>Trainer</a></li><li><a href=#ray-init>Ray Init</a></li></ul></li></ul></nav></div></details></div><div class=post-content><blockquote><p>VeRL框架的参数众多，基于当前（2025.8.5）主线分支整理，附带了相关的理解，一些描述不一定完全正确，供学习参考。</p></blockquote><h2 id=batch-size>Batch Size<a hidden class=anchor aria-hidden=true href=#batch-size>#</a></h2><table><thead><tr><th>参数名称</th><th>详细解释</th></tr></thead><tbody><tr><td><code>data.train_batch_size</code></td><td><strong>作用</strong>：定义了单次训练发送给 Rollout Engine 的样本数量，也即这是在每个 PPO 迭代开始时，从训练数据集中采样的提示 （Prompt）数量。<br><br><strong>详细解释</strong>：这个值是 RL 训练中的基本样本数量。例如，设置为 1024 意味着在一次迭代中会：<br>1. 从数据集中随机抽取 1024 个 prompt。<br>2. 将这 1024 个 prompt 发送给当前的 Rollout Engine 中，从而得到 1024 组完整的 trajectories（prompt, response）。<br>3. 接下来，这 1024 个 trajectories 进行经验计算（make experience），后续用于 Actor 和 Critic 模型的更新。<br><br><strong>影响与权衡</strong>：影响总共训练的样本量。</td></tr><tr><td><code>data.val_batch_size</code> （Deprecated)</td><td><strong>作用</strong>：在 Validation 阶段使用的批次大小。<br><br><strong>详细解释</strong>：这与 <code>train_batch_size</code> 类似，但仅用于评估模型性能，不参与训练。如果设置为 <code>null</code>，会使用验证集的大小作为默认值。Note: 已经deprecated，推荐设置为 null。此时，整个 validation dataset 一次性发给 SGLang engines，自行进行内存管理。</td></tr><tr><td><code>actor_rollout_ref.actor.ppo_mini_batch_size</code><br><code>critic.ppo_mini_batch_size</code></td><td><strong>作用</strong>：定义了 PPO 训练更新中的 mini-batch 大小。<br><br><strong>详细解释</strong>：<code>data.train_batch_size</code> 收集到的全部经验数据将被分割成多个 mini-batch，每块的大小就是 <code>ppo_mini_batch_size</code>。模型每处理完一个 mini-batch，才会进行一次参数更新。<br>例如，如果 <code>train_batch_size = 1024</code>，<code>ppo_mini_batch_size = 256</code>，那么在一个 PPO Epoch 中，模型会进行 <code>1024 / 256 = 4</code> 次参数更新。<br><br><strong>影响与权衡</strong>：增大 mini-batch，单次更新的梯度更稳定，但更新频率更低，更新次数减少。</td></tr><tr><td><code>actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu</code><br><code>critic.ppo_micro_batch_size_per_gpu</code></td><td><strong>作用</strong>：定义了在单个 GPU 上进行一次 forward/backward 的数据大小。<br><br><strong>详细解释</strong>：这是实现梯度累积的核心参数。mini-batch 会被再次切分为若干个 micro-batch。例如，在单卡上，<code>ppo_mini_batch_size = 256</code>，<code>ppo_micro_batch_size_per_gpu = 32</code>，那么梯度累积的步数就是 <code>256 / 32 = 8</code>。这意味着模型会运行 8 次 forward 得到 loss，然后 backward 的到 gradient。每次处理 32 个样本，直到累积完整个 mini-batch 计算出的梯度。此时，使用累积的总梯度，对模型参数进行一次更新（<code>optimizer.step()</code>）。这个值必须根据显存大小来严格调整，是防止 OOM 的关键。<br><br><strong>影响与权衡</strong>：增大此值，减少了梯度累积的次数，可以提高训练的吞吐量，增大显存消耗。</td></tr><tr><td><code>actor_rollout_ref.actor.ppo_micro_batch_size</code><br><code>critic.ppo_micro_batch_size</code>（Deprecated)</td><td><strong>作用</strong>：已弃用，被 <code>per_gpu</code> 版本取代，因为它能更好地适应分布式训练环境。</td></tr></tbody></table><h2 id=dynamic-batch-size>Dynamic Batch Size<a hidden class=anchor aria-hidden=true href=#dynamic-batch-size>#</a></h2><p>当样本长度差异很大时，按样本数量划分批次可能导致不同批次的计算量极不均衡，而基于 token 总数来控制 batch size 是一种平衡每个 batch 训练时间的方案。</p><table><thead><tr><th>参数名称</th><th>详细解释</th></tr></thead><tbody><tr><td><code>actor_rollout_ref.actor.ppo_max_token_len_per_gpu</code><br><code>critic.ppo_max_token_len_per_gpu</code></td><td><strong>作用</strong>：定义了一个 PPO micro batch size 中，单个 GPU 能处理的最大 Token 总数。<br><br><strong>详细解释</strong>：这是 <code>ppo_micro_batch_size_per_gpu</code> 的替代方案，与 <code>use_dynamic_bsz</code> 配合使用。系统会自动打包样本，直到总 Token 量（<code>prompt_len + response_len</code>）接近这个阈值，形成一个动态的 micro batch size，从而稳定计算效率；无论长短样本，每个微批次的计算量都相对恒定。<br>例如，设置为 <code>actor_rollout_ref.actor.ppo_max_token_len_per_gpu = 16384</code>，系统可能会打包 16 个长度为 1024 的样本（16 * 1024 = 16384）或者 64个长度为 256 的样本（64 * 256 = 16384）。<br><br><strong>影响与权衡</strong>：通常比固定样本数的微批次效率更高，能更好地利用计算资源，减少 GPU 不稳定性。通常设置为 <code>n * ({data.max_prompt_length} + {data.max_response_length})</code></td></tr><tr><td><code>reward_model.forward_max_token_len_per_gpu</code><br><code>critic.forward_max_token_len_per_gpu</code><br><code>actor_rollout_ref.ref.log_prob_max_token_len_per_gpu</code></td><td><strong>作用</strong>：只进行 forward 计算的 Model 的一个 micro-batch 的 token 最大数量。<br><br><strong>详细解释</strong>：一些模型（Reward Model, Critic 求 value, Reference Model 求 log probs）在 make experience 阶段只有 forward 计算，此时 rollout engine 已经 offload 了，而 training engine 还没启动，显存占用是很少的。因此，可以为它们设置一个更大的 batch size 以加速计算。这些参数同样是 <code>use_dynamic_bsz</code> 的一部分，用于优化这些特定任务的执行效率。</td></tr><tr><td><code>critic.forward_micro_batch_size_per_gpu</code><br><code>reward_model.micro_batch_size_per_gpu</code><br><code>actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu</code></td><td><strong>作用</strong>：同样为只进行 forward 计算的 model 设置 micro-batch size。<br><br><strong>详细解释</strong>：同上一行参数。</td></tr><tr><td><code>actor_rollout_ref.actor.use_dynamic_bsz</code><br><code>critic.use_dynamic_bsz</code><br><code>reward_model.use_dynamic_bsz</code></td><td><strong>作用</strong>：是否启用 Dynamic Batch Size。<br><br><strong>详细解释</strong>：当此项为 <code>True</code> 时，系统会忽略基于样本数的 <code>micro_batch_size_per_gpu</code> 参数，转而使用基于 Token 数的 <code>max_token_len_per_gpu</code> 参数来构建 batch。</td></tr><tr><td><code>trainer.balance_batch</code></td><td><strong>作用</strong>：是否在分布式训练的各个 dp rank 间平衡 batch size。<br><br><strong>详细解释</strong>：在 single controller 上将 data 重新排序使得每个 dp rank 获得相似数目的 token。</td></tr></tbody></table><h2 id=rollout-sampling-parameters>Rollout Sampling Parameters<a hidden class=anchor aria-hidden=true href=#rollout-sampling-parameters>#</a></h2><table><thead><tr><th>参数名称</th><th>作用与解释</th></tr></thead><tbody><tr><td><code>actor_rollout_ref.rollout.temperature</code></td><td>temperature 值越高，概率分布越平滑，生成结果更多样、更随机；值越低，分布越尖锐，生成结果更倾向于高概率词元，更确定、更保守。<code>temperature=0</code> 通常等同于 Greedy Decoding。</td></tr><tr><td><code>actor_rollout_ref.rollout.top_k</code></td><td>在每一步生成时，只考虑概率最高的 K 个 token 进行采样。例如，<code>top_k=50</code> 表示只从概率前 50 的 token 中选择。<br>- 禁用时：在 Hugging Face 中设置为 <code>0</code> 或 <code>None</code>，在 SGLang 中设置为 <code>-1</code>（此时从整个词汇表采样）。</td></tr><tr><td><code>actor_rollout_ref.rollout.top_p</code></td><td>从概率最高的 token 开始累加，直到它们的总概率达到 P，然后从这个 nucleus token 集合中进行采样。是一种动态选择采样范围的方法。<code>top_p=1.0</code> 表示不限制。</td></tr><tr><td><code>actor_rollout_ref.rollout.use_fire_sampling</code></td><td>是否使用 Fire Sampling，来自字节的<a href=https://arxiv.org/abs/2410.21236>论文</a>。</td></tr><tr><td><code>actor_rollout_ref.rollout.n</code></td><td>为每个 prompt 生成的 response 数量，也即 GRPO 中的 group size。</td></tr><tr><td><code>actor_rollout_ref.rollout.ignore_eos</code></td><td>是否忽略 EOS (End-of-Sentence) 标记。如果为 <code>True</code>，即使模型生成了 EOS 标记，也会继续生成直到达到 <code>max_response_length</code>。</td></tr></tbody></table><h2 id=performance-and-resource-management>Performance and Resource Management<a hidden class=anchor aria-hidden=true href=#performance-and-resource-management>#</a></h2><table><thead><tr><th>参数名称</th><th>作用与解释</th></tr></thead><tbody><tr><td><code>actor_rollout_ref.rollout.prompt_length</code></td><td>最大的 prompt 长度，过长则被截断。</td></tr><tr><td><code>actor_rollout_ref.rollout.response_length</code></td><td>最大的 response 长度，到达最大长度时 SGLang engine 会直接返回。</td></tr><tr><td><code>actor_rollout_ref.rollout.dtype</code></td><td>模型数据类型。例如 <code>bfloat16</code>, <code>float16</code>，需要与训练阶段的模型类型对齐，否则更新模型参数的时候还需要做量化。</td></tr><tr><td><code>actor_rollout_ref.rollout.gpu_memory_utilization</code></td><td>SGLang 中模型参数和 KV Cache占显存的比例，如果使用 0.4.8.post1 以上版本 SGLang，则可以设置到 0.85，使用以下版本的 SGLang 则需要设置到 0.5 左右。</td></tr><tr><td><code>actor_rollout_ref.rollout.free_cache_engine</code></td><td>Rollout 后是否释放引擎缓存；SGLang 中启用此选项将触发 <code>flush_cache()</code> 操作：清空 kv cache pool，将所有 slots 标记为可用。通过释放 KV Cache 的逻辑占用，但是不释放物理显存。为什么需要 flush kv cache 可以参考<a href=https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/code-walk-through/readme.md#sglangrolloutasyncengine>此处</a>。</td></tr><tr><td><code>actor_rollout_ref.rollout.load_format</code></td><td>模型权重加载模式。例如 <code>dummy_dtensor</code>（随机初始化权重，用于快速调试）、<code>hf</code>、<code>safetensors</code>（推荐，安全且高效）。</td></tr><tr><td><code>actor_rollout_ref.rollout.tensor_model_parallel_size</code> (TP_SIZE)</td><td>张量并行大小，表示用多少个 GPU 来共同运行一个 SGLang engine。例如，<code>TP_SIZE=4</code> 表示将一个大模型的权重切成 4 份，由 4 个 GPU 协同完成推理。</td></tr><tr><td><code>actor_rollout_ref.rollout.max_model_len</code></td><td>模型能处理的最大总长度（prompt + response）；如果未设置，通常由模型配置决定。</td></tr><tr><td><code>actor_rollout_ref.rollout.max_num_seqs</code></td><td>引擎能同时处理的最大请求量，或者说同时推理的最多 prompts 数量。</td></tr><tr><td><code>actor_rollout_ref.rollout.enable_chunked_prefill</code></td><td>是否启用 Chunked Prefill，对于非常长的 Prompt，可以将其分块处理，减少显存峰值，但是降低吞吐量。</td></tr><tr><td><code>actor_rollout_ref.rollout.disable_log_stats</code></td><td>是否禁用推理引擎的统计日志，以减少控制台输出。</td></tr></tbody></table><hr><h3 id=sglang-配置>SGLang 配置<a hidden class=anchor aria-hidden=true href=#sglang-配置>#</a></h3><table><thead><tr><th>参数名称</th><th>作用与解释</th></tr></thead><tbody><tr><td><code>actor_rollout_ref.rollout.engine_kwargs.sglang.attention_backend</code></td><td><strong>SGLang 使用的注意力后端</strong>。可以选择如 <code>flashinfer</code>, <code>triton</code>, <code>flashmla</code>, <code>null</code> 几种实现，以适应自身显卡。</td></tr></tbody></table><hr><h3 id=multi-turn-tool-calling>multi-turn tool calling<a hidden class=anchor aria-hidden=true href=#multi-turn-tool-calling>#</a></h3><p>这部分参数主要用于需要多轮交互的场景，如工具调用、连续对话等，由 SGLang Engine 支持。</p><table><thead><tr><th>参数名称</th><th>作用与解释</th></tr></thead><tbody><tr><td><code>actor_rollout_ref.rollout.multi_turn.enable</code></td><td>是否启用多轮对话模式。</td></tr><tr><td><code>actor_rollout_ref.rollout.multi_turn.max_turns</code></td><td>最多进行 tool calling 的轮次，null 时会默认设置成 <code>max_model_len // 3</code> 来避免无限对话。</td></tr><tr><td><code>actor_rollout_ref.rollout.multi_turn.tool_config_path</code></td><td>工具配置文件路径，定义模型可以调用的外部工具。</td></tr><tr><td><code>actor_rollout_ref.rollout.multi_turn.completion_callback</code></td><td>自定义 callback function，在每轮生成后可以执行自定义逻辑。</td></tr><tr><td><code>actor_rollout_ref.rollout.multi_turn.use_inference_chat_template</code></td><td>是否使用模型在 inference 阶段的 chat template。<code>True</code> 表示遵循 inference 阶段的模板格式。<code>False</code> 表示使用预训练中的模板，可能包含额外思考过程的完整 Token 序列。对于任何模型，一定要保证在 post training 和后续 inference 进行测试的阶段采用一致的模板。</td></tr><tr><td><code>actor_rollout_ref.rollout.multi_turn.enable_tokenization_sanity_check</code></td><td>是否进行 tokenization 安全性检查，检查逐轮 tokenize 的结果与一次 tokenize 整个 chat history 的结果一致。</td></tr></tbody></table><h3 id=验证阶段配置>验证阶段配置<a hidden class=anchor aria-hidden=true href=#验证阶段配置>#</a></h3><table><thead><tr><th>参数名称</th><th>作用与解释</th></tr></thead><tbody><tr><td><code>actor_rollout_ref.rollout.val_kwargs.*</code></td><td>验证阶段的 sampling parameters，这允许我们在 post training 和 validation 时使用不同的 sampling parameters。例如，验证时通常设置 <code>temperature=0</code> 和 <code>do_sample=False</code> 来进行贪心解码，以获得更稳定的评估结果。</td></tr></tbody></table><h3 id=dataset>Dataset<a hidden class=anchor aria-hidden=true href=#dataset>#</a></h3><table><thead><tr><th>参数名称</th><th>作用与解释</th></tr></thead><tbody><tr><td><code>data.tokenizer</code></td><td>Tokenizer 的类或路径。如果为 null，将从模型中自动推断。</td></tr><tr><td><code>data.use_shm</code></td><td>是否使用共享内存（shared memory）来加载数据。</td></tr><tr><td><code>data.train_files</code></td><td>训练集 parquet 文件。可以是列表或单个文件；路径可以是本地路径或 HDFS 路径。</td></tr><tr><td><code>data.val_files</code></td><td>验证集 parquet 文件。可以是列表或单个文件。</td></tr><tr><td><code>data.prompt_key</code></td><td>数据集中 prompt 的字段。默认为 <code>prompt</code>。</td></tr><tr><td><code>data.reward_fn_key</code></td><td>用于选择奖励函数（如果每个样本使用不同奖励函数）的字段。</td></tr><tr><td><code>data.max_prompt_length</code></td><td>最大提示长度。所有提示将向左填充到此长度。</td></tr><tr><td><code>data.return_raw_input_ids</code></td><td>是否返回未添加聊天模板的原始 <code>input_ids</code>;当 reward model 的 chat template 与 policy model 不同时使用。</td></tr><tr><td><code>data.return_raw_chat</code></td><td>是否返回未应用聊天模板的原始 response。</td></tr><tr><td><code>data.return_full_prompt</code></td><td>是否返回带有聊天模板的完整 prompt。</td></tr><tr><td><code>data.shuffle</code></td><td>是否在 DataLoader 中打乱数据。</td></tr><tr><td><code>data.validation_shuffle</code></td><td>是否打乱验证集。</td></tr><tr><td><code>data.filter_overlong_prompts</code></td><td>是否过滤超长的 prompt。</td></tr><tr><td><code>data.filter_overlong_prompts_workers</code></td><td>过滤超长 prompt 的工作进程数。对于大型数据集，使用多进程加速。默认为 1。</td></tr><tr><td><code>data.truncation</code></td><td>如果 <code>input_ids</code> 或 <code>prompt</code> 超过最大长度，则进行截断。</td></tr><tr><td><code>data.image_key</code></td><td>多模态数据集中表示图像的字段。默认为 <code>images</code>。</td></tr><tr><td><code>data.video_key</code></td><td>多模态数据集中表示视频的字段。</td></tr><tr><td><code>data.trust_remote_code</code></td><td>是否信任本地的的 huggingface cache；注意，这个 remote 是相对 huggingface 而言的，所以这个参数考虑的是“是否信任本地”。</td></tr><tr><td><code>data.custom_cls.path</code></td><td>包含自定义数据集类的文件路径。如果未指定，将使用预实现的默认数据集。</td></tr><tr><td><code>data.custom_cls.name</code></td><td>指定文件中的数据集类名。</td></tr></tbody></table><h3 id=actor-rollout--reference-worker-配置>Actor, Rollout & Reference Worker 配置<a hidden class=anchor aria-hidden=true href=#actor-rollout--reference-worker-配置>#</a></h3><p>Critic 和 Actor 的参数是非常一致的，不再赘述。</p><table><thead><tr><th>参数名称</th><th>描述</th></tr></thead><tbody><tr><td><code>actor_rollout_ref.hybrid_engine</code></td><td>目前只支持 hybird engine，将 actor 和 rollout 模型放在同一资源组上。</td></tr><tr><td><code>actor_rollout_ref.model.path</code></td><td>Huggingface 模型路径。可以是本地路径或 HDFS 路径。</td></tr><tr><td><code>actor_rollout_ref.model.use_shm</code></td><td>是否使用共享内存（SHM）来加速模型权重的加载。</td></tr><tr><td><code>actor_rollout_ref.model.external_lib</code></td><td>用于注册 Huggingface 模型/分词器的额外 Python 包。</td></tr><tr><td><code>actor_rollout_ref.model.override_config</code></td><td>用于覆盖模型原始配置，主要用于 dropout。</td></tr><tr><td><code>actor_rollout_ref.model.enable_gradient_checkpointing</code></td><td>actor 训练过程是否重算梯度，以时间换空间。</td></tr><tr><td><code>actor_rollout_ref.model.enable_activation_offload</code></td><td>actor 训练是否将 activation offload 到 CPU。</td></tr><tr><td><code>actor_rollout_ref.model.use_remove_padding</code></td><td>训练期间是否移除输入中的 padding元。</td></tr><tr><td><code>actor_rollout_ref.model.use_liger</code></td><td>是否使用 Liger kernel 进行线性层融合。</td></tr><tr><td><code>actor_rollout_ref.model.use_fused_kernels</code></td><td>是否使用自定义 fused kernel（如 FlashAttention, fused MLP）。</td></tr><tr><td><code>actor_rollout_ref.model.fused_kernel_options.impl_backend</code></td><td>融合核的实现后端，triton 或 torch。需要和 <code>use_fused_kernels</code> 配合使用</td></tr><tr><td><code>actor_rollout_ref.model.trust_remote_code</code></td><td>是否信任本地的的 huggingface cache；注意，这个 remote 是相对 huggingface 而言的，所以这个参数考虑的是“是否信任本地”。</td></tr><tr><td><code>actor_rollout_ref.actor.strategy</code></td><td>训练 backend fsdp, fsdp2 或 megatron。</td></tr><tr><td><code>actor_rollout_ref.actor.grad_clip</code></td><td>Actor 更新的梯度裁剪。</td></tr><tr><td><code>actor_rollout_ref.actor.clip_ratio</code></td><td>PPO 裁剪比率。</td></tr><tr><td><code>actor_rollout_ref.actor.clip_ratio_low</code></td><td>非对称裁剪的下界（用于 dual-clip PPO）。</td></tr><tr><td><code>actor_rollout_ref.actor.clip_ratio_high</code></td><td>非对称裁剪的上界（用于 dual-clip PPO）。</td></tr><tr><td><code>actor_rollout_ref.actor.clip_ratio_c</code></td><td>Dual-clip PPO 中的常数 C；当优势 &lt; -C 时进行裁剪。</td></tr><tr><td><code>actor_rollout_ref.actor.loss_agg_mode</code></td><td>损失聚合模式：<code>token-mean</code>, <code>seq-mean-token-sum</code>, 或 <code>seq-mean-token-mean</code>。</td></tr><tr><td><code>actor_rollout_ref.actor.entropy_coeff</code></td><td>PPO 损失中的熵正则化系数。</td></tr><tr><td><code>actor_rollout_ref.actor.use_kl_loss</code></td><td>是否使用 KL 损失代替 KL 奖励惩罚。对于 GRPO 为 True。</td></tr><tr><td><code>actor_rollout_ref.actor.use_torch_compile</code></td><td>是否使用 <code>torch.compile()</code>。</td></tr><tr><td><code>actor_rollout_ref.actor.kl_loss_coef</code></td><td>启用 <code>use_kl_loss</code> 时的 KL 损失系数，用于 GRPO。</td></tr><tr><td><code>actor_rollout_ref.actor.kl_loss_type</code></td><td>KL 散度损失的类型。选项：<code>kl</code>, <code>abs</code>, <code>mse</code>, <code>low_var_kl</code>, <code>full</code>。</td></tr><tr><td><code>actor_rollout_ref.actor.ppo_epochs</code></td><td>PPO 轮数。</td></tr><tr><td><code>actor_rollout_ref.actor.shuffle</code></td><td>打乱训练数据。</td></tr><tr><td><code>actor_rollout_ref.actor.ulysses_sequence_parallel_size</code></td><td>Ulysses 类的 sequence parallel 大小。</td></tr><tr><td><code>actor_rollout_ref.actor.entropy_from_logits_with_chunking</code></td><td>通过分块计算熵以减少显存峰值。</td></tr><tr><td><code>actor_rollout_ref.actor.entropy_checkpointing</code></td><td>是否将 entropy 通过 checkpoint 存下来。</td></tr><tr><td><code>actor_rollout_ref.actor.checkpoint.save_contents</code></td><td>保存的检查点中包含的内容。</td></tr><tr><td><code>actor_rollout_ref.actor.checkpoint.load_contents</code></td><td>从检查点加载时指定的内容。</td></tr><tr><td><code>actor_rollout_ref.actor.optim.lr</code></td><td>学习率。</td></tr><tr><td><code>actor_rollout_ref.actor.optim.lr_warmup_steps</code></td><td>预热步数；负值则由 <code>lr_warmup_steps_ratio</code> 决定。</td></tr><tr><td><code>actor_rollout_ref.actor.optim.lr_warmup_steps_ratio</code></td><td>预热步数比例（当 <code>lr_warmup_steps</code> 为负时使用）。</td></tr><tr><td><code>actor_rollout_ref.actor.optim.min_lr_ratio</code></td><td>余弦调度器的最小学习率比例。</td></tr><tr><td><code>actor_rollout_ref.actor.optim.num_cycles</code></td><td>学习率调度中的余弦周期数。</td></tr><tr><td><code>actor_rollout_ref.actor.optim.warmup_style</code></td><td>学习率预热风格：<code>constant</code> 或 <code>cosine</code>。</td></tr><tr><td><code>actor_rollout_ref.actor.optim.total_training_steps</code></td><td>总训练步数。</td></tr><tr><td><code>actor_rollout_ref.actor.optim.weight_decay</code></td><td>权重衰减系数，控制训练过程中对权重施加的 L2 正则化的强度。</td></tr><tr><td><code>actor_rollout_ref.actor.fsdp_config.wrap_policy.min_num_params</code></td><td>触发 FSDP 包装一个层的最小参数数量。</td></tr><tr><td><code>actor_rollout_ref.actor.fsdp_config.param_offload</code></td><td>是否将模型参数卸载到 CPU（以速度换内存）。</td></tr><tr><td><code>actor_rollout_ref.actor.fsdp_config.optimizer_offload</code></td><td>是否将优化器状态卸载到 CPU。</td></tr><tr><td><code>actor_rollout_ref.actor.fsdp_config.offload_policy</code></td><td>仅用于 FSDP2：训练期间卸载参数/梯度/优化器。</td></tr><tr><td><code>actor_rollout_ref.actor.fsdp_config.reshard_after_forward</code></td><td>仅用于 FSDP2：前向传播后重新分片以减少内存占用。</td></tr><tr><td><code>actor_rollout_ref.actor.fsdp_config.fsdp_size</code></td><td>每个 FSDP 分片组中的 GPU 数量；-1 表示自动。</td></tr><tr><td><code>actor_rollout_ref.actor.fsdp_config.forward_prefetch</code></td><td>仅用于 FSDP1：在前向计算完成前预取下一次前向传播的 all-gather。</td></tr><tr><td><code>actor_rollout_ref.actor.profiler.discrete</code></td><td>True 表示每个任务有自己的数据库，False 表示所有任务共享一个。</td></tr><tr><td><code>actor_rollout_ref.actor.profiler.all_ranks</code></td><td>是否对所有 rank 进行性能分析。</td></tr><tr><td><code>actor_rollout_ref.actor.profiler.ranks</code></td><td>将被分析的 rank。null 或 [0,1,&mldr;]。</td></tr><tr><td><code>actor_rollout_ref.ref.strategy</code></td><td>Reference 模型的 FSDP 配置，与 actor 相同。</td></tr><tr><td><code>actor_rollout_ref.ref.fsdp_config.param_offload</code></td><td>FSDP 中是否卸载参数。</td></tr><tr><td><code>actor_rollout_ref.ref.fsdp_config.reshard_after_forward</code></td><td>仅用于 FSDP2：是否在模型前向传播后重新分片以节省内存。</td></tr><tr><td><code>actor_rollout_ref.ref.fsdp_config.forward_prefetch</code></td><td>仅用于 FSDP1：在前向计算完成前预取下一次前向传播的 all-gather。</td></tr><tr><td><code>actor_rollout_ref.ref.fsdp_config.wrap_policy.min_num_params</code></td><td>FSDP 包装模块中的最小参数量。</td></tr><tr><td><code>actor_rollout_ref.ref.profiler.discrete</code></td><td>True 表示每个任务有自己的数据库，False 表示所有任务共享一个。</td></tr><tr><td><code>actor_rollout_ref.ref.profiler.all_ranks</code></td><td>是否对所有 rank 进行性能分析。</td></tr><tr><td><code>actor_rollout_ref.ref.profiler.ranks</code></td><td>将被分析的 rank。null 或 [0,1,&mldr;]。</td></tr></tbody></table><h3 id=reward-model>Reward Model<a hidden class=anchor aria-hidden=true href=#reward-model>#</a></h3><table><thead><tr><th>参数名称</th><th>描述</th></tr></thead><tbody><tr><td><code>reward_model.enable</code></td><td>是否启用奖励模型。如果为 False，则仅使用用户定义的奖励函数计算奖励。</td></tr><tr><td><code>reward_model.strategy</code></td><td>FSDP 策略：<code>fsdp</code> 或 <code>fsdp2</code>或<code>megatron</code>。</td></tr><tr><td><code>reward_model.model.input_tokenizer</code></td><td>输入分词器。如果奖励模型的聊天模板与策略不一致，则需要此项。</td></tr><tr><td><code>reward_model.model.path</code></td><td>RM 的 HDFS 路径或本地路径。仅支持 AutoModelForSequenceClassification。</td></tr><tr><td><code>reward_model.model.use_shm</code></td><td>是否使用共享内存加载模型。</td></tr><tr><td><code>reward_model.model.external_lib</code></td><td>外部模型实现（可选）。</td></tr><tr><td><code>reward_model.model.use_remove_padding</code></td><td>使用移除填充优化（节省计算）。</td></tr><tr><td><code>reward_model.model.use_fused_kernels</code></td><td>是否使用融合的奖励核以加速。</td></tr><tr><td><code>reward_model.model.trust_remote_code</code></td><td>是否允许加载远程代码模型，默认为 False。</td></tr><tr><td><code>reward_model.model.fsdp_config.wrap_policy.min_num_params</code></td><td>触发 FSDP 包装的最小参数数量。</td></tr><tr><td><code>reward_model.model.fsdp_config.param_offload</code></td><td>是否将模型参数卸载到 CPU。</td></tr><tr><td><code>reward_model.model.fsdp_config.reshard_after_forward</code></td><td>仅用于 FSDP2：前向传播后重新分片以减少内存占用。</td></tr><tr><td><code>reward_model.model.fsdp_config.fsdp_size</code></td><td>每个 FSDP 分片组中的 GPU 数量；-1 表示自动。</td></tr><tr><td><code>reward_model.model.fsdp_config.forward_prefetch</code></td><td>仅用于 FSDP1：在前向计算完成前预取下一次前向传播的 all-gather。</td></tr><tr><td><code>reward_model.reward_manager</code></td><td>定义计算基于规则的奖励和处理不同奖励源的机制。</td></tr><tr><td><code>reward_model.launch_reward_fn_async</code></td><td>是否在 log_prob 期间异步启动自定义奖励函数。</td></tr><tr><td><code>reward_model.sandbox_fusion.url</code></td><td>用于远程 reward 函数的 URL。</td></tr><tr><td><code>reward_model.sandbox_fusion.max_concurrent</code></td><td>允许到沙箱的最大并发请求数。</td></tr><tr><td><code>reward_model.profiler.discrete</code></td><td>True 表示每个任务有自己的数据库，False 表示所有任务共享一个。</td></tr></tbody></table><h3 id=custom-reward-function>Custom Reward Function<a hidden class=anchor aria-hidden=true href=#custom-reward-function>#</a></h3><table><thead><tr><th>参数名称</th><th>描述</th></tr></thead><tbody><tr><td><code>custom_reward_function.path</code></td><td>包含自定义奖励函数的文件路径。</td></tr><tr><td><code>custom_reward_function.name</code></td><td>指定文件中的奖励函数名称。默认为 <code>compute_score</code>。</td></tr></tbody></table><h3 id=algorithm>Algorithm<a hidden class=anchor aria-hidden=true href=#algorithm>#</a></h3><table><thead><tr><th>参数名称</th><th>描述</th></tr></thead><tbody><tr><td><code>algorithm.gamma</code></td><td>未来奖励的折扣因子。</td></tr><tr><td><code>algorithm.lam</code></td><td>GAE 估计器中偏差和方差的权衡。</td></tr><tr><td><code>algorithm.adv_estimator</code></td><td>优势估计器类型：<code>gae</code>, <code>grpo</code>, <code>reinforce_plus_plus</code> 等。</td></tr><tr><td><code>algorithm.norm_adv_by_std_in_grpo</code></td><td>是否在 GRPO 中按标准差归一化优势。</td></tr><tr><td><code>algorithm.use_kl_in_reward</code></td><td>是否在奖励中启用 KL 惩罚。</td></tr><tr><td><code>algorithm.kl_penalty</code></td><td>如何估计 KL 散度：<code>kl</code>, <code>abs</code>, <code>mse</code>, <code>low_var_kl</code>, 或 <code>full</code>。</td></tr><tr><td><code>algorithm.kl_ctrl.type</code></td><td>KL 控制类型：<code>fixed</code> 或 <code>adaptive</code>。</td></tr><tr><td><code>algorithm.kl_ctrl.kl_coef</code></td><td>KL 惩罚的初始系数。</td></tr><tr><td><code>algorithm.kl_ctrl.horizon</code></td><td>自适应控制器的 horizon 值（如果启用）。</td></tr><tr><td><code>algorithm.kl_ctrl.target_kl</code></td><td>目标 KL 散度（用于自适应控制器）。</td></tr><tr><td><code>algorithm.use_pf_ppo</code></td><td>是否启用偏好反馈 PPO。</td></tr><tr><td><code>algorithm.pf_ppo.reweight_method</code></td><td>样本重加权方法：<code>pow</code>, <code>max_min</code>, 或 <code>max_random</code>。</td></tr><tr><td><code>algorithm.pf_ppo.weight_pow</code></td><td><code>pow</code> 方法中用于权重缩放的幂。</td></tr></tbody></table><h3 id=trainer>Trainer<a hidden class=anchor aria-hidden=true href=#trainer>#</a></h3><table><thead><tr><th>参数名称</th><th>描述</th></tr></thead><tbody><tr><td><code>trainer.balance_batch</code></td><td>是否在分布式工作节点间平衡批次大小。</td></tr><tr><td><code>trainer.total_epochs</code></td><td>训练的总轮数。</td></tr><tr><td><code>trainer.total_training_steps</code></td><td>总训练步数（可显式设置或从轮数派生）。</td></tr><tr><td><code>trainer.profile_steps</code></td><td>将被分析的步骤。null 表示不进行分析。</td></tr><tr><td><code>trainer.controller_nsight_options.trace</code></td><td>对于controller进程，选择要追踪的 API（比如cuda，nvtx，cublas，etc）。</td></tr><tr><td><code>trainer.controller_nsight_options.cuda-memory-usage</code></td><td>对于controller进程，是否profile CUDA 内存使用情况。必须是字符串 <code>"true"</code> 或 <code>"false"</code>。</td></tr><tr><td><code>trainer.controller_nsight_options.cuda-graph-trace</code></td><td>对于controller进程，是否将CUDA graphs 将被作为一个整体进行追踪。</td></tr><tr><td><code>trainer.worker_nsight_options.trace</code></td><td>对于worker进程，选择要追踪的 API。</td></tr><tr><td><code>trainer.worker_nsight_options.cuda-memory-usage</code></td><td>对于worker进程，是否profile CUDA 内存使用情况。必须是字符串 <code>"true"</code> 或 <code>"false"</code>。</td></tr><tr><td><code>trainer.worker_nsight_options.cuda-graph-trace</code></td><td>对于worker进程，是否CUDA graphs 将被作为一个整体进行追踪。</td></tr><tr><td><code>trainer.worker_nsight_options.capture-range</code></td><td>仅在 torch.cuda.profiler.start 和 stop 范围内进行分析。默认值为cudaProfilerApi，不要更改此配置。</td></tr><tr><td><code>trainer.worker_nsight_options.capture-range-end</code></td><td>指定捕获范围结束时的期望行为。</td></tr><tr><td><code>trainer.worker_nsight_options.kill</code></td><td>向目标应用程序的进程组发送信号。我们让程序自行退出。</td></tr><tr><td><code>trainer.project_name</code></td><td>用于实验跟踪（如 wandb）的项目名称。</td></tr><tr><td><code>trainer.experiment_name</code></td><td>用于在跟踪工具中识别运行的实验名称。</td></tr><tr><td><code>trainer.logger</code></td><td>使用的日志后端：<code>console</code>, <code>wandb</code> 等。</td></tr><tr><td><code>trainer.log_val_generations</code></td><td>验证期间要记录的生成数量。</td></tr><tr><td><code>trainer.rollout_data_dir</code></td><td>用于记录 rollout 数据的目录；如果为 null 则不转储。</td></tr><tr><td><code>trainer.validation_data_dir</code></td><td>用于记录验证数据的目录；如果为 null 则不转储。</td></tr><tr><td><code>trainer.nnodes</code></td><td>训练中使用的节点数。</td></tr><tr><td><code>trainer.n_gpus_per_node</code></td><td>每个节点的 GPU 数量。</td></tr><tr><td><code>trainer.save_freq</code></td><td>模型检查点的保存频率（按迭代次数）。</td></tr><tr><td><code>trainer.resume_mode</code></td><td>恢复模式：<code>auto</code>, <code>disable</code>, 或 <code>resume_path</code>。</td></tr><tr><td><code>trainer.resume_from_path</code></td><td>从该路径恢复训练（仅当 resume_mode 为 <code>resume_path</code> 时使用）。</td></tr><tr><td><code>trainer.val_before_train</code></td><td>是否在训练开始前运行验证。</td></tr><tr><td><code>trainer.val_only</code></td><td>是否只运行验证。</td></tr><tr><td><code>trainer.test_freq</code></td><td>验证频率（以训练迭代次数计）。</td></tr><tr><td><code>trainer.critic_warmup</code></td><td>在更新策略之前预热 critic 的迭代次数。</td></tr><tr><td><code>trainer.default_hdfs_dir</code></td><td>用于保存检查点的默认分布式文件系统路径。</td></tr><tr><td><code>trainer.del_local_ckpt_after_load</code></td><td>加载后是否删除本地检查点。</td></tr><tr><td><code>trainer.default_local_dir</code></td><td>用于保存检查点的默认本地目录。</td></tr><tr><td><code>trainer.max_actor_ckpt_to_keep</code></td><td>保留的 actor 检查点的最大数量。</td></tr><tr><td><code>trainer.max_critic_ckpt_to_keep</code></td><td>保留的 critic 检查点的最大数量。</td></tr><tr><td><code>trainer.ray_wait_register_center_timeout</code></td><td>Ray worker 等待注册的超时时间（秒）。</td></tr><tr><td><code>trainer.device</code></td><td>运行训练的设备（如 <code>cuda</code>, <code>cpu</code>）。</td></tr></tbody></table><h3 id=ray-init>Ray Init<a hidden class=anchor aria-hidden=true href=#ray-init>#</a></h3><table><thead><tr><th>参数名称</th><th>描述</th></tr></thead><tbody><tr><td><code>ray_init.num_cpus</code></td><td>Ray 使用的 CPU 数量。使用 SLURM 时应使用固定数字而不是 null。</td></tr><tr><td><code>ray_init.timeline_json_file</code></td><td>保存 Ray 时间线 JSON 文件以进行性能分析的路径。</td></tr></tbody></table></div><footer class=post-footer><ul class=post-tags><li><a href=https://pillumina.github.io/tags/framework/>Framework</a></li><li><a href=https://pillumina.github.io/tags/llm/>LLM</a></li><li><a href=https://pillumina.github.io/tags/rl/>RL</a></li></ul><nav class=paginav><a class=next href=https://pillumina.github.io/posts/aiinfra/04-aiinfra-thinking/><span class=title>Next »</span><br><span>AI Infra：颠覆性创新，还是经典工程范式的华丽转身？</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] 参数速览 on x" href="https://x.com/intent/tweet/?text=%5bVeRL%5d%20%e5%8f%82%e6%95%b0%e9%80%9f%e8%a7%88&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f05-verl-params%2f&amp;hashtags=framework%2cLLM%2cRL"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] 参数速览 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f05-verl-params%2f&amp;title=%5bVeRL%5d%20%e5%8f%82%e6%95%b0%e9%80%9f%e8%a7%88&amp;summary=%5bVeRL%5d%20%e5%8f%82%e6%95%b0%e9%80%9f%e8%a7%88&amp;source=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f05-verl-params%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] 参数速览 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f05-verl-params%2f&title=%5bVeRL%5d%20%e5%8f%82%e6%95%b0%e9%80%9f%e8%a7%88"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] 参数速览 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f05-verl-params%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] 参数速览 on whatsapp" href="https://api.whatsapp.com/send?text=%5bVeRL%5d%20%e5%8f%82%e6%95%b0%e9%80%9f%e8%a7%88%20-%20https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f05-verl-params%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] 参数速览 on telegram" href="https://telegram.me/share/url?text=%5bVeRL%5d%20%e5%8f%82%e6%95%b0%e9%80%9f%e8%a7%88&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f05-verl-params%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [VeRL] 参数速览 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5bVeRL%5d%20%e5%8f%82%e6%95%b0%e9%80%9f%e8%a7%88&u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f05-verl-params%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul><div class=related-posts><div class=related-series><h3>同系列文章</h3><ul><li><a href=/posts/aiinfra/02-slime/>[RL4LLM] 异步RL框架: Slime</a>
<span class=meta>2025-08-07
· 15 min read</span></li><li><a href=/posts/aiinfra/03-areal/>[RL4LLM] 异步RL框架: Areal</a>
<span class=meta>2025-08-07
· 23 min read</span></li></ul></div><div class=related-tags><h3>相关文章</h3><ul><li><a href=/posts/aiinfra/02-slime/>[RL4LLM] 异步RL框架: Slime</a>
<span class=meta>2025-08-07
· 15 min read
· Tags: framework, LLM, RL</span></li><li><a href=/posts/aiinfra/03-areal/>[RL4LLM] 异步RL框架: Areal</a>
<span class=meta>2025-08-07
· 23 min read
· Tags: framework, LLM, RL</span></li></ul></div></div></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://pillumina.github.io/>CctoctoFX</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div class=reading-progress-bar></div><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelector(".reading-progress-bar");if(!t)return;const n=document.querySelector(".post-single");if(!n)return;function s(){const e=n.getBoundingClientRect(),s=e.height,o=window.innerHeight,i=window.scrollY||window.pageYOffset,a=i/(s-o)*100;t.style.width=`${Math.min(100,Math.max(0,a))}%`}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){s(),e=!1}),e=!0)}),s()}),document.addEventListener("DOMContentLoaded",function(){mediumZoom("article img:not(.nozoom)",{margin:24,background:"var(--theme)",scrollOffset:0})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>