<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[RL4LLM] 异步RL框架: Slime | CctoctoFX</title><meta name=keywords content="framework,LLM,RL"><meta name=description content="
https://github.com/THUDM/slime
一个异步实现但是非完全异步的RL框架
总体架构

从源码模块划分，有三大核心模块：

training（Megatron）：主训练流程，负责模型参数更新。
rollout（SGLang + router）：负责采样、奖励/验证生成，产生训练数据。
data buffer：桥接训练与采样，管理数据流、缓存与生成方式。


分布式调度：关于资源分配、actor启动、任务调度都由于Ray管理，支持异步训练和采样
插件机制：支持自定义buffer、模型、模型格式转换（mbridge）


  flowchart LR
    subgraph Ray[Ray 分布式调度]
        A1[Actor Group<br>训练 Actor]
        A2[Rollout Group<br>采样/生成 Actor]
        A3[Placement Group<br>资源分配]
    end
    subgraph Training[Training <Megatron>]
        T1[模型训练]
        T2[权重同步]
        T3[评估/保存]
    end
    subgraph Rollout[Rollout <SGLang+Router>]
        R1[采样/生成]
        R2[奖励模型]
        R3[过滤器]
    end
    subgraph Buffer[Data Buffer]
        B1[数据缓存]
        B2[数据流转]
        B3[Offload/Onload]
    end
    subgraph Plugins[插件机制]
        P1[Buffer 插件]
        P2[Model 插件]
        P3[mbridge 格式转换]
    end

    A1-->|训练数据|B1
    A2-->|生成数据|B1
    B1-->|数据流|A1
    B1-->|数据流|A2
    A1-->|权重同步|A2
    A1-->|评估/保存|T3
    A2-->|采样/奖励/过滤|R1
    R1-->|奖励|R2
    R1-->|过滤|R3
    B1-->|插件扩展|P1
    A1-->|模型扩展|P2
    A1-->|格式转换|P3
    A3-->|资源分配|A1
    A3-->|资源分配|A2


各模块视角的关系图
slime/rollout 组件图
rollout 负责采样、奖励、过滤，支持多种采样/奖励/过滤策略。"><meta name=author content="Me"><link rel=canonical href=https://pillumina.github.io/posts/aiinfra/02-slime/><link crossorigin=anonymous href=/assets/css/stylesheet.9d388901283682bb45dd422fcaa0d0a2054a3c8ff47c9cc6b2baab15508b1b90.css integrity="sha256-nTiJASg2grtF3UIvyqDQogVKPI/0fJzGsrqrFVCLG5A=" rel="preload stylesheet" as=style><link rel=icon href=https://pillumina.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://pillumina.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://pillumina.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://pillumina.github.io/apple-touch-icon.png><link rel=mask-icon href=https://pillumina.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://pillumina.github.io/posts/aiinfra/02-slime/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>(function(){function t(){return document.querySelector(".post-content")||document.querySelector(".post-single")||document.body}function n(e){return/\$\$[\s\S]+?\$\$|\\\(|\\\)|\\\[|\\\]/.test(e)}function s(e){if(window.__mathjaxLoaded)return;window.__mathjaxLoaded=!0,window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code","tt"],ignoreHtmlClass:"no-math"}};var t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js",t.defer=!0,t.onload=function(){window.MathJax&&window.MathJax.typesetPromise&&window.MathJax.typesetPromise([e]).catch(function(e){console.warn("MathJax typeset error",e)})},document.head.appendChild(t)}function e(){try{if(typeof renderMathInElement=="function"){const e=t();renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,strict:!1,trust:!0,ignoredTags:["script","noscript","style","textarea","pre","code","tt"],ignoredClasses:["no-math"],macros:{"\\boldsymbol":"\\mathbf{#1}","\\bm":"\\mathbf{#1}"}}),setTimeout(function(){n(e.innerHTML)&&s(e)},200)}}catch(e){console.warn("KaTeX render error:",e)}}document.addEventListener("DOMContentLoaded",function(){e(),setTimeout(e,200)}),window.addEventListener("load",function(){setTimeout(e,0)})})()</script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>const config={startOnLoad:!0,theme:"neutral",themeVariables:{lineColor:"#0f0f0f"},flowchart:{useMaxWidth:!1,htmlLabels:!0}};mermaid.initialize(config),window.onload=()=>{window.mermaid.init(0[0],document.querySelectorAll(".language-mermaid"))}</script><link rel=stylesheet href=/css/custom.min.de5dbc794941fcaf859be4ddf58c8ebc96bd8b6f47c4b2b10a1d309a8ccd26f1.css integrity="sha256-3l28eUlB/K+Fm+Td9YyOvJa9i29HxLKxCh0wmozNJvE="><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]"),n=document.querySelectorAll(".toc a");if(t.length===0||n.length===0)return;const s={};t.forEach(e=>{s[e.id]=e.offsetTop});function i(){const t=window.scrollY+100;let e="";for(const[n,o]of Object.entries(s))if(t>=o)e=n;else break;return e}function o(){const e=i();if(n.forEach(e=>{e.classList.remove("active")}),e){const t=document.querySelector(`.toc a[href="#${e}"]`);t&&t.classList.add("active")}}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){o(),e=!1}),e=!0)}),o()})</script><meta property="og:url" content="https://pillumina.github.io/posts/aiinfra/02-slime/"><meta property="og:site_name" content="CctoctoFX"><meta property="og:title" content="[RL4LLM] 异步RL框架: Slime"><meta property="og:description" content=" https://github.com/THUDM/slime
一个异步实现但是非完全异步的RL框架
总体架构 从源码模块划分，有三大核心模块： training（Megatron）：主训练流程，负责模型参数更新。 rollout（SGLang + router）：负责采样、奖励/验证生成，产生训练数据。 data buffer：桥接训练与采样，管理数据流、缓存与生成方式。 分布式调度：关于资源分配、actor启动、任务调度都由于Ray管理，支持异步训练和采样 插件机制：支持自定义buffer、模型、模型格式转换（mbridge） flowchart LR subgraph Ray[Ray 分布式调度] A1[Actor Group<br>训练 Actor] A2[Rollout Group<br>采样/生成 Actor] A3[Placement Group<br>资源分配] end subgraph Training[Training <Megatron>] T1[模型训练] T2[权重同步] T3[评估/保存] end subgraph Rollout[Rollout <SGLang+Router>] R1[采样/生成] R2[奖励模型] R3[过滤器] end subgraph Buffer[Data Buffer] B1[数据缓存] B2[数据流转] B3[Offload/Onload] end subgraph Plugins[插件机制] P1[Buffer 插件] P2[Model 插件] P3[mbridge 格式转换] end A1-->|训练数据|B1 A2-->|生成数据|B1 B1-->|数据流|A1 B1-->|数据流|A2 A1-->|权重同步|A2 A1-->|评估/保存|T3 A2-->|采样/奖励/过滤|R1 R1-->|奖励|R2 R1-->|过滤|R3 B1-->|插件扩展|P1 A1-->|模型扩展|P2 A1-->|格式转换|P3 A3-->|资源分配|A1 A3-->|资源分配|A2 各模块视角的关系图 slime/rollout 组件图 rollout 负责采样、奖励、过滤，支持多种采样/奖励/过滤策略。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-07T17:10:12+08:00"><meta property="article:modified_time" content="2025-08-07T17:10:12+08:00"><meta property="article:tag" content="Framework"><meta property="article:tag" content="LLM"><meta property="article:tag" content="RL"><meta property="og:image" content="https://pillumina.github.io/imgs/icon_head.png"><meta property="og:see_also" content="https://pillumina.github.io/posts/aiinfra/03-areal/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://pillumina.github.io/imgs/icon_head.png"><meta name=twitter:title content="[RL4LLM] 异步RL框架: Slime"><meta name=twitter:description content="
https://github.com/THUDM/slime
一个异步实现但是非完全异步的RL框架
总体架构

从源码模块划分，有三大核心模块：

training（Megatron）：主训练流程，负责模型参数更新。
rollout（SGLang + router）：负责采样、奖励/验证生成，产生训练数据。
data buffer：桥接训练与采样，管理数据流、缓存与生成方式。


分布式调度：关于资源分配、actor启动、任务调度都由于Ray管理，支持异步训练和采样
插件机制：支持自定义buffer、模型、模型格式转换（mbridge）


  flowchart LR
    subgraph Ray[Ray 分布式调度]
        A1[Actor Group<br>训练 Actor]
        A2[Rollout Group<br>采样/生成 Actor]
        A3[Placement Group<br>资源分配]
    end
    subgraph Training[Training <Megatron>]
        T1[模型训练]
        T2[权重同步]
        T3[评估/保存]
    end
    subgraph Rollout[Rollout <SGLang+Router>]
        R1[采样/生成]
        R2[奖励模型]
        R3[过滤器]
    end
    subgraph Buffer[Data Buffer]
        B1[数据缓存]
        B2[数据流转]
        B3[Offload/Onload]
    end
    subgraph Plugins[插件机制]
        P1[Buffer 插件]
        P2[Model 插件]
        P3[mbridge 格式转换]
    end

    A1-->|训练数据|B1
    A2-->|生成数据|B1
    B1-->|数据流|A1
    B1-->|数据流|A2
    A1-->|权重同步|A2
    A1-->|评估/保存|T3
    A2-->|采样/奖励/过滤|R1
    R1-->|奖励|R2
    R1-->|过滤|R3
    B1-->|插件扩展|P1
    A1-->|模型扩展|P2
    A1-->|格式转换|P3
    A3-->|资源分配|A1
    A3-->|资源分配|A2


各模块视角的关系图
slime/rollout 组件图
rollout 负责采样、奖励、过滤，支持多种采样/奖励/过滤策略。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://pillumina.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AI Infra","item":"https://pillumina.github.io/posts/aiinfra/"},{"@type":"ListItem","position":3,"name":"[RL4LLM] 异步RL框架: Slime","item":"https://pillumina.github.io/posts/aiinfra/02-slime/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[RL4LLM] 异步RL框架: Slime","name":"[RL4LLM] 异步RL框架: Slime","description":" https://github.com/THUDM/slime\n一个异步实现但是非完全异步的RL框架\n总体架构 从源码模块划分，有三大核心模块： training（Megatron）：主训练流程，负责模型参数更新。 rollout（SGLang + router）：负责采样、奖励/验证生成，产生训练数据。 data buffer：桥接训练与采样，管理数据流、缓存与生成方式。 分布式调度：关于资源分配、actor启动、任务调度都由于Ray管理，支持异步训练和采样 插件机制：支持自定义buffer、模型、模型格式转换（mbridge） flowchart LR subgraph Ray[Ray 分布式调度] A1[Actor Group\u0026lt;br\u0026gt;训练 Actor] A2[Rollout Group\u0026lt;br\u0026gt;采样/生成 Actor] A3[Placement Group\u0026lt;br\u0026gt;资源分配] end subgraph Training[Training \u0026lt;Megatron\u0026gt;] T1[模型训练] T2[权重同步] T3[评估/保存] end subgraph Rollout[Rollout \u0026lt;SGLang+Router\u0026gt;] R1[采样/生成] R2[奖励模型] R3[过滤器] end subgraph Buffer[Data Buffer] B1[数据缓存] B2[数据流转] B3[Offload/Onload] end subgraph Plugins[插件机制] P1[Buffer 插件] P2[Model 插件] P3[mbridge 格式转换] end A1--\u0026gt;|训练数据|B1 A2--\u0026gt;|生成数据|B1 B1--\u0026gt;|数据流|A1 B1--\u0026gt;|数据流|A2 A1--\u0026gt;|权重同步|A2 A1--\u0026gt;|评估/保存|T3 A2--\u0026gt;|采样/奖励/过滤|R1 R1--\u0026gt;|奖励|R2 R1--\u0026gt;|过滤|R3 B1--\u0026gt;|插件扩展|P1 A1--\u0026gt;|模型扩展|P2 A1--\u0026gt;|格式转换|P3 A3--\u0026gt;|资源分配|A1 A3--\u0026gt;|资源分配|A2 各模块视角的关系图 slime/rollout 组件图 rollout 负责采样、奖励、过滤，支持多种采样/奖励/过滤策略。\n","keywords":["framework","LLM","RL"],"articleBody":" https://github.com/THUDM/slime\n一个异步实现但是非完全异步的RL框架\n总体架构 从源码模块划分，有三大核心模块： training（Megatron）：主训练流程，负责模型参数更新。 rollout（SGLang + router）：负责采样、奖励/验证生成，产生训练数据。 data buffer：桥接训练与采样，管理数据流、缓存与生成方式。 分布式调度：关于资源分配、actor启动、任务调度都由于Ray管理，支持异步训练和采样 插件机制：支持自定义buffer、模型、模型格式转换（mbridge） flowchart LR subgraph Ray[Ray 分布式调度] A1[Actor Group\n训练 Actor] A2[Rollout Group\n采样/生成 Actor] A3[Placement Group\n资源分配] end subgraph Training[Training ] T1[模型训练] T2[权重同步] T3[评估/保存] end subgraph Rollout[Rollout ] R1[采样/生成] R2[奖励模型] R3[过滤器] end subgraph Buffer[Data Buffer] B1[数据缓存] B2[数据流转] B3[Offload/Onload] end subgraph Plugins[插件机制] P1[Buffer 插件] P2[Model 插件] P3[mbridge 格式转换] end A1--\u003e|训练数据|B1 A2--\u003e|生成数据|B1 B1--\u003e|数据流|A1 B1--\u003e|数据流|A2 A1--\u003e|权重同步|A2 A1--\u003e|评估/保存|T3 A2--\u003e|采样/奖励/过滤|R1 R1--\u003e|奖励|R2 R1--\u003e|过滤|R3 B1--\u003e|插件扩展|P1 A1--\u003e|模型扩展|P2 A1--\u003e|格式转换|P3 A3--\u003e|资源分配|A1 A3--\u003e|资源分配|A2 各模块视角的关系图 slime/rollout 组件图 rollout 负责采样、奖励、过滤，支持多种采样/奖励/过滤策略。\nflowchart TD AR[agent_rollout.py\n采样主逻辑] SE[sglang_example.py\nSGLang采样示例] SF[sft_example.py\n有监督微调采样] RM[rm_hub/\n奖励模型集] FH[filter_hub/\n过滤器集] AR--\u003e|调用|SE AR--\u003e|调用|SF AR--\u003e|奖励|RM AR--\u003e|过滤|FH agent_rollout.py：采样主流程，调度 SGLang、奖励模型、过滤器。\nsglang_example.py/sft_example.py：采样实现示例。\nrm_hub/：奖励模型集合。\nfilter_hub/：过滤器集合。\nslime/ray 组件图 ray 负责分布式 actor、buffer、PPO 训练、资源分配。\nflowchart TD PG[placement_group.py\n资源分配] AG[ray_actor.py\nActor基类] PA[ppo_actor.py\nPPO训练Actor] RO[rollout.py\nRollout Actor] BU[buffer.py\n数据Buffer] UT[utils.py\n工具函数] PG--\u003e|分配|AG AG--\u003e|继承|PA AG--\u003e|继承|RO PA--\u003e|训练数据|BU RO--\u003e|生成数据|BU BU--\u003e|数据流|PA BU--\u003e|数据流|RO slime/backends 组件图 后端适配，支持 Megatron、SGLang。\nflowchart TD MEG[megatron_utils/\nMegatron适配] SGL[sglang_utils/\nSGLang适配] MEG--\u003e|接口|训练/采样 SGL--\u003e|接口|训练/采样 slime/utils 组件图 工具、参数、类型、分布式、数据等通用功能。\nflowchart TD AR[arguments.py\n参数解析] DT[data.py\n数据工具] TY[types.py\n类型定义] PU[ppo_utils.py\nPPO工具] SU[seqlen_balancing.py\n序列长度平衡] TU[timer.py\n计时] DU[distributed_utils.py\n分布式工具] FU[flops_utils.py\nFLOPs工具] HU[http_utils.py\nHTTP工具] MU[mask_utils.py\n掩码工具] MEM[memory_utils.py\n内存工具] MI[misc.py\n杂项] AU[async_utils.py\n异步工具] AR--\u003e|参数|主流程 DT--\u003e|数据|主流程 TY--\u003e|类型|主流程 PU--\u003e|PPO|训练 SU--\u003e|平衡|训练 TU--\u003e|计时|训练/采样 DU--\u003e|分布式|训练/采样 FU--\u003e|FLOPs|训练 HU--\u003e|HTTP|采样 MU--\u003e|掩码|训练 MEM--\u003e|内存|训练 MI--\u003e|杂项|主流程 AU--\u003e|异步|主流程 slime_plugins/models 组件图 模型插件，支持不同模型适配.\nflowchart TD GLM[glm4.py\nGLM4模型适配] GLM--\u003e|模型接口|主流程 slime_plugins/mbridge 组件图 模型格式转换插件。\nflowchart TD GLM[glm4.py\nGLM4格式转换] GLM--\u003e|格式转换|主流程 关键类角度的实现关系 全局视角 classDiagram %% 主入口和配置 class MainTrain { +main() +create_placement_groups() +create_actor_group() +create_rollout_group() } class Arguments { +colocate: bool +offload: bool +actor_num_nodes: int +rollout_num_gpus: int +hf_checkpoint: str +rollout_function_path: str } %% Ray Actor 基类 class RayActor { +_get_current_node_ip_and_free_port() +get_master_addr_and_port() } %% 核心数据类 class Sample { +index: int +prompt: str +tokens: list[int] +response: str +response_length: int +reward: float +loss_mask: list[int] +status: Status +metadata: dict } class ParamInfo { +name: str +dtype: torch.dtype +shape: torch.Size +attrs: dict +size: int +src_rank: int } %% 训练相关类 class TrainRayActor { +args: Arguments +model: list +ref: list +old_actor: list +data_buffer: Buffer +rollout_engines: list +init() +train() +eval() +update_weights() +sleep() +wake_up() } class RayTrainGroup { +_actor_handlers: list[TrainRayActor] +async_init() +async_train() +async_eval() +async_update_weights() } %% 采样相关类 class RolloutRayActor { +args: Arguments +rank: int +infer_engine: SglangEngine +init() +update_weights_from_distributed() +update_weights_from_tensor() +reset_prefix_cache() +sleep() +wake_up() } class RolloutGroup { +args: Arguments +data_buffer: Buffer +rollout_engines: list[RolloutRayActor] +rollout_engine_lock: Lock +async_init() +async_generate() +async_reset_prefix_cache() } %% 数据管理类 class Buffer { +args: Arguments +dataset: JsonlDataset +buffer: list[list[Sample]] +train_data_pool: dict +eval_data_pool: dict +metadata: dict +get_samples() +add_samples() +generate() +get_data() +save() +load() } class JsonlDataset { +samples: list[Sample] +shuffle() +__len__() } %% 后端引擎类 class SglangEngine { +args: Arguments +rank: int +llm: HttpServerEngineAdapter +init_process_group() +update_weights_from_distributed() +update_weights_from_tensor() +reset_prefix_cache() +sleep() +wake_up() } class HttpServerEngineAdapter { +router_ip: str +router_port: int +init_weights_update_group() +update_weights_from_distributed() +update_weights_from_tensor() +flush_cache() } %% 插件类 class RolloutBuffer { +buffer: BufferQueue +lock: RLock +visualizer: BufferStatsVisualizer +write() +read() +peek() +get_stats() +close() } class BufferQueue { +group_size: int +max_buffer_size: int +append() +popleft() +get_batch() +__len__() } %% 工具类 class CuMemAllocator { +get_instance() +sleep() +wake_up() } class PlacementGroup { +bundles: list +strategy: str +ready() } %% 继承关系 RayActor \u003c|-- TrainRayActor RayActor \u003c|-- RolloutRayActor %% 聚合关系 MainTrain --\u003e Arguments MainTrain --\u003e RayTrainGroup MainTrain --\u003e RolloutGroup MainTrain --\u003e PlacementGroup RayTrainGroup --\u003e TrainRayActor : contains RolloutGroup --\u003e RolloutRayActor : contains RolloutGroup --\u003e Buffer : contains TrainRayActor --\u003e Buffer : uses TrainRayActor --\u003e SglangEngine : connects to RolloutRayActor --\u003e SglangEngine : contains SglangEngine --\u003e HttpServerEngineAdapter : contains Buffer --\u003e Sample : manages Buffer --\u003e JsonlDataset : uses TrainRayActor --\u003e CuMemAllocator : uses TrainRayActor --\u003e ParamInfo : manages %% 插件关系 RolloutBuffer --\u003e BufferQueue : contains RolloutBuffer --\u003e BufferStatsVisualizer : contains %% 依赖关系 Arguments --\u003e TrainRayActor : configures Arguments --\u003e RolloutRayActor : configures Arguments --\u003e Buffer : configures Arguments --\u003e SglangEngine : configures %% 数据流关系 Sample --\u003e Buffer : stored in Buffer --\u003e TrainRayActor : provides data Buffer --\u003e RolloutRayActor : receives data %% 权重同步关系 TrainRayActor --\u003e RolloutRayActor : syncs weights SglangEngine --\u003e HttpServerEngineAdapter : syncs weights 主流程精简版 classDiagram %% PPO 核心流程类 class MainTrain { +main() +create_placement_groups() +create_actor_group() +create_rollout_group() } class Arguments { +colocate: bool +offload: bool +actor_num_gpus_per_node: int +rollout_num_gpus: int +hf_checkpoint: str +rollout_function_path: str } %% 核心数据类 class Sample { +index: int +prompt: str +tokens: list[int] +response: str +response_length: int +reward: float +loss_mask: list[int] +status: Status } %% 训练 Actor class TrainRayActor { +model: list +ref: list +data_buffer: Buffer +rollout_engines: list +train() +eval() +update_weights() +sleep() +wake_up() } class RayTrainGroup { +_actor_handlers: list[TrainRayActor] +async_train() +async_eval() +async_update_weights() } %% 采样 Actor class RolloutRayActor { +infer_engine: SglangEngine +update_weights_from_distributed() +update_weights_from_tensor() +reset_prefix_cache() } class RolloutGroup { +data_buffer: Buffer +rollout_engines: list[RolloutRayActor] +async_generate() } %% 数据管理 class Buffer { +buffer: list[list[Sample]] +train_data_pool: dict +eval_data_pool: dict +get_samples() +add_samples() +generate() +get_data() +save() +load() } %% 推理引擎 class SglangEngine { +llm: HttpServerEngineAdapter +update_weights_from_distributed() +update_weights_from_tensor() +reset_prefix_cache() } %% 内存管理 class CuMemAllocator { +sleep() +wake_up() } %% 继承关系 TrainRayActor --\u003e RayActor RolloutRayActor --\u003e RayActor %% PPO 核心流程关系 MainTrain --\u003e Arguments MainTrain --\u003e RayTrainGroup MainTrain --\u003e RolloutGroup RayTrainGroup --\u003e TrainRayActor : contains RolloutGroup --\u003e RolloutRayActor : contains RolloutGroup --\u003e Buffer : contains TrainRayActor --\u003e Buffer : uses TrainRayActor --\u003e SglangEngine : syncs weights RolloutRayActor --\u003e SglangEngine : contains TrainRayActor --\u003e CuMemAllocator : uses %% 数据流关系 Sample --\u003e Buffer : stored in Buffer --\u003e TrainRayActor : provides training data Buffer --\u003e RolloutRayActor : receives generated data 关于异步实现的方式 需要注意的问题：\nslime的RL训练是rollout_id同步，不是完全的异步训练（即推理可以不等待训练完成，或者训练可以不等待推理完成）。 权重同步在每个rollout训练完成后立刻执行，确保下一个rollout使用最新权重。 1 2 3 4 5 6 7 8 9 10 # train.py 主循环 - 实际上是同步的 for rollout_id in range(args.start_rollout_id, args.num_rollout): # 1. 等待采样完成 ray.get(rollout_generator.async_generate(rollout_id)) # 2. 等待训练完成 ray.get(actor_model.async_train(rollout_id)) # 3. 等待权重同步完成 ray.get(actor_model.async_update_weights()) 虽然使用 Ray actor 的异步方法，但主循环用 ray.get() 等待每个步骤完成\n每个 rollout_id 必须按顺序完成：采样 → 训练 → 权重同步\n不是 rollout 一直生成、train actor 一直消费的完全异步模式\n因此slime中异步的边界是内部的异步优化，多个actor分布式并行训练，但是主循环还是等待所有都要完成。\n权重如何同步 权重同步流程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # RayTrainGroup.async_update_weights def async_update_weights(self): \"\"\"Broadcast weights from rank 0 to all other ranks.\"\"\" return [actor.update_weights.remote() for actor in self._actor_handlers] # TrainRayActor.update_weights @timer def update_weights(self): if self.args.debug_train_only or self.args.debug_rollout_only: return torch.cuda.empty_cache() if not self.args.colocate: self.update_weights_from_distributed() # 分布式模式 else: self.update_weights_from_tensor() # 张量模式 dist.barrier() clear_memory() print_memory(\"after update_weights\") 以及权重同步的两种模式：\n其中分布式模式适用于多节点分布式训练，利用高效的集合通信，比如大规模模型训练、需要跨节点权重同步的。\n张量模式无网络依赖，延迟低。适合单机多进程训练，内存充足（利用共享内存，传输快）的场景。适合中小规模模型、且对网络延迟比较敏感的场景。\n分析一下不同场景的延迟来源:\n$$Latency_{distributed} = Latency_{net} + Time_{serialize} + Time_{broadcast}$$\n$$Latency_{tensor} = Time_{memcopy} + Time_{serialize} + Time_{ipctransfer}$$ 分布式模式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def update_weights_from_distributed(self): # 1. 暂停 rollout engines if dist.get_rank() == 0: ray.get([engine.pause_generation.remote() for engine in self.rollout_engines]) ray.get([engine.reset_prefix_cache.remote() for engine in self.rollout_engines]) dist.barrier() # 2. 通过 NCCL 广播权重 buffer_size = 0 converted_named_tensors = [] for name, param in update_weight_utils.named_parameters(self.args, self.model): param = update_weight_utils.all_gather_param(name, param) param = update_weight_utils.remove_padding(name, param, self.vocab_size) if buffer_size + param_size \u003e self.args.update_weight_buffer_size: self._update_param_from_distributed(converted_named_tensors) buffer_size = 0 converted_named_tensors += update_weight_utils.convert_to_hf( self.args, self.model_name, name, param, self.quantization_config ) buffer_size += param_size # 3. 恢复 rollout engines if dist.get_rank() == 0: ray.get([engine.continue_generation.remote() for engine in self.rollout_engines]) dist.barrier() 张量模式 训练GPU -\u003e PCIe -\u003e CPU内存 -\u003e 序列化 -\u003e 共享内存 -\u003e 反序列化 -\u003e CPU内存 -\u003e PCIe -\u003e rollout GPU\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def update_weights_from_tensor(self): # 1. 重置 prefix cache if rank == 0: ray.get([engine.reset_prefix_cache.remote() for engine in self.rollout_engines]) dist.barrier() # 2. 通过 IPC 共享内存传输权重 for param_infos in self.param_info_buckets: params = [] for info in param_infos: if dist.get_rank() == info.src_rank: params.append(torch.nn.Parameter(self.params_dict[info.name].to(device=torch.cuda.current_device()))) else: params.append(torch.empty(info.shape, dtype=info.dtype, device=torch.cuda.current_device())) # 广播参数 if pp_size \u003e 1: handles = [] for info, param in zip(param_infos, params): handles.append(torch.distributed.broadcast(param, src=info.src_rank, group=mpu.get_pipeline_model_parallel_group(), async_op=True)) for handle in handles: handle.wait() # 3. 通过 IPC 传输到 rollout engines converted_named_tensors = [] for info, param in zip(param_infos, params): param = update_weight_utils.all_gather_param(info.name, param) param = update_weight_utils.remove_padding(info.name, param, self.vocab_size) converted_named_tensors.extend(update_weight_utils.convert_to_hf( self.args, self.model_name, info.name, param, self.quantization_config )) self._update_converted_params_from_tensor(converted_named_tensors) def _update_converted_params_from_tensor(self, converted_named_tensors): # 序列化权重并通过 IPC 传输 ipc_handle = MultiprocessingSerializer.serialize(converted_named_tensors, output_str=True) ipc_handles = [None] * dist.get_world_size(self._ipc_gather_group) if self._ipc_gather_src == dist.get_rank() else None dist.gather_object(ipc_handle, object_gather_list=ipc_handles, dst=self._ipc_gather_src, group=self._ipc_gather_group) if dist.get_rank() == self._ipc_gather_src: ref = self._ipc_engine.update_weights_from_tensor.remote(ipc_handles=ipc_handles) ray.get(ref) 训练数据流转角度 train.py主流程中训练、采样数据流转、权重同步时序：\nsequenceDiagram participant User participant Main participant Ray participant ActorGroup participant RolloutGroup participant DataBuffer User-\u003e\u003eMain: start training Main-\u003e\u003eRay: create placement groups Ray-\u003e\u003eActorGroup: launch training actors Ray-\u003e\u003eRolloutGroup: launch rollout actors Main-\u003e\u003eActorGroup: initialize model/weights Main-\u003e\u003eRolloutGroup: initialize rollout/data buffer loop for each rollout_id Main-\u003e\u003eRolloutGroup: async_generate(rollout_id) RolloutGroup-\u003e\u003eDataBuffer: write new data Main-\u003e\u003eActorGroup: async_train(rollout_id) ActorGroup-\u003e\u003eDataBuffer: read training data ActorGroup--\u003e\u003eMain: training done Main-\u003e\u003eActorGroup: async_update_weights() alt evaluation or save needed Main-\u003e\u003eActorGroup: async_eval/async_save_model end end 关于data buffer 在slime/ray/buffer.py下，实现为Ray actor(@ray.remote class Buffer)，支持高效的本地缓存和流转，数据在传输时使用Ray的对象存储，数据结构保存在Ray actor的进程内存中。\n主要功能 数据缓存与流转：缓存采样生成的数据，供训练 actor 消费，实现采样与训练的解耦。\n支持多种数据源：可从全局数据集（如 prompt 数据）或采样生成数据中获取样本。\n数据分组与批处理：每组样本可包含多个 prompt/response，便于批量训练和采样。\n元数据与状态管理：支持元数据、epoch、样本索引等状态的保存与恢复。\n支持 offload/onload：可将 buffer 状态保存到本地/远程，支持断点续训和分布式场景。\n详细类图 classDiagram class Buffer { - args - buffer : list","wordCount":"3119","inLanguage":"en","image":"https://pillumina.github.io/imgs/icon_head.png","datePublished":"2025-08-07T17:10:12+08:00","dateModified":"2025-08-07T17:10:12+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://pillumina.github.io/posts/aiinfra/02-slime/"},"publisher":{"@type":"Organization","name":"CctoctoFX","logo":{"@type":"ImageObject","url":"https://pillumina.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://pillumina.github.io/ accesskey=h title="CctoctoFX (Alt + H)"><img src=https://pillumina.github.io/apple-touch-icon.png alt aria-label=logo height=30>CctoctoFX</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://pillumina.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://pillumina.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://pillumina.github.io/posts/aiinfra/ title="AI Infra"><span>AI Infra</span></a></li><li><a href=https://pillumina.github.io/posts/llmtheory/ title=Thoery><span>Thoery</span></a></li><li><a href=https://pillumina.github.io/posts/programming/ title=Programming><span>Programming</span></a></li><li><a href=https://pillumina.github.io/social/ title=Social><span>Social</span></a></li><li><a href=https://pillumina.github.io/open_courses/ title=Study><span>Study</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://pillumina.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/aiinfra/>AI Infra</a></div><h1 class="post-title entry-hint-parent">[RL4LLM] 异步RL框架: Slime</h1><div class=post-meta><span title='2025-08-07 17:10:12 +0800 CST'>August 7, 2025</span>&nbsp;·&nbsp;15 min&nbsp;·&nbsp;3119 words&nbsp;·&nbsp;Me</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#总体架构>总体架构</a></li><li><a href=#各模块视角的关系图>各模块视角的关系图</a><ul><li><a href=#slimerollout-组件图>slime/rollout 组件图</a></li><li><a href=#slimeray组件图>slime/ray 组件图</a></li><li><a href=#slimebackends-组件图>slime/backends 组件图</a></li><li><a href=#slimeutils-组件图>slime/utils 组件图</a></li><li><a href=#slime_pluginsmodels-组件图>slime_plugins/models 组件图</a></li><li><a href=#slime_pluginsmbridge-组件图>slime_plugins/mbridge 组件图</a></li></ul></li><li><a href=#关键类角度的实现关系>关键类角度的实现关系</a><ul><li><a href=#全局视角>全局视角</a></li><li><a href=#主流程精简版>主流程精简版</a></li></ul></li><li><a href=#关于异步实现的方式>关于异步实现的方式</a></li><li><a href=#权重如何同步>权重如何同步</a></li><li><a href=#训练数据流转角度>训练数据流转角度</a></li><li><a href=#关于data-buffer>关于<code>data buffer</code></a><ul><li><a href=#主要功能>主要功能</a></li><li><a href=#详细类图>详细类图</a></li><li><a href=#buffer和主流程关键模块的关联图>Buffer和主流程关键模块的关联图</a></li><li><a href=#数据流转过程举例>数据流转过程举例</a></li><li><a href=#buffer的generate职责以及如何实现弱耦合>Buffer的<code>generate</code>职责以及如何实现弱耦合</a></li><li><a href=#默认对sglang的支持>默认对SGLang的支持</a></li><li><a href=#训推与buffer的api层级的交互>训推与buffer的api层级的交互</a><ul><li><a href=#主流程trainpy>主流程(train.py)</a></li><li><a href=#rollout-actor与buffer交互><code>Rollout Actor</code>与<code>Buffer</code>交互</a></li><li><a href=#train-actor与buffer交互><code>Train Actor</code>与<code>Buffer</code>交互</a></li></ul></li><li><a href=#更细粒度的实现分析>更细粒度的实现分析</a><ul><li><a href=#构造参数>构造参数</a></li><li><a href=#主要成员变量>主要成员变量</a></li><li><a href=#依赖对象与数据结构>依赖对象与数据结构</a><ul><li><a href=#sample>Sample</a></li><li><a href=#jsonldataset>JsonlDataset</a></li><li><a href=#buffer_filter>buffer_filter</a></li><li><a href=#generate_rollout--eval_generate_rollout>generate_rollout / eval_generate_rollout</a></li></ul></li><li><a href=#主要方法定义与作用>主要方法定义与作用</a><ul><li><a href=#initself-args><strong>init</strong>(self, args)</a></li><li><a href=#get_num_rollout_per_epochself>get_num_rollout_per_epoch(self)</a></li><li><a href=#get_samplesself-num_samples>get_samples(self, num_samples)</a></li><li><a href=#_get_samples_from_bufferself-num_samples>_get_samples_from_buffer(self, num_samples)</a></li><li><a href=#add_samplesself-samples>add_samples(self, samples)</a></li><li><a href=#generateself-rollout_id-evaluationfalse>generate(self, rollout_id, evaluation=False)</a></li><li><a href=#get_dataself-rollout_id-evaluationfalse>get_data(self, rollout_id, evaluation=False)</a></li><li><a href=#_convert_samples_to_train_dataself-samples>_convert_samples_to_train_data(self, samples)</a></li><li><a href=#_set_dataself-data-evaluationfalse>_set_data(self, data, evaluation=False)</a></li><li><a href=#update_metadataself-metadata>update_metadata(self, metadata)</a></li><li><a href=#get_metadataself>get_metadata(self)</a></li><li><a href=#get_buffer_lengthself>get_buffer_length(self)</a></li><li><a href=#saveself-rollout_id>save(self, rollout_id)</a></li><li><a href=#loadself-rollout_idnone>load(self, rollout_id=None)</a></li></ul></li><li><a href=#主要数据结构>主要数据结构</a><ul><li><a href=#selfbuffer><strong>self.buffer</strong></a></li><li><a href=#selftrain_><strong>self.train_data_pool</strong></a></li><li><a href=#selfeval_><strong>self.eval_data_pool</strong></a></li><li><a href=#selfdataset><strong>self.dataset</strong></a></li><li><a href=#selfmetadata><strong>self.metadata</strong></a></li></ul></li><li><a href=#数据流转示意>数据流转示意</a></li></ul></li></ul></li><li><a href=#流水掩盖分析>流水掩盖分析</a><ul><li><a href=#当前框架的约束>当前框架的约束</a></li><li><a href=#当前框架的掩盖效果>当前框架的掩盖效果</a></li><li><a href=#不能掩盖的部分>不能掩盖的部分</a></li><li><a href=#data-buffer当前的收益><code>data buffer</code>当前的收益</a></li><li><a href=#可以进一步掩盖的方案>可以进一步掩盖的方案</a><ul><li><a href=#log_p计算掩盖><code>log_p</code>计算掩盖</a></li></ul></li></ul></li></ul></li></ul></nav></div></details></div><div class=post-content><blockquote><p><a href=https://github.com/THUDM/slime>https://github.com/THUDM/slime</a><br>一个异步实现但是非完全异步的RL框架</p></blockquote><h2 id=总体架构>总体架构<a hidden class=anchor aria-hidden=true href=#总体架构>#</a></h2><ul><li>从源码模块划分，有三大核心模块：<ul><li>training（Megatron）：主训练流程，负责模型参数更新。</li><li>rollout（SGLang + router）：负责采样、奖励/验证生成，产生训练数据。</li><li>data buffer：桥接训练与采样，管理数据流、缓存与生成方式。</li></ul></li><li>分布式调度：关于资源分配、actor启动、任务调度都由于Ray管理，支持异步训练和采样</li><li>插件机制：支持自定义buffer、模型、模型格式转换（mbridge）</li></ul><pre class=mermaid>
  flowchart LR
    subgraph Ray[Ray 分布式调度]
        A1[Actor Group&lt;br&gt;训练 Actor]
        A2[Rollout Group&lt;br&gt;采样/生成 Actor]
        A3[Placement Group&lt;br&gt;资源分配]
    end
    subgraph Training[Training &lt;Megatron&gt;]
        T1[模型训练]
        T2[权重同步]
        T3[评估/保存]
    end
    subgraph Rollout[Rollout &lt;SGLang+Router&gt;]
        R1[采样/生成]
        R2[奖励模型]
        R3[过滤器]
    end
    subgraph Buffer[Data Buffer]
        B1[数据缓存]
        B2[数据流转]
        B3[Offload/Onload]
    end
    subgraph Plugins[插件机制]
        P1[Buffer 插件]
        P2[Model 插件]
        P3[mbridge 格式转换]
    end

    A1--&gt;|训练数据|B1
    A2--&gt;|生成数据|B1
    B1--&gt;|数据流|A1
    B1--&gt;|数据流|A2
    A1--&gt;|权重同步|A2
    A1--&gt;|评估/保存|T3
    A2--&gt;|采样/奖励/过滤|R1
    R1--&gt;|奖励|R2
    R1--&gt;|过滤|R3
    B1--&gt;|插件扩展|P1
    A1--&gt;|模型扩展|P2
    A1--&gt;|格式转换|P3
    A3--&gt;|资源分配|A1
    A3--&gt;|资源分配|A2
</pre><h2 id=各模块视角的关系图>各模块视角的关系图<a hidden class=anchor aria-hidden=true href=#各模块视角的关系图>#</a></h2><h3 id=slimerollout-组件图>slime/rollout 组件图<a hidden class=anchor aria-hidden=true href=#slimerollout-组件图>#</a></h3><p>rollout 负责采样、奖励、过滤，支持多种采样/奖励/过滤策略。</p><pre class=mermaid>
  flowchart TD
    AR[agent_rollout.py&lt;br&gt;采样主逻辑]
    SE[sglang_example.py&lt;br&gt;SGLang采样示例]
    SF[sft_example.py&lt;br&gt;有监督微调采样]
    RM[rm_hub/&lt;br&gt;奖励模型集]
    FH[filter_hub/&lt;br&gt;过滤器集]

    AR--&gt;|调用|SE
    AR--&gt;|调用|SF
    AR--&gt;|奖励|RM
    AR--&gt;|过滤|FH
</pre><ul><li><p><code>agent_rollout.py</code>：采样主流程，调度 SGLang、奖励模型、过滤器。</p></li><li><p><code>sglang_example.py/sft_example.py</code>：采样实现示例。</p></li><li><p><code>rm_hub/</code>：奖励模型集合。</p></li><li><p><code>filter_hub/</code>：过滤器集合。</p></li></ul><h3 id=slimeray组件图>slime/ray 组件图<a hidden class=anchor aria-hidden=true href=#slimeray组件图>#</a></h3><p>ray 负责分布式 actor、buffer、PPO 训练、资源分配。</p><pre class=mermaid>
  flowchart TD
    PG[placement_group.py&lt;br&gt;资源分配]
    AG[ray_actor.py&lt;br&gt;Actor基类]
    PA[ppo_actor.py&lt;br&gt;PPO训练Actor]
    RO[rollout.py&lt;br&gt;Rollout Actor]
    BU[buffer.py&lt;br&gt;数据Buffer]
    UT[utils.py&lt;br&gt;工具函数]

    PG--&gt;|分配|AG
    AG--&gt;|继承|PA
    AG--&gt;|继承|RO
    PA--&gt;|训练数据|BU
    RO--&gt;|生成数据|BU
    BU--&gt;|数据流|PA
    BU--&gt;|数据流|RO
</pre><h3 id=slimebackends-组件图>slime/backends 组件图<a hidden class=anchor aria-hidden=true href=#slimebackends-组件图>#</a></h3><p>后端适配，支持 Megatron、SGLang。</p><pre class=mermaid>
  flowchart TD
    MEG[megatron_utils/&lt;br&gt;Megatron适配]
    SGL[sglang_utils/&lt;br&gt;SGLang适配]

    MEG--&gt;|接口|训练/采样
    SGL--&gt;|接口|训练/采样
</pre><h3 id=slimeutils-组件图>slime/utils 组件图<a hidden class=anchor aria-hidden=true href=#slimeutils-组件图>#</a></h3><p>工具、参数、类型、分布式、数据等通用功能。</p><pre class=mermaid>
  flowchart TD
    AR[arguments.py&lt;br&gt;参数解析]
    DT[data.py&lt;br&gt;数据工具]
    TY[types.py&lt;br&gt;类型定义]
    PU[ppo_utils.py&lt;br&gt;PPO工具]
    SU[seqlen_balancing.py&lt;br&gt;序列长度平衡]
    TU[timer.py&lt;br&gt;计时]
    DU[distributed_utils.py&lt;br&gt;分布式工具]
    FU[flops_utils.py&lt;br&gt;FLOPs工具]
    HU[http_utils.py&lt;br&gt;HTTP工具]
    MU[mask_utils.py&lt;br&gt;掩码工具]
    MEM[memory_utils.py&lt;br&gt;内存工具]
    MI[misc.py&lt;br&gt;杂项]
    AU[async_utils.py&lt;br&gt;异步工具]

    AR--&gt;|参数|主流程
    DT--&gt;|数据|主流程
    TY--&gt;|类型|主流程
    PU--&gt;|PPO|训练
    SU--&gt;|平衡|训练
    TU--&gt;|计时|训练/采样
    DU--&gt;|分布式|训练/采样
    FU--&gt;|FLOPs|训练
    HU--&gt;|HTTP|采样
    MU--&gt;|掩码|训练
    MEM--&gt;|内存|训练
    MI--&gt;|杂项|主流程
    AU--&gt;|异步|主流程
</pre><h3 id=slime_pluginsmodels-组件图>slime_plugins/models 组件图<a hidden class=anchor aria-hidden=true href=#slime_pluginsmodels-组件图>#</a></h3><p>模型插件，支持不同模型适配.</p><pre class=mermaid>
  flowchart TD
    GLM[glm4.py&lt;br&gt;GLM4模型适配]

    GLM--&gt;|模型接口|主流程
</pre><h3 id=slime_pluginsmbridge-组件图>slime_plugins/mbridge 组件图<a hidden class=anchor aria-hidden=true href=#slime_pluginsmbridge-组件图>#</a></h3><p>模型格式转换插件。</p><pre class=mermaid>
  flowchart TD
    GLM[glm4.py&lt;br&gt;GLM4格式转换]

    GLM--&gt;|格式转换|主流程
</pre><h2 id=关键类角度的实现关系>关键类角度的实现关系<a hidden class=anchor aria-hidden=true href=#关键类角度的实现关系>#</a></h2><h3 id=全局视角>全局视角<a hidden class=anchor aria-hidden=true href=#全局视角>#</a></h3><pre class=mermaid>
  classDiagram
    %% 主入口和配置
    class MainTrain {
        +main()
        +create_placement_groups()
        +create_actor_group()
        +create_rollout_group()
    }
    
    class Arguments {
        +colocate: bool
        +offload: bool
        +actor_num_nodes: int
        +rollout_num_gpus: int
        +hf_checkpoint: str
        +rollout_function_path: str
    }
    
    %% Ray Actor 基类
    class RayActor {
        +_get_current_node_ip_and_free_port()
        +get_master_addr_and_port()
    }
    
    %% 核心数据类
    class Sample {
        +index: int
        +prompt: str
        +tokens: list[int]
        +response: str
        +response_length: int
        +reward: float
        +loss_mask: list[int]
        +status: Status
        +metadata: dict
    }
    
    class ParamInfo {
        +name: str
        +dtype: torch.dtype
        +shape: torch.Size
        +attrs: dict
        +size: int
        +src_rank: int
    }
    
    %% 训练相关类
    class TrainRayActor {
        +args: Arguments
        +model: list
        +ref: list
        +old_actor: list
        +data_buffer: Buffer
        +rollout_engines: list
        +init()
        +train()
        +eval()
        +update_weights()
        +sleep()
        +wake_up()
    }
    
    class RayTrainGroup {
        +_actor_handlers: list[TrainRayActor]
        +async_init()
        +async_train()
        +async_eval()
        +async_update_weights()
    }
    
    %% 采样相关类
    class RolloutRayActor {
        +args: Arguments
        +rank: int
        +infer_engine: SglangEngine
        +init()
        +update_weights_from_distributed()
        +update_weights_from_tensor()
        +reset_prefix_cache()
        +sleep()
        +wake_up()
    }
    
    class RolloutGroup {
        +args: Arguments
        +data_buffer: Buffer
        +rollout_engines: list[RolloutRayActor]
        +rollout_engine_lock: Lock
        +async_init()
        +async_generate()
        +async_reset_prefix_cache()
    }
    
    %% 数据管理类
    class Buffer {
        +args: Arguments
        +dataset: JsonlDataset
        +buffer: list[list[Sample]]
        +train_data_pool: dict
        +eval_data_pool: dict
        +metadata: dict
        +get_samples()
        +add_samples()
        +generate()
        +get_data()
        +save()
        +load()
    }
    
    class JsonlDataset {
        +samples: list[Sample]
        +shuffle()
        +__len__()
    }
    
    %% 后端引擎类
    class SglangEngine {
        +args: Arguments
        +rank: int
        +llm: HttpServerEngineAdapter
        +init_process_group()
        +update_weights_from_distributed()
        +update_weights_from_tensor()
        +reset_prefix_cache()
        +sleep()
        +wake_up()
    }
    
    class HttpServerEngineAdapter {
        +router_ip: str
        +router_port: int
        +init_weights_update_group()
        +update_weights_from_distributed()
        +update_weights_from_tensor()
        +flush_cache()
    }
    
    %% 插件类
    class RolloutBuffer {
        +buffer: BufferQueue
        +lock: RLock
        +visualizer: BufferStatsVisualizer
        +write()
        +read()
        +peek()
        +get_stats()
        +close()
    }
    
    class BufferQueue {
        +group_size: int
        +max_buffer_size: int
        +append()
        +popleft()
        +get_batch()
        +__len__()
    }
    
    %% 工具类
    class CuMemAllocator {
        +get_instance()
        +sleep()
        +wake_up()
    }
    
    class PlacementGroup {
        +bundles: list
        +strategy: str
        +ready()
    }
    
    %% 继承关系
    RayActor &lt;|-- TrainRayActor
    RayActor &lt;|-- RolloutRayActor
    
    %% 聚合关系
    MainTrain --&gt; Arguments
    MainTrain --&gt; RayTrainGroup
    MainTrain --&gt; RolloutGroup
    MainTrain --&gt; PlacementGroup
    
    RayTrainGroup --&gt; TrainRayActor : contains
    RolloutGroup --&gt; RolloutRayActor : contains
    RolloutGroup --&gt; Buffer : contains
    
    TrainRayActor --&gt; Buffer : uses
    TrainRayActor --&gt; SglangEngine : connects to
    RolloutRayActor --&gt; SglangEngine : contains
    
    SglangEngine --&gt; HttpServerEngineAdapter : contains
    
    Buffer --&gt; Sample : manages
    Buffer --&gt; JsonlDataset : uses
    
    TrainRayActor --&gt; CuMemAllocator : uses
    TrainRayActor --&gt; ParamInfo : manages
    
    %% 插件关系
    RolloutBuffer --&gt; BufferQueue : contains
    RolloutBuffer --&gt; BufferStatsVisualizer : contains
    
    %% 依赖关系
    Arguments --&gt; TrainRayActor : configures
    Arguments --&gt; RolloutRayActor : configures
    Arguments --&gt; Buffer : configures
    Arguments --&gt; SglangEngine : configures
    
    %% 数据流关系
    Sample --&gt; Buffer : stored in
    Buffer --&gt; TrainRayActor : provides data
    Buffer --&gt; RolloutRayActor : receives data
    
    %% 权重同步关系
    TrainRayActor --&gt; RolloutRayActor : syncs weights
    SglangEngine --&gt; HttpServerEngineAdapter : syncs weights
</pre><h3 id=主流程精简版>主流程精简版<a hidden class=anchor aria-hidden=true href=#主流程精简版>#</a></h3><pre class=mermaid>
  classDiagram
    %% PPO 核心流程类
    class MainTrain {
        +main()
        +create_placement_groups()
        +create_actor_group()
        +create_rollout_group()
    }
    
    class Arguments {
        +colocate: bool
        +offload: bool
        +actor_num_gpus_per_node: int
        +rollout_num_gpus: int
        +hf_checkpoint: str
        +rollout_function_path: str
    }
    
    %% 核心数据类
    class Sample {
        +index: int
        +prompt: str
        +tokens: list[int]
        +response: str
        +response_length: int
        +reward: float
        +loss_mask: list[int]
        +status: Status
    }
    
    %% 训练 Actor
    class TrainRayActor {
        +model: list
        +ref: list
        +data_buffer: Buffer
        +rollout_engines: list
        +train()
        +eval()
        +update_weights()
        +sleep()
        +wake_up()
    }
    
    class RayTrainGroup {
        +_actor_handlers: list[TrainRayActor]
        +async_train()
        +async_eval()
        +async_update_weights()
    }
    
    %% 采样 Actor
    class RolloutRayActor {
        +infer_engine: SglangEngine
        +update_weights_from_distributed()
        +update_weights_from_tensor()
        +reset_prefix_cache()
    }
    
    class RolloutGroup {
        +data_buffer: Buffer
        +rollout_engines: list[RolloutRayActor]
        +async_generate()
    }
    
    %% 数据管理
    class Buffer {
        +buffer: list[list[Sample]]
        +train_data_pool: dict
        +eval_data_pool: dict
        +get_samples()
        +add_samples()
        +generate()
        +get_data()
        +save()
        +load()
    }
    
    %% 推理引擎
    class SglangEngine {
        +llm: HttpServerEngineAdapter
        +update_weights_from_distributed()
        +update_weights_from_tensor()
        +reset_prefix_cache()
    }
    
    %% 内存管理
    class CuMemAllocator {
        +sleep()
        +wake_up()
    }
    
    %% 继承关系
    TrainRayActor --&gt; RayActor
    RolloutRayActor --&gt; RayActor
    
    %% PPO 核心流程关系
    MainTrain --&gt; Arguments
    MainTrain --&gt; RayTrainGroup
    MainTrain --&gt; RolloutGroup
    
    RayTrainGroup --&gt; TrainRayActor : contains
    RolloutGroup --&gt; RolloutRayActor : contains
    RolloutGroup --&gt; Buffer : contains
    
    TrainRayActor --&gt; Buffer : uses
    TrainRayActor --&gt; SglangEngine : syncs weights
    RolloutRayActor --&gt; SglangEngine : contains
    
    TrainRayActor --&gt; CuMemAllocator : uses
    
    %% 数据流关系
    Sample --&gt; Buffer : stored in
    Buffer --&gt; TrainRayActor : provides training data
    Buffer --&gt; RolloutRayActor : receives generated data
</pre><h2 id=关于异步实现的方式>关于异步实现的方式<a hidden class=anchor aria-hidden=true href=#关于异步实现的方式>#</a></h2><blockquote><p>需要注意的问题：</p><ol><li><code>slime</code>的RL训练是<code>rollout_id</code>同步，不是完全的异步训练（即推理可以不等待训练完成，或者训练可以不等待推理完成）。</li><li>权重同步在每个rollout训练完成后立刻执行，确保下一个rollout使用最新权重。</li></ol></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># train.py 主循环 - 实际上是同步的</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>rollout_id</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>start_rollout_id</span><span class=p>,</span> <span class=n>args</span><span class=o>.</span><span class=n>num_rollout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1. 等待采样完成</span>
</span></span><span class=line><span class=cl>    <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>rollout_generator</span><span class=o>.</span><span class=n>async_generate</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 2. 等待训练完成  </span>
</span></span><span class=line><span class=cl>    <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>actor_model</span><span class=o>.</span><span class=n>async_train</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 3. 等待权重同步完成</span>
</span></span><span class=line><span class=cl>    <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>actor_model</span><span class=o>.</span><span class=n>async_update_weights</span><span class=p>())</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><p>虽然使用 Ray actor 的异步方法，但主循环用 ray.get() 等待每个步骤完成</p></li><li><p>每个 rollout_id 必须按顺序完成：采样 → 训练 → 权重同步</p></li><li><p>不是 rollout 一直生成、train actor 一直消费的完全异步模式</p></li><li><p>因此<code>slime</code>中异步的边界是内部的异步优化，多个actor分布式并行训练，但是主循环还是等待所有都要完成。</p></li></ul><h2 id=权重如何同步>权重如何同步<a hidden class=anchor aria-hidden=true href=#权重如何同步>#</a></h2><p>权重同步流程：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># RayTrainGroup.async_update_weights</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>async_update_weights</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Broadcast weights from rank 0 to all other ranks.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[</span><span class=n>actor</span><span class=o>.</span><span class=n>update_weights</span><span class=o>.</span><span class=n>remote</span><span class=p>()</span> <span class=k>for</span> <span class=n>actor</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>_actor_handlers</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># TrainRayActor.update_weights</span>
</span></span><span class=line><span class=cl><span class=nd>@timer</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>update_weights</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>args</span><span class=o>.</span><span class=n>debug_train_only</span> <span class=ow>or</span> <span class=bp>self</span><span class=o>.</span><span class=n>args</span><span class=o>.</span><span class=n>debug_rollout_only</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>empty_cache</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>args</span><span class=o>.</span><span class=n>colocate</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>update_weights_from_distributed</span><span class=p>()</span>  <span class=c1># 分布式模式</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>update_weights_from_tensor</span><span class=p>()</span>       <span class=c1># 张量模式</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>dist</span><span class=o>.</span><span class=n>barrier</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>clear_memory</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>print_memory</span><span class=p>(</span><span class=s2>&#34;after update_weights&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>以及权重同步的两种模式：</p><blockquote><p>其中分布式模式适用于多节点分布式训练，利用高效的集合通信，比如大规模模型训练、需要跨节点权重同步的。<br>张量模式无网络依赖，延迟低。适合单机多进程训练，内存充足（利用共享内存，传输快）的场景。适合中小规模模型、且对网络延迟比较敏感的场景。</p></blockquote><p>分析一下不同场景的延迟来源:<br></p>$$Latency_{distributed} = Latency_{net} + Time_{serialize} + Time_{broadcast}$$<p><br></p>$$Latency_{tensor} = Time_{memcopy} + Time_{serialize} + Time_{ipctransfer}$$<ul><li>分布式模式</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>update_weights_from_distributed</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1. 暂停 rollout engines</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>dist</span><span class=o>.</span><span class=n>get_rank</span><span class=p>()</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>([</span><span class=n>engine</span><span class=o>.</span><span class=n>pause_generation</span><span class=o>.</span><span class=n>remote</span><span class=p>()</span> <span class=k>for</span> <span class=n>engine</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>rollout_engines</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>([</span><span class=n>engine</span><span class=o>.</span><span class=n>reset_prefix_cache</span><span class=o>.</span><span class=n>remote</span><span class=p>()</span> <span class=k>for</span> <span class=n>engine</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>rollout_engines</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>dist</span><span class=o>.</span><span class=n>barrier</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2. 通过 NCCL 广播权重</span>
</span></span><span class=line><span class=cl>    <span class=n>buffer_size</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>converted_named_tensors</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>update_weight_utils</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>args</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>param</span> <span class=o>=</span> <span class=n>update_weight_utils</span><span class=o>.</span><span class=n>all_gather_param</span><span class=p>(</span><span class=n>name</span><span class=p>,</span> <span class=n>param</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>param</span> <span class=o>=</span> <span class=n>update_weight_utils</span><span class=o>.</span><span class=n>remove_padding</span><span class=p>(</span><span class=n>name</span><span class=p>,</span> <span class=n>param</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>buffer_size</span> <span class=o>+</span> <span class=n>param_size</span> <span class=o>&gt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>args</span><span class=o>.</span><span class=n>update_weight_buffer_size</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_update_param_from_distributed</span><span class=p>(</span><span class=n>converted_named_tensors</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>buffer_size</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>converted_named_tensors</span> <span class=o>+=</span> <span class=n>update_weight_utils</span><span class=o>.</span><span class=n>convert_to_hf</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>args</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>model_name</span><span class=p>,</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>quantization_config</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>buffer_size</span> <span class=o>+=</span> <span class=n>param_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3. 恢复 rollout engines</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>dist</span><span class=o>.</span><span class=n>get_rank</span><span class=p>()</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>([</span><span class=n>engine</span><span class=o>.</span><span class=n>continue_generation</span><span class=o>.</span><span class=n>remote</span><span class=p>()</span> <span class=k>for</span> <span class=n>engine</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>rollout_engines</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>dist</span><span class=o>.</span><span class=n>barrier</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>张量模式</li></ul><blockquote><p>训练GPU -> PCIe -> CPU内存 -> 序列化 -> 共享内存 -> 反序列化 -> CPU内存 -> PCIe -> rollout GPU</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>update_weights_from_tensor</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1. 重置 prefix cache</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>([</span><span class=n>engine</span><span class=o>.</span><span class=n>reset_prefix_cache</span><span class=o>.</span><span class=n>remote</span><span class=p>()</span> <span class=k>for</span> <span class=n>engine</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>rollout_engines</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>dist</span><span class=o>.</span><span class=n>barrier</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 2. 通过 IPC 共享内存传输权重</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>param_infos</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>param_info_buckets</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>params</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>info</span> <span class=ow>in</span> <span class=n>param_infos</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>dist</span><span class=o>.</span><span class=n>get_rank</span><span class=p>()</span> <span class=o>==</span> <span class=n>info</span><span class=o>.</span><span class=n>src_rank</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>params</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>params_dict</span><span class=p>[</span><span class=n>info</span><span class=o>.</span><span class=n>name</span><span class=p>]</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>current_device</span><span class=p>())))</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>params</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>empty</span><span class=p>(</span><span class=n>info</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>info</span><span class=o>.</span><span class=n>dtype</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>current_device</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 广播参数</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>pp_size</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>handles</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>info</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>param_infos</span><span class=p>,</span> <span class=n>params</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>handles</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>broadcast</span><span class=p>(</span><span class=n>param</span><span class=p>,</span> <span class=n>src</span><span class=o>=</span><span class=n>info</span><span class=o>.</span><span class=n>src_rank</span><span class=p>,</span> <span class=n>group</span><span class=o>=</span><span class=n>mpu</span><span class=o>.</span><span class=n>get_pipeline_model_parallel_group</span><span class=p>(),</span> <span class=n>async_op</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>handle</span> <span class=ow>in</span> <span class=n>handles</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>handle</span><span class=o>.</span><span class=n>wait</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3. 通过 IPC 传输到 rollout engines</span>
</span></span><span class=line><span class=cl>        <span class=n>converted_named_tensors</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>info</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>param_infos</span><span class=p>,</span> <span class=n>params</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>param</span> <span class=o>=</span> <span class=n>update_weight_utils</span><span class=o>.</span><span class=n>all_gather_param</span><span class=p>(</span><span class=n>info</span><span class=o>.</span><span class=n>name</span><span class=p>,</span> <span class=n>param</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>param</span> <span class=o>=</span> <span class=n>update_weight_utils</span><span class=o>.</span><span class=n>remove_padding</span><span class=p>(</span><span class=n>info</span><span class=o>.</span><span class=n>name</span><span class=p>,</span> <span class=n>param</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>converted_named_tensors</span><span class=o>.</span><span class=n>extend</span><span class=p>(</span><span class=n>update_weight_utils</span><span class=o>.</span><span class=n>convert_to_hf</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>args</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>model_name</span><span class=p>,</span> <span class=n>info</span><span class=o>.</span><span class=n>name</span><span class=p>,</span> <span class=n>param</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>quantization_config</span>
</span></span><span class=line><span class=cl>            <span class=p>))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_update_converted_params_from_tensor</span><span class=p>(</span><span class=n>converted_named_tensors</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>_update_converted_params_from_tensor</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>converted_named_tensors</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 序列化权重并通过 IPC 传输</span>
</span></span><span class=line><span class=cl>    <span class=n>ipc_handle</span> <span class=o>=</span> <span class=n>MultiprocessingSerializer</span><span class=o>.</span><span class=n>serialize</span><span class=p>(</span><span class=n>converted_named_tensors</span><span class=p>,</span> <span class=n>output_str</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ipc_handles</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span><span class=p>]</span> <span class=o>*</span> <span class=n>dist</span><span class=o>.</span><span class=n>get_world_size</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_ipc_gather_group</span><span class=p>)</span> <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_ipc_gather_src</span> <span class=o>==</span> <span class=n>dist</span><span class=o>.</span><span class=n>get_rank</span><span class=p>()</span> <span class=k>else</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>dist</span><span class=o>.</span><span class=n>gather_object</span><span class=p>(</span><span class=n>ipc_handle</span><span class=p>,</span> <span class=n>object_gather_list</span><span class=o>=</span><span class=n>ipc_handles</span><span class=p>,</span> <span class=n>dst</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_ipc_gather_src</span><span class=p>,</span> <span class=n>group</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_ipc_gather_group</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>dist</span><span class=o>.</span><span class=n>get_rank</span><span class=p>()</span> <span class=o>==</span> <span class=bp>self</span><span class=o>.</span><span class=n>_ipc_gather_src</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>ref</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_ipc_engine</span><span class=o>.</span><span class=n>update_weights_from_tensor</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>ipc_handles</span><span class=o>=</span><span class=n>ipc_handles</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>ref</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=训练数据流转角度>训练数据流转角度<a hidden class=anchor aria-hidden=true href=#训练数据流转角度>#</a></h2><p><code>train.py</code>主流程中训练、采样数据流转、权重同步时序：</p><pre class=mermaid>
  sequenceDiagram
    participant User
    participant Main
    participant Ray
    participant ActorGroup
    participant RolloutGroup
    participant DataBuffer

    User-&gt;&gt;Main: start training
    Main-&gt;&gt;Ray: create placement groups
    Ray-&gt;&gt;ActorGroup: launch training actors
    Ray-&gt;&gt;RolloutGroup: launch rollout actors
    Main-&gt;&gt;ActorGroup: initialize model/weights
    Main-&gt;&gt;RolloutGroup: initialize rollout/data buffer
    loop for each rollout_id
        Main-&gt;&gt;RolloutGroup: async_generate(rollout_id)
        RolloutGroup-&gt;&gt;DataBuffer: write new data
        Main-&gt;&gt;ActorGroup: async_train(rollout_id)
        ActorGroup-&gt;&gt;DataBuffer: read training data
        ActorGroup--&gt;&gt;Main: training done
        Main-&gt;&gt;ActorGroup: async_update_weights()
        alt evaluation or save needed
            Main-&gt;&gt;ActorGroup: async_eval/async_save_model
        end
    end
</pre><h2 id=关于data-buffer>关于<code>data buffer</code><a hidden class=anchor aria-hidden=true href=#关于data-buffer>#</a></h2><blockquote><p>在<code>slime/ray/buffer.py</code>下，实现为Ray actor(@ray.remote class Buffer)，支持高效的本地缓存和流转，数据在传输时使用Ray的对象存储，数据结构保存在Ray actor的进程内存中。</p></blockquote><h3 id=主要功能>主要功能<a hidden class=anchor aria-hidden=true href=#主要功能>#</a></h3><ul><li><p><code>数据缓存与流转</code>：缓存采样生成的数据，供训练 actor 消费，实现采样与训练的解耦。</p></li><li><p><code>支持多种数据源</code>：可从全局数据集（如 prompt 数据）或采样生成数据中获取样本。</p></li><li><p><code>数据分组与批处理</code>：每组样本可包含多个 prompt/response，便于批量训练和采样。</p></li><li><p><code>元数据与状态管理</code>：支持元数据、epoch、样本索引等状态的保存与恢复。</p></li><li><p><code>支持 offload/onload</code>：可将 buffer 状态保存到本地/远程，支持断点续训和分布式场景。</p></li></ul><h3 id=详细类图>详细类图<a hidden class=anchor aria-hidden=true href=#详细类图>#</a></h3><pre class=mermaid>
  classDiagram
    class Buffer {
        - args
        - buffer : list&lt;list&lt;Sample&gt;&gt;
        - buffer_filter
        - train_data_pool : dict
        - eval_data_pool : dict
        - epoch_id : int
        - sample_index : int
        - sample_offset : int
        - metadata : dict
        - dataset : JsonlDataset | None
        - generate_rollout
        - eval_generate_rollout
        + __init__(args)
        + get_num_rollout_per_epoch()
        + get_samples(num_samples)
        + add_samples(samples)
        + generate(rollout_id, evaluation)
        + get_data(rollout_id, evaluation)
        + save(rollout_id)
        + load(rollout_id)
        + update_metadata(metadata)
        + get_metadata()
        + get_buffer_length()
    }
    class Sample {
        +index
        +tokens
        +response_length
        +reward
        +rewards
        +status
        +loss_mask
        +metadata
    }
    class JsonlDataset {
        +samples : list&lt;Sample&gt;
        +shuffle(epoch_id)
    }
    Buffer o-- &#34;list&lt;list&gt;&#34; Sample
    Buffer o-- JsonlDataset
    Buffer ..&gt; buffer_filter : uses
    Buffer ..&gt; generate_rollout : uses
    Buffer ..&gt; eval_generate_rollout : uses
</pre><h3 id=buffer和主流程关键模块的关联图>Buffer和主流程关键模块的关联图<a hidden class=anchor aria-hidden=true href=#buffer和主流程关键模块的关联图>#</a></h3><pre class=mermaid>
  flowchart TD
    subgraph RayActors
        TrainActor[训练 Actor &lt;PPOActor&gt;]
        RolloutActor[采样 Actor &lt;RolloutGroup&gt;]
    end
    BufferInst[Buffer &lt;Ray actor&gt;]
    Dataset[JsonlDataset]
    SampleObj[Sample]

    RolloutActor -- 生成数据 --&gt; BufferInst
    BufferInst -- add_samples(samples) --&gt; BufferInst
    TrainActor -- get_samples(num) --&gt; BufferInst
    BufferInst -- get_samples 返回 list&lt;list&lt;Sample&gt;&gt; --&gt; TrainActor
    BufferInst -- dataset.samples --&gt; Dataset
    BufferInst -- 缓存/流转 --&gt; SampleObj
</pre><h3 id=数据流转过程举例>数据流转过程举例<a hidden class=anchor aria-hidden=true href=#数据流转过程举例>#</a></h3><blockquote><p>典型流程：采样生成 -> buffer缓存 -> 训练消费</p></blockquote><ol><li>采样生成数据</li></ol><ul><li><p>RolloutActor（采样 actor）调用 buffer.generate(rollout_id)</p></li><li><p>generate 方法会调用 generate_rollout 函数，生成一批样本（Sample 对象），如：</p><ul><li>data = generate_rollout(args, rollout_id, buffer, evaluation=False)</li></ul></li><li><p>生成的数据通过 set_data 写入 train_data_pool</p></li></ul><ol start=2><li>采样数据写入 buffer</li></ol><ul><li><p>采样 actor 也可以直接调用 buffer.add_samples(samples)</p></li><li><p>samples 是 list[list[Sample]]，每组样本对应一个 prompt 的多个采样</p></li></ul><ol start=3><li>训练 actor 获取数据</li></ol><ul><li><p>训练 actor（PPOActor）调用 buffer.get_samples(num_samples)</p></li><li><p>get_samples 优先从 buffer（缓存队列）出队样本组，不足时从 dataset 生成</p></li><li><p>返回 list[list[Sample]]，每组样本可直接用于训练</p></li></ul><ol start=4><li>训练 actor 消费数据</li></ol><ul><li>训练 actor 拿到样本后，进行训练、更新权重等操作</li></ul><h3 id=buffer的generate职责以及如何实现弱耦合>Buffer的<code>generate</code>职责以及如何实现弱耦合<a hidden class=anchor aria-hidden=true href=#buffer的generate职责以及如何实现弱耦合>#</a></h3><ul><li>Buffer 的 generate 方法本质上是调用外部采样/推理函数（如 SGLang、模型采样等），这些函数通过<strong>参数动态注入（如 <code>args.rollout_function_path</code>）</strong>，所以只要采样接口一致，可以自己实现一个比如<code>generate_rollout_vllm.py</code>并在参数中指向它即可。</li><li>实现新的采样函数，其接口为：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>generate_rollout</span><span class=p>(</span><span class=n>args</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>,</span> <span class=n>buffer</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=c1># 这里调用 vllm 的推理接口</span>
</span></span><span class=line><span class=cl>      <span class=n>samples</span> <span class=o>=</span> <span class=n>vllm_generate</span><span class=p>(</span><span class=n>args</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=k>return</span> <span class=n>samples</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>在启动训练的时候，参数指定:</li></ul><pre tabindex=0><code>  --rollout_function_path path/to/generate_rollout_vllm.py:generate_rollout
</code></pre><ul><li>buffer中的伪代码可以表示为:</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># buffer.py 内部</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>generate_rollout</span> <span class=o>=</span> <span class=n>load_function</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>args</span><span class=o>.</span><span class=n>rollout_function_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>generate_fn</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>eval_generate_rollout</span> <span class=k>if</span> <span class=n>evaluation</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>generate_rollout</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>generate_fn</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>args</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>,</span> <span class=bp>self</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=n>evaluation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>_set_data</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=n>evaluation</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>因此Buffer 只负责：</li></ul><ol><li><p>调用 generate_rollout（外部推理/采样后端）</p></li><li><p>缓存采样得到的数据</p></li><li><p>提供数据给训练 actor</p></li></ol><ul><li>推理/采样的具体实现（如 SGLang、模型后端）完全在 generate_rollout 这样的外部函数里，Buffer 只是“调度者”和“缓存者”。</li></ul><h3 id=默认对sglang的支持>默认对SGLang的支持<a hidden class=anchor aria-hidden=true href=#默认对sglang的支持>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># sglang_example.py - 默认的 SGLang 采样实现</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_rollout</span><span class=p>(</span><span class=n>args</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>,</span> <span class=n>data_buffer</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;默认的 generate_rollout 函数，使用 SGLang 进行采样&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>args</span><span class=o>.</span><span class=n>rollout_global_dataset</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>evaluation</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>run</span><span class=p>(</span><span class=n>eval_rollout</span><span class=p>(</span><span class=n>args</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>run</span><span class=p>(</span><span class=n>generate_rollout_async</span><span class=p>(</span><span class=n>args</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>,</span> <span class=n>data_buffer</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 异步采样实现</span>
</span></span><span class=line><span class=cl><span class=k>async</span> <span class=k>def</span> <span class=nf>generate_rollout_async</span><span class=p>(</span><span class=n>args</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>data_buffer</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=nb>list</span><span class=p>[</span><span class=n>Sample</span><span class=p>]]:</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1. 从 buffer 获取 prompt 样本</span>
</span></span><span class=line><span class=cl>    <span class=n>samples</span> <span class=o>=</span> <span class=n>data_buffer</span><span class=o>.</span><span class=n>get_samples</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>over_sampling_batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 2. 提交 SGLang 生成任务</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span><span class=o>.</span><span class=n>submit_generate_tasks</span><span class=p>(</span><span class=n>samples</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 3. 等待生成完成</span>
</span></span><span class=line><span class=cl>    <span class=n>done</span><span class=p>,</span> <span class=n>state</span><span class=o>.</span><span class=n>pendings</span> <span class=o>=</span> <span class=k>await</span> <span class=n>asyncio</span><span class=o>.</span><span class=n>wait</span><span class=p>(</span><span class=n>state</span><span class=o>.</span><span class=n>pendings</span><span class=p>,</span> <span class=n>return_when</span><span class=o>=</span><span class=n>asyncio</span><span class=o>.</span><span class=n>FIRST_COMPLETED</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 4. 处理生成结果</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>task</span> <span class=ow>in</span> <span class=n>done</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>group</span> <span class=o>=</span> <span class=n>task</span><span class=o>.</span><span class=n>result</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>data</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>group</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>data</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=训推与buffer的api层级的交互>训推与buffer的api层级的交互<a hidden class=anchor aria-hidden=true href=#训推与buffer的api层级的交互>#</a></h3><blockquote><p>rollout_id对应一次完整的采样-训练-评估循环</p></blockquote><p>数据流动方式为：</p><ul><li><p>采样：prompt 样本 → SGLang 生成 → 存入 pool</p></li><li><p>训练：从 pool 获取 → 训练 → 删除</p></li><li><p>评估：从 pool 获取 → 评估 → 删除</p></li></ul><p>databuffer中数据结构的作用区别：</p><ul><li><p>self.buffer：采样过程中的 prompt 缓存和样本管理</p></li><li><p>train_data_pool/eval_data_pool：rollout 粒度的数据对齐和生命周期管理</p></li></ul><h4 id=主流程trainpy>主流程(train.py)<a hidden class=anchor aria-hidden=true href=#主流程trainpy>#</a></h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span><span class=n>args</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 初始化</span>
</span></span><span class=line><span class=cl>    <span class=n>actor_model</span> <span class=o>=</span> <span class=n>create_actor_group</span><span class=p>(</span><span class=n>args</span><span class=p>,</span> <span class=n>pgs</span><span class=p>[</span><span class=s2>&#34;actor&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>rollout_generator</span> <span class=o>=</span> <span class=n>create_rollout_group</span><span class=p>(</span><span class=n>args</span><span class=p>,</span> <span class=n>pgs</span><span class=p>[</span><span class=s2>&#34;rollout&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 主循环</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>rollout_id</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>start_rollout_id</span><span class=p>,</span> <span class=n>args</span><span class=o>.</span><span class=n>num_rollout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 训练采样</span>
</span></span><span class=line><span class=cl>        <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>rollout_generator</span><span class=o>.</span><span class=n>async_generate</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=c1># 训练</span>
</span></span><span class=line><span class=cl>        <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>actor_model</span><span class=o>.</span><span class=n>async_train</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=c1># 评估采样</span>
</span></span><span class=line><span class=cl>        <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>rollout_generator</span><span class=o>.</span><span class=n>async_generate</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=c1># 评估</span>
</span></span><span class=line><span class=cl>        <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>actor_model</span><span class=o>.</span><span class=n>async_eval</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=rollout-actor与buffer交互><code>Rollout Actor</code>与<code>Buffer</code>交互<a hidden class=anchor aria-hidden=true href=#rollout-actor与buffer交互>#</a></h4><ul><li>采样阶段</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># RolloutGroup.async_generate</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>async_generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>data_buffer</span><span class=o>.</span><span class=n>generate</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=n>evaluation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Buffer.generate</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 调用外部采样函数</span>
</span></span><span class=line><span class=cl>    <span class=n>generate_fn</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>eval_generate_rollout</span> <span class=k>if</span> <span class=n>evaluation</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>generate_rollout</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>generate_fn</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>args</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>,</span> <span class=bp>self</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=n>evaluation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 写入对应的 pool</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>_set_data</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=n>evaluation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Buffer._set_data</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>_set_data</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>data</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>data_pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>eval_data_pool</span> <span class=k>if</span> <span class=n>evaluation</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>train_data_pool</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=n>evaluation</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>data</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_convert_samples_to_train_data</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>  <span class=c1># 转换为训练格式</span>
</span></span><span class=line><span class=cl>    <span class=n>data_pool</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>rollout_id</span><span class=p>]</span> <span class=o>=</span> <span class=n>data</span>  <span class=c1># 存入 pool</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>其中外部采样函数（以 sglang_example.py 为例）</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># generate_rollout (sglang_example.py)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_rollout</span><span class=p>(</span><span class=n>args</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>,</span> <span class=n>data_buffer</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>evaluation</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>run</span><span class=p>(</span><span class=n>eval_rollout</span><span class=p>(</span><span class=n>args</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 从 buffer 获取 prompt 样本</span>
</span></span><span class=line><span class=cl>    <span class=n>samples</span> <span class=o>=</span> <span class=n>data_buffer</span><span class=o>.</span><span class=n>get_samples</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>over_sampling_batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 使用 SGLang 生成 response</span>
</span></span><span class=line><span class=cl>    <span class=c1># ... 生成逻辑 ...</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 返回生成的样本</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 在生成过程中可能还会调用</span>
</span></span><span class=line><span class=cl><span class=n>data_buffer</span><span class=o>.</span><span class=n>add_samples</span><span class=p>(</span><span class=n>samples</span><span class=p>)</span>  <span class=c1># 添加中间结果到 buffer</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=train-actor与buffer交互><code>Train Actor</code>与<code>Buffer</code>交互<a hidden class=anchor aria-hidden=true href=#train-actor与buffer交互>#</a></h4><ul><li>训练阶段</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># RayTrainGroup.async_train</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>async_train</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>,</span> <span class=n>with_data_fetching</span><span class=o>=</span><span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[</span><span class=n>actor</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>,</span> <span class=n>with_data_fetching</span><span class=o>=</span><span class=n>with_data_fetching</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>actor</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>_actor_handlers</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># TrainRayActor.train</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>,</span> <span class=n>with_data_fetching</span><span class=o>=</span><span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>with_data_fetching</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>get_rollout_data</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>)</span>  <span class=c1># 获取训练数据</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># ... 训练逻辑 ...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># TrainRayActor.get_rollout_data</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_rollout_data</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>megatron_utils</span><span class=o>.</span><span class=n>process_rollout_data</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>args</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>data_buffer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># process_rollout_data (megatron_utils/data.py)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>process_rollout_data</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>,</span> <span class=n>args</span><span class=p>,</span> <span class=n>data_buffer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 从 buffer 获取训练数据</span>
</span></span><span class=line><span class=cl>        <span class=n>data</span> <span class=o>=</span> <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>data_buffer</span><span class=o>.</span><span class=n>get_data</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>dist</span><span class=o>.</span><span class=n>broadcast_object_list</span><span class=p>([</span><span class=n>data</span><span class=p>],</span> <span class=n>src</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>data</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>dist</span><span class=o>.</span><span class=n>broadcast_object_list</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>src</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>data</span> <span class=o>=</span> <span class=n>data</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 处理数据用于训练</span>
</span></span><span class=line><span class=cl>    <span class=c1># ... 数据预处理逻辑 ...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Buffer.get_data</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_data</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>data_pool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>train_data_pool</span> <span class=k>if</span> <span class=ow>not</span> <span class=n>evaluation</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>eval_data_pool</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>rollout_id</span> <span class=ow>in</span> <span class=n>data_pool</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>data_pool</span><span class=p>[</span><span class=n>rollout_id</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>del</span> <span class=n>data_pool</span><span class=p>[</span><span class=n>rollout_id</span><span class=p>]</span>  <span class=c1># 取出后删除</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>data</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>评估阶段</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># RayTrainGroup.async_eval</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>async_eval</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[</span><span class=n>actor</span><span class=o>.</span><span class=n>eval</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>)</span> <span class=k>for</span> <span class=n>actor</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>_actor_handlers</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># TrainRayActor.eval</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>eval</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>rollout_id</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>megatron_utils</span><span class=o>.</span><span class=n>log_eval_data</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>args</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>data_buffer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># log_eval_data (megatron_utils/data.py)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>log_eval_data</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>,</span> <span class=n>args</span><span class=p>,</span> <span class=n>data_buffer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>data</span> <span class=o>=</span> <span class=n>ray</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>data_buffer</span><span class=o>.</span><span class=n>get_data</span><span class=o>.</span><span class=n>remote</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=c1># ... 评估和日志逻辑 ...</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=更细粒度的实现分析>更细粒度的实现分析<a hidden class=anchor aria-hidden=true href=#更细粒度的实现分析>#</a></h3><h4 id=构造参数>构造参数<a hidden class=anchor aria-hidden=true href=#构造参数>#</a></h4><ul><li><code>args</code>：配置参数对象，通常由 argparse 解析得到，包含所有训练/采样/数据相关的配置项。</li></ul><h4 id=主要成员变量>主要成员变量<a hidden class=anchor aria-hidden=true href=#主要成员变量>#</a></h4><table><thead><tr><th>名称</th><th>类型</th><th>作用与说明</th></tr></thead><tbody><tr><td><code>self.args</code></td><td>object</td><td>配置参数，包含所有 buffer 运行所需的参数。</td></tr><tr><td><code>self.buffer</code></td><td>list[list[Sample]]</td><td>主缓存队列，存储样本组（每组为同一 prompt 的多个采样）。</td></tr><tr><td><code>self.buffer_filter</code></td><td>function</td><td>样本出队策略函数，决定如何从 buffer 取出样本组。可自定义。</td></tr><tr><td><code>self.train_data_pool</code></td><td>dict[int, Any]</td><td>训练数据池，key 为 rollout_id，value 为训练数据。</td></tr><tr><td><code>self.eval_data_pool</code></td><td>dict[int, Any]</td><td>评估数据池，key 为 rollout_id，value 为评估数据。</td></tr><tr><td><code>self.epoch_id</code></td><td>int</td><td>当前数据集 epoch 号，用于 shuffle。</td></tr><tr><td><code>self.sample_index</code></td><td>int</td><td>样本全局索引，递增。</td></tr><tr><td><code>self.sample_offset</code></td><td>int</td><td>当前数据集采样偏移量。</td></tr><tr><td><code>self.metadata</code></td><td>dict</td><td>存储元数据（如采样状态、统计信息等）。</td></tr><tr><td><code>self.dataset</code></td><td>JsonlDataset or None</td><td>全局数据集对象，支持 prompt 初始化、shuffle。</td></tr><tr><td><code>self.generate_rollout</code></td><td>function</td><td>训练采样函数，外部注入，负责生成训练数据。</td></tr><tr><td><code>self.eval_generate_rollout</code></td><td>function</td><td>评估采样函数，外部注入，负责生成评估数据。</td></tr><tr><td><code>self.rollout_id</code></td><td>int</td><td>当前 rollout 的 id（仅 generate 时临时赋值）。</td></tr></tbody></table><h4 id=依赖对象与数据结构>依赖对象与数据结构<a hidden class=anchor aria-hidden=true href=#依赖对象与数据结构>#</a></h4><h5 id=sample>Sample<a hidden class=anchor aria-hidden=true href=#sample>#</a></h5><ul><li>采样/训练的基本数据单元，定义见 slime/utils/types.py。</li><li>典型字段：index, tokens, response_length, reward, rewards, status, loss_mask, metadata 等。</li></ul><h5 id=jsonldataset>JsonlDataset<a hidden class=anchor aria-hidden=true href=#jsonldataset>#</a></h5><ul><li>数据集对象，支持从 jsonl 文件加载样本，支持 shuffle、按 key 取 prompt/label/metadata。</li><li>主要属性：samples（list[Sample]），shuffle(epoch_id)。</li></ul><h5 id=buffer_filter>buffer_filter<a hidden class=anchor aria-hidden=true href=#buffer_filter>#</a></h5><ul><li>样本出队策略函数，签名为 buffer_filter(args, rollout_id, buffer, num_samples)。</li><li>默认实现为 pop_first（先进先出），可通过参数自定义。</li></ul><h5 id=generate_rollout--eval_generate_rollout>generate_rollout / eval_generate_rollout<a hidden class=anchor aria-hidden=true href=#generate_rollout--eval_generate_rollout>#</a></h5><ul><li>外部注入的采样/推理函数，签名为 generate_rollout(args, rollout_id, buffer, evaluation=False)。</li><li>负责实际调用推理后端（如 SGLang、vllm）生成样本。</li></ul><hr><h4 id=主要方法定义与作用>主要方法定义与作用<a hidden class=anchor aria-hidden=true href=#主要方法定义与作用>#</a></h4><h5 id=initself-args><strong>init</strong>(self, args)<a hidden class=anchor aria-hidden=true href=#initself-args>#</a></h5><ul><li>初始化 buffer，加载参数、数据集、采样/评估函数、buffer_filter 等。</li></ul><h5 id=get_num_rollout_per_epochself>get_num_rollout_per_epoch(self)<a hidden class=anchor aria-hidden=true href=#get_num_rollout_per_epochself>#</a></h5><ul><li>返回每个 epoch 可采样的 rollout 数量（仅全局数据集模式下有效）。</li></ul><h5 id=get_samplesself-num_samples>get_samples(self, num_samples)<a hidden class=anchor aria-hidden=true href=#get_samplesself-num_samples>#</a></h5><ul><li>获取指定数量的样本组（list[list[Sample]]）。</li><li>优先从 self.buffer 出队，不足时从 self.dataset 生成新样本组。</li><li>支持分组采样（每组 n_samples_per_prompt 个样本）。</li></ul><h5 id=_get_samples_from_bufferself-num_samples>_get_samples_from_buffer(self, num_samples)<a hidden class=anchor aria-hidden=true href=#_get_samples_from_bufferself-num_samples>#</a></h5><ul><li>内部方法，调用 buffer_filter 从 self.buffer 出队样本组。</li></ul><h5 id=add_samplesself-samples>add_samples(self, samples)<a hidden class=anchor aria-hidden=true href=#add_samplesself-samples>#</a></h5><ul><li>向 buffer 添加样本组（list[list[Sample]]）。</li><li>每组样本对应同一 prompt 的多个采样。</li></ul><h5 id=generateself-rollout_id-evaluationfalse>generate(self, rollout_id, evaluation=False)<a hidden class=anchor aria-hidden=true href=#generateself-rollout_id-evaluationfalse>#</a></h5><ul><li>调用 generate_rollout 或 eval_generate_rollout 生成数据，写入 train_data_pool 或 eval_data_pool。</li><li>采样逻辑由外部函数实现，buffer 只负责调度和缓存。</li></ul><h5 id=get_dataself-rollout_id-evaluationfalse>get_data(self, rollout_id, evaluation=False)<a hidden class=anchor aria-hidden=true href=#get_dataself-rollout_id-evaluationfalse>#</a></h5><ul><li>获取指定 rollout_id 的训练/评估数据（从 train_data_pool 或 eval_data_pool 取出并删除）。</li></ul><h5 id=_convert_samples_to_train_dataself-samples>_convert_samples_to_train_data(self, samples)<a hidden class=anchor aria-hidden=true href=#_convert_samples_to_train_dataself-samples>#</a></h5><ul><li>将采样得到的样本（Sample 列表）转换为训练数据格式（如 tokens、rewards、loss_masks 等）。</li></ul><h5 id=_set_dataself-data-evaluationfalse>_set_data(self, data, evaluation=False)<a hidden class=anchor aria-hidden=true href=#_set_dataself-data-evaluationfalse>#</a></h5><ul><li>将数据写入 train_data_pool 或 eval_data_pool。</li><li>支持 debug 数据保存。</li></ul><h5 id=update_metadataself-metadata>update_metadata(self, metadata)<a hidden class=anchor aria-hidden=true href=#update_metadataself-metadata>#</a></h5><ul><li>更新 buffer 的元数据。</li></ul><h5 id=get_metadataself>get_metadata(self)<a hidden class=anchor aria-hidden=true href=#get_metadataself>#</a></h5><ul><li>获取 buffer 的元数据。</li></ul><h5 id=get_buffer_lengthself>get_buffer_length(self)<a hidden class=anchor aria-hidden=true href=#get_buffer_lengthself>#</a></h5><ul><li>返回当前 buffer 中缓存的样本组数量。</li></ul><h5 id=saveself-rollout_id>save(self, rollout_id)<a hidden class=anchor aria-hidden=true href=#saveself-rollout_id>#</a></h5><ul><li>保存 buffer 状态（如 sample_offset、epoch_id、sample_index、metadata）到本地文件。</li></ul><h5 id=loadself-rollout_idnone>load(self, rollout_id=None)<a hidden class=anchor aria-hidden=true href=#loadself-rollout_idnone>#</a></h5><ul><li>加载 buffer 状态（如 sample_offset、epoch_id、sample_index、metadata）从本地文件。</li></ul><h4 id=主要数据结构>主要数据结构<a hidden class=anchor aria-hidden=true href=#主要数据结构>#</a></h4><h5 id=selfbuffer><strong>self.buffer</strong><a hidden class=anchor aria-hidden=true href=#selfbuffer>#</a></h5><ul><li>类型：list[list[Sample]]</li><li>结构：每个元素是一个样本组（同一 prompt 的多个采样），每组为 list[Sample]。</li><li>用途：缓存采样生成的数据，供训练 actor 批量消费。</li></ul><h5 id=selftrain_><strong>self.train_data_pool</strong><a hidden class=anchor aria-hidden=true href=#selftrain_>#</a></h5><ul><li><p>作用：缓存训练数据，供训练 actor 消费</p></li><li><p>数据结构：dict[int, Any]，key 为 rollout_id，value 为训练数据（包含 tokens、rewards、loss_masks 等）</p></li><li><p>用途：存储每个 rollout 的训练样本，用于 PPO 训练</p></li></ul><h5 id=selfeval_><strong>self.eval_data_pool</strong><a hidden class=anchor aria-hidden=true href=#selfeval_>#</a></h5><ul><li><p>作用：缓存评估数据，供评估流程使用</p></li><li><p>数据结构：dict[int, Any]，key 为 rollout_id，value 为评估数据</p></li><li><p>用途：存储每个 rollout 的评估样本，用于模型性能评估</p></li></ul><h5 id=selfdataset><strong>self.dataset</strong><a hidden class=anchor aria-hidden=true href=#selfdataset>#</a></h5><p>类型：JsonlDataset<br>结构：包含 samples（list[Sample]），支持 shuffle。<br>用途：全局数据集模式下，按需生成新样本组。</p><h5 id=selfmetadata><strong>self.metadata</strong><a hidden class=anchor aria-hidden=true href=#selfmetadata>#</a></h5><p>类型：dict<br>结构：任意元数据（如采样状态、统计信息等）。<br>用途：记录 buffer 的附加信息，便于状态恢复和监控。</p><hr><h4 id=数据流转示意>数据流转示意<a hidden class=anchor aria-hidden=true href=#数据流转示意>#</a></h4><ol><li><p><strong>采样 actor 生成数据</strong></p><ul><li>调用 buffer.generate(rollout_id)</li><li>generate_rollout(args, rollout_id, buffer) → 返回 list[Sample]</li><li>buffer._set_data(data) → 写入 train_data_pool[rollout_id]</li></ul></li><li><p><strong>训练 actor 获取数据</strong></p><ul><li>调用 buffer.get_samples(num_samples)</li><li>优先从 self.buffer 出队，不足时从 self.dataset 生成</li><li>返回 list[list[Sample]]，供训练使用</li></ul></li><li><p><strong>训练 actor 消费数据</strong></p><ul><li>训练 actor 拿到样本组后，进行训练、权重更新等操作</li></ul></li></ol><p>具体的例子：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 1. 采样生成训练数据</span>
</span></span><span class=line><span class=cl><span class=n>rollout_id</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=n>rollout_generator</span><span class=o>.</span><span class=n>async_generate</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>)</span>  <span class=c1># evaluation=False</span>
</span></span><span class=line><span class=cl><span class=c1># Buffer 内部：</span>
</span></span><span class=line><span class=cl><span class=c1># - 调用 generate_rollout(args, 100, buffer, evaluation=False)</span>
</span></span><span class=line><span class=cl><span class=c1># - 生成训练样本：[Sample1, Sample2, ...]</span>
</span></span><span class=line><span class=cl><span class=c1># - 转换为训练格式：{&#34;tokens&#34;: [...], &#34;rewards&#34;: [...], &#34;loss_masks&#34;: [...]}</span>
</span></span><span class=line><span class=cl><span class=c1># - 写入 train_data_pool[100]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 训练消费数据  </span>
</span></span><span class=line><span class=cl><span class=n>actor_model</span><span class=o>.</span><span class=n>async_train</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Buffer 内部：</span>
</span></span><span class=line><span class=cl><span class=c1># - 从 train_data_pool[100] 取出训练数据</span>
</span></span><span class=line><span class=cl><span class=c1># - 返回给训练 actor</span>
</span></span><span class=line><span class=cl><span class=c1># - 删除 train_data_pool[100]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 评估生成数据</span>
</span></span><span class=line><span class=cl><span class=n>rollout_generator</span><span class=o>.</span><span class=n>async_generate</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>,</span> <span class=n>evaluation</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Buffer 内部：</span>
</span></span><span class=line><span class=cl><span class=c1># - 调用 eval_generate_rollout(args, 100, buffer, evaluation=True)  </span>
</span></span><span class=line><span class=cl><span class=c1># - 生成评估样本</span>
</span></span><span class=line><span class=cl><span class=c1># - 写入 eval_data_pool[100]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 评估消费数据</span>
</span></span><span class=line><span class=cl><span class=n>actor_model</span><span class=o>.</span><span class=n>async_eval</span><span class=p>(</span><span class=n>rollout_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Buffer 内部：</span>
</span></span><span class=line><span class=cl><span class=c1># - 从 eval_data_pool[100] 取出评估数据</span>
</span></span><span class=line><span class=cl><span class=c1># - 返回给评估 actor</span>
</span></span><span class=line><span class=cl><span class=c1># - 删除 eval_data_pool[100]</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=流水掩盖分析>流水掩盖分析<a hidden class=anchor aria-hidden=true href=#流水掩盖分析>#</a></h2><blockquote><p>目前slime框架计算ref log_p -> old log_p -> current log_p是顺序执行的，训练阶段需要等待所有log_p计算完成，且采样->训练->权重同步之间也是通过ray.get()进行同步的，因此单actor异步掩盖的部分是计算reward、数据转换、数据存储等，而且主要的优化<strong>不是主流程上的流水掩盖</strong>而是<strong>并行异步</strong>带来的优化，比如并行计算多个样本的奖励、生成响应的时候同时计算奖励，当然还有远程reward model的网络I/O可以掩盖掉。</p></blockquote><h3 id=当前框架的约束>当前框架的约束<a hidden class=anchor aria-hidden=true href=#当前框架的约束>#</a></h3><pre class=mermaid>
  sequenceDiagram
    participant Main as Main Loop
    participant Rollout as Rollout Actor
    participant Buffer as Data Buffer
    participant Train as Train Actor
    participant RM as Reward Model
    
    Note over Main, RM: 时间线 T1 - Rollout 1
    Main-&gt;&gt;Rollout: async_generate(rollout_id=1)
    Rollout-&gt;&gt;Buffer: 生成样本
    Buffer-&gt;&gt;RM: 计算奖励（异步，可以掩盖）
    Rollout--&gt;&gt;Main: 完成
    
    Note over Main, RM: 时间线 T2 - Train 1（必须等待 T1 完成）
    Main-&gt;&gt;Train: async_train(rollout_id=1)
    Train-&gt;&gt;Buffer: 获取数据
    Train--&gt;&gt;Main: 完成
    
    Note over Main, RM: 时间线 T3 - Update Weights（必须等待 T2 完成）
    Main-&gt;&gt;Train: async_update_weights()
    Train--&gt;&gt;Main: 完成
    
    Note over Main, RM: 时间线 T4 - Rollout 2（必须等待 T3 完成）
    Main-&gt;&gt;Rollout: async_generate(rollout_id=2)
</pre><h3 id=当前框架的掩盖效果>当前框架的掩盖效果<a hidden class=anchor aria-hidden=true href=#当前框架的掩盖效果>#</a></h3><pre class=mermaid>
  graph TD
    subgraph &#34;Rollout 阶段内部掩盖&#34;
        A1[生成响应] --&gt; B1[计算奖励]
        A2[生成响应] --&gt; B2[数据预处理]
        A3[生成响应] --&gt; B3[格式转换]
        
        B1 --&gt; C1[样本完成]
        B2 --&gt; C1
        B3 --&gt; C1
        
        style A1 fill:#e8f5e8
        style B1 fill:#e8f5e8
        style B2 fill:#e8f5e8
        style B3 fill:#e8f5e8
    end
    
    subgraph &#34;多 Actor 并行&#34;
        D1[Train Actor 1] --&gt; E1[并行训练]
        D2[Train Actor 2] --&gt; E2[并行训练]
        D3[Train Actor 3] --&gt; E3[并行训练]
        
        style D1 fill:#e8f5e8
        style D2 fill:#e8f5e8
        style D3 fill:#e8f5e8
    end
    
    subgraph &#34;内存管理掩盖&#34;
        F1[模型 offload] --&gt; G1[内存释放]
        F2[数据持久化] --&gt; G2[磁盘 I/O]
        
        style F1 fill:#e8f5e8
        style F2 fill:#e8f5e8
    end
</pre><h3 id=不能掩盖的部分>不能掩盖的部分<a hidden class=anchor aria-hidden=true href=#不能掩盖的部分>#</a></h3><pre class=mermaid>
  graph TD
    subgraph &#34;主流程同步约束&#34;
        A[Rollout 1] --&gt; B[Train 1]
        B --&gt; C[Update Weights 1]
        C --&gt; D[Rollout 2]
        D --&gt; E[Train 2]
        E --&gt; F[Update Weights 2]
        
        style A fill:#ffebee
        style B fill:#ffebee
        style C fill:#ffebee
        style D fill:#ffebee
        style E fill:#ffebee
        style F fill:#ffebee
    end
    
    subgraph &#34;训练阶段同步&#34;
        G[获取数据] --&gt; H[计算 ref log_p]
        H --&gt; I[计算 old log_p]
        I --&gt; J[计算 current log_p]
        J --&gt; K[执行训练]
        
        style G fill:#ffebee
        style H fill:#ffebee
        style I fill:#ffebee
        style J fill:#ffebee
        style K fill:#ffebee
    end
</pre><h3 id=data-buffer当前的收益><code>data buffer</code>当前的收益<a hidden class=anchor aria-hidden=true href=#data-buffer当前的收益>#</a></h3><pre class=mermaid>
  sequenceDiagram
    participant Rollout as Rollout Actor
    participant Buffer as Data Buffer
    participant Train as Train Actor
    participant RM as Reward Model
    
    Note over Rollout, RM: 时间线 T1 - 并行操作
    Rollout-&gt;&gt;Buffer: 添加样本（异步）
    Buffer-&gt;&gt;RM: 计算奖励（异步，与添加并行）
    Rollout-&gt;&gt;Buffer: 数据转换（异步）
    
    Note over Rollout, RM: 时间线 T2 - 数据获取
    Train-&gt;&gt;Buffer: 获取训练数据（异步）
    Buffer-&gt;&gt;Train: 返回数据
    
    Note over Rollout, RM: 时间线 T3 - 内存管理
    Buffer-&gt;&gt;Buffer: 数据持久化（异步）
    Buffer-&gt;&gt;Buffer: 内存清理（异步）
</pre><ul><li><p>异步数据操作：添加、获取、转换可以并行</p></li><li><p>内存管理：持久化和清理可以异步</p></li><li><p>多进程访问：多个 Actor 可以并发访问</p></li></ul><h3 id=可以进一步掩盖的方案>可以进一步掩盖的方案<a hidden class=anchor aria-hidden=true href=#可以进一步掩盖的方案>#</a></h3><h4 id=log_p计算掩盖><code>log_p</code>计算掩盖<a hidden class=anchor aria-hidden=true href=#log_p计算掩盖>#</a></h4><blockquote><p>以PPO流程为例，reference model的log_p计算，权重固定（不参与训练），不计算梯度，可以提前计算；old actor的log_p计算，使用训练前的模型状态，可以在数据生成时提前计算。不过因为reward计算和log_p计算都需要完整的采样生成，所以实际应该只是reward计算（以及其他可并行项）和log_p计算之间的流水掩盖。</p></blockquote><pre class=mermaid>
  graph TD
    subgraph &#34;当前实现&#34;
        A1[生成数据] --&gt; B1[等待完成]
        B1 --&gt; C1[计算 ref log_p]
        C1 --&gt; D1[计算 old log_p]
        D1 --&gt; E1[计算 current log_p]
        E1 --&gt; F1[训练]
        
        style A1 fill:#ffebee
        style B1 fill:#ffebee
        style C1 fill:#ffebee
        style D1 fill:#ffebee
        style E1 fill:#ffebee
        style F1 fill:#ffebee
    end
    
    subgraph &#34;优化方案&#34;
        A2[生成数据] --&gt; B2[并行计算 log_p]
        B2 --&gt; C2[ref log_p 计算]
        B2 --&gt; D2[old log_p 计算]
        C2 --&gt; E2[数据准备完成]
        D2 --&gt; E2
        E2 --&gt; F2[训练（只需 current log_p）]
        
        style A2 fill:#e8f5e8
        style B2 fill:#e8f5e8
        style C2 fill:#e8f5e8
        style D2 fill:#e8f5e8
        style E2 fill:#e8f5e8
        style F2 fill:#e8f5e8
    end
</pre><pre class=mermaid>
  sequenceDiagram
    participant Main as Main Loop
    participant Rollout as Rollout Actor
    participant Buffer as Data Buffer
    participant Train as Train Actor
    
    Note over Main, Train: 当前实现时间线
    Main-&gt;&gt;Rollout: Rollout 1
    Rollout-&gt;&gt;Buffer: 生成 + 奖励计算
    Buffer--&gt;&gt;Rollout: 完成
    Rollout--&gt;&gt;Main: 完成
    Main-&gt;&gt;Train: Train 1
    Train-&gt;&gt;Train: 计算所有 log_p（顺序）
    Train-&gt;&gt;Train: 训练
    Train--&gt;&gt;Main: 完成
    
    Note over Main, Train: 优化方案时间线
    Main-&gt;&gt;Rollout: Rollout 1
    Rollout-&gt;&gt;Buffer: 生成 + 奖励计算 + 并行 log_p
    Buffer--&gt;&gt;Rollout: 完成（更快）
    Rollout--&gt;&gt;Main: 完成
    Main-&gt;&gt;Train: Train 1
    Train-&gt;&gt;Train: 只需计算 current log_p
    Train-&gt;&gt;Train: 训练
    Train--&gt;&gt;Main: 完成
</pre><pre class=mermaid>
  sequenceDiagram
    participant Rollout as Rollout Actor
    participant Buffer as Data Buffer
    participant Ref as Reference Model
    participant Old as Old Actor
    participant Train as Train Actor
    
    Note over Rollout, Train: 时间线 T1 - 并行计算
    Rollout-&gt;&gt;Buffer: 生成样本数据
    Buffer-&gt;&gt;Ref: 计算 ref log_p（并行）
    Buffer-&gt;&gt;Old: 计算 old log_p（并行）
    
    Note over Rollout, Train: 时间线 T2 - 训练
    Train-&gt;&gt;Buffer: 获取数据 + log_p
    Train-&gt;&gt;Train: 计算 current log_p
    Train-&gt;&gt;Train: 执行训练步骤
</pre></div><footer class=post-footer><ul class=post-tags><li><a href=https://pillumina.github.io/tags/framework/>Framework</a></li><li><a href=https://pillumina.github.io/tags/llm/>LLM</a></li><li><a href=https://pillumina.github.io/tags/rl/>RL</a></li></ul><nav class=paginav><a class=prev href=https://pillumina.github.io/posts/llmtheory/1-moe/><span class=title>« Prev</span><br><span>MoE环游记：1、从几何意义出发</span>
</a><a class=next href=https://pillumina.github.io/posts/aiinfra/03-areal/><span class=title>Next »</span><br><span>[RL4LLM] 异步RL框架: Areal</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [RL4LLM] 异步RL框架: Slime on x" href="https://x.com/intent/tweet/?text=%5bRL4LLM%5d%20%e5%bc%82%e6%ad%a5RL%e6%a1%86%e6%9e%b6%3a%20Slime&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f02-slime%2f&amp;hashtags=framework%2cLLM%2cRL"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [RL4LLM] 异步RL框架: Slime on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f02-slime%2f&amp;title=%5bRL4LLM%5d%20%e5%bc%82%e6%ad%a5RL%e6%a1%86%e6%9e%b6%3a%20Slime&amp;summary=%5bRL4LLM%5d%20%e5%bc%82%e6%ad%a5RL%e6%a1%86%e6%9e%b6%3a%20Slime&amp;source=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f02-slime%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [RL4LLM] 异步RL框架: Slime on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f02-slime%2f&title=%5bRL4LLM%5d%20%e5%bc%82%e6%ad%a5RL%e6%a1%86%e6%9e%b6%3a%20Slime"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [RL4LLM] 异步RL框架: Slime on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f02-slime%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [RL4LLM] 异步RL框架: Slime on whatsapp" href="https://api.whatsapp.com/send?text=%5bRL4LLM%5d%20%e5%bc%82%e6%ad%a5RL%e6%a1%86%e6%9e%b6%3a%20Slime%20-%20https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f02-slime%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [RL4LLM] 异步RL框架: Slime on telegram" href="https://telegram.me/share/url?text=%5bRL4LLM%5d%20%e5%bc%82%e6%ad%a5RL%e6%a1%86%e6%9e%b6%3a%20Slime&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f02-slime%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [RL4LLM] 异步RL框架: Slime on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5bRL4LLM%5d%20%e5%bc%82%e6%ad%a5RL%e6%a1%86%e6%9e%b6%3a%20Slime&u=https%3a%2f%2fpillumina.github.io%2fposts%2faiinfra%2f02-slime%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul><div class=related-posts><div class=related-series><h3>同系列文章</h3><ul><li><a href=/posts/aiinfra/03-areal/>[RL4LLM] 异步RL框架: Areal</a>
<span class=meta>2025-08-07
· 23 min read</span></li></ul></div><div class=related-tags><h3>相关文章</h3><ul><li><a href=/posts/aiinfra/14-deterministic-rl/>[Deterministic RL] 确定性问题的来源 & Reproducible RL</a>
<span class=meta>2025-11-20
· 6 min read
· Tags: deterministic, RL</span></li><li><a href=/posts/aiinfra/10-verl-dataproto/>[VeRL] DataProto介绍</a>
<span class=meta>2025-08-25
· 17 min read
· Tags: framework, verl</span></li><li><a href=/posts/aiinfra/09-verl-agentloop/>[VeRL] AgentLoop源码走读</a>
<span class=meta>2025-08-14
· 15 min read
· Tags: framework, verl, sglang</span></li><li><a href=/posts/aiinfra/05-verl-params/>[VeRL] 参数速览</a>
<span class=meta>2025-08-14
· 6 min read
· Tags: framework, verl</span></li></ul></div></div></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://pillumina.github.io/>CctoctoFX</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div class=reading-progress-bar></div><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelector(".reading-progress-bar");if(!t)return;const n=document.querySelector(".post-single");if(!n)return;function s(){const e=n.getBoundingClientRect(),s=e.height,o=window.innerHeight,i=window.scrollY||window.pageYOffset,a=i/(s-o)*100;t.style.width=`${Math.min(100,Math.max(0,a))}%`}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){s(),e=!1}),e=!0)}),s()}),document.addEventListener("DOMContentLoaded",function(){mediumZoom("article img:not(.nozoom)",{margin:24,background:"var(--theme)",scrollOffset:0})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>