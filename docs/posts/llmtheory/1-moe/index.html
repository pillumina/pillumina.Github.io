<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>MoE环游记：1、从几何意义出发 | CctoctoFX</title><meta name=keywords content="MoE"><meta name=description content="MoE（Mixture of Experts）架构的流行自不必多说，近来火出圈的DeepSeek-V3便是MoE架构，传言GPT-5也是MoE架构，国内最近出的一些模型（Qwen3系列相关）也有不少用上了MoE。然而，虽然MoE的研究由来已久，但其应用长时间内都不愠不火，大致上是从去年初的《Mixtral of Experts》开始，MoE才逐渐吸引大家的注意力，其显著优点是参数量大，但训练和推理成本都显著低。
但同时MoE也有一些难题，如训练不稳定、负载不均衡、效果不够好等，这也是它早年没有流行起来的主要原因。不过随着这两年关注度的提升，这些问题在很大程度上已经得到解决，我们在接下来的介绍中会逐一谈到这些内容。
问题定义
我们知道，Transformer模型由Attention层和MLP层组成，MoE替换的是模型中MLP层。MLP层又分FFN（FeedForward Network）和GLU（Gated Linear Unit）两种，主流的是GLU，但简单起见我们还是以FFN为例：$$y=f(xW^{(A)})W^{(B)}$$其中$x\in\mathbb{R}^d$ 是输入向量（行向量），$W^{(A)}\in\mathbb{R}^{d\times{D}}$, $W^{(B)}\in\mathbb{R}^{D\times{d}}$ 是两个参数矩阵，$f$是Element-wise的激活函数，设$n$是一个能整除$D$的整数，那么上面的FFN可以用分块矩阵等价：

$$ \begin{equation}\boldsymbol{y} = f\big(\boldsymbol{x}\begin{bmatrix}\boldsymbol{W}^{(A)}_1 & \boldsymbol{W}^{(A)}_2 & \cdots & \boldsymbol{W}^{(A)}_n\end{bmatrix}\big)\begin{bmatrix}\boldsymbol{W}^{(B)}_1 \\ \boldsymbol{W}^{(B)}_2 \\ \vdots \\ \boldsymbol{W}^{(B)}_n\end{bmatrix} = \sum_{i=1}^n \underbrace{f(\boldsymbol{x}\boldsymbol{W}^{(A)}_i)\boldsymbol{W}^{(B)}_i}_{\boldsymbol{v}_i}\end{equation} $$

其中
$W^{(A)}_i = W^{(A)}_{[:,(i-1)c:ic]}$, $W^{(B)}_i = W^{(B)}_{[(i-1)c:ic,:]}$, $c= D/n$，这里的切片按照Python规则来。由此可见，FFN可以等价表示成n个向量
$\boldsymbol{v}_1,\boldsymbol{v}_2,\cdots,\boldsymbol{v}_n$
之和，每个向量代表了一个小模型$f(\boldsymbol{x}\boldsymbol{W}^{(A)}_i)\boldsymbol{W}^{(B)}_i$的输出，每个小模型计算量相同，这些小模型就是MoE中的“Expert”。
MoE提出的问题是：

能否只挑k个向量的和来逼近n个向量的和呢？这样就可以将计算量降低到k/n了。
模长排序
要解决上述的问题，实质上是要解决低秩近似的问题，数学公式就是:

$$\begin{equation}\mathop{\text{argmin}}_{\lambda_1,\lambda_2,\cdots,\lambda_n\in\{0,1\}}\left\Vert\sum_{i=1}^n \lambda_i \boldsymbol{v}_i - \sum_{i=1}^n\boldsymbol{v}_i\right\Vert^2\quad\text{s.t.}\quad \sum_{i=1}^n \lambda_i = k\end{equation}$$ 

记$\gamma_i = 1 - \lambda_i$，那么它又可以写成：

$$\begin{equation}\mathop{\text{argmin}}_{\gamma_1,\gamma_2,\cdots,\gamma_n\in\{0,1\}}\left\Vert\sum_{i=1}^n \gamma_i \boldsymbol{v}_i\right\Vert^2\quad\text{s.t.}\quad \sum_{i=1}^n \gamma_i = n - k\end{equation}$$

这个问题的精确求解是比较困难的（NP Hard），但有一个简单的近似解：当$v_i$两两正交时，我们有

$$\begin{equation}\left\Vert\sum_{i=1}^n \gamma_i \boldsymbol{v}_i\right\Vert^2 = \sum_{i=1}^n \gamma_i^2 \Vert\boldsymbol{v}_i\Vert^2 = \sum_{i=1}^n \gamma_i \Vert\boldsymbol{v}_i\Vert^2\end{equation}$$

上式最优解显然就是让模长$\Vert\boldsymbol{v}_i\Vert$最小的$n-k$个$\gamma_i$等于1，这又等价于说挑出模长最大的$k$个向量来逼近$n$个向量之和。当$v_i$不满足两两正交的条件时，我们依然用它来作为一个近似解。它的几何意义也很直观，模长越大的向量，在求和过程中越不容易被抵消，从而作用越突出。"><meta name=author content="Me"><link rel=canonical href=https://pillumina.github.io/posts/llmtheory/1-moe/><link crossorigin=anonymous href=/assets/css/stylesheet.9d388901283682bb45dd422fcaa0d0a2054a3c8ff47c9cc6b2baab15508b1b90.css integrity="sha256-nTiJASg2grtF3UIvyqDQogVKPI/0fJzGsrqrFVCLG5A=" rel="preload stylesheet" as=style><link rel=icon href=https://pillumina.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://pillumina.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://pillumina.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://pillumina.github.io/apple-touch-icon.png><link rel=mask-icon href=https://pillumina.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://pillumina.github.io/posts/llmtheory/1-moe/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>(function(){function t(){return document.querySelector(".post-content")||document.querySelector(".post-single")||document.body}function n(e){return/\$\$[\s\S]+?\$\$|\\\(|\\\)|\\\[|\\\]/.test(e)}function s(e){if(window.__mathjaxLoaded)return;window.__mathjaxLoaded=!0,window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code","tt"],ignoreHtmlClass:"no-math"}};var t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js",t.defer=!0,t.onload=function(){window.MathJax&&window.MathJax.typesetPromise&&window.MathJax.typesetPromise([e]).catch(function(e){console.warn("MathJax typeset error",e)})},document.head.appendChild(t)}function e(){try{if(typeof renderMathInElement=="function"){const e=t();renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,strict:!1,trust:!0,ignoredTags:["script","noscript","style","textarea","pre","code","tt"],ignoredClasses:["no-math"],macros:{"\\boldsymbol":"\\mathbf{#1}","\\bm":"\\mathbf{#1}"}}),setTimeout(function(){n(e.innerHTML)&&s(e)},200)}}catch(e){console.warn("KaTeX render error:",e)}}document.addEventListener("DOMContentLoaded",function(){e(),setTimeout(e,200)}),window.addEventListener("load",function(){setTimeout(e,0)})})()</script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>const config={startOnLoad:!0,theme:"neutral",themeVariables:{lineColor:"#0f0f0f"},flowchart:{useMaxWidth:!1,htmlLabels:!0}};mermaid.initialize(config),window.onload=()=>{window.mermaid.init(0[0],document.querySelectorAll(".language-mermaid"))}</script><link rel=stylesheet href=/css/custom.min.bda7229c4269a242639e058fb11a4782f02f8d77071ba16609befee67cc41c49.css integrity="sha256-vacinEJpokJjngWPsRpHgvAvjXcHG6FmCb7+5nzEHEk="><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]"),n=document.querySelectorAll(".toc a");if(t.length===0||n.length===0)return;const s={};t.forEach(e=>{s[e.id]=e.offsetTop});function i(){const t=window.scrollY+100;let e="";for(const[n,o]of Object.entries(s))if(t>=o)e=n;else break;return e}function o(){const e=i();if(n.forEach(e=>{e.classList.remove("active")}),e){const t=document.querySelector(`.toc a[href="#${e}"]`);t&&t.classList.add("active")}}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){o(),e=!1}),e=!0)}),o()})</script><meta property="og:url" content="https://pillumina.github.io/posts/llmtheory/1-moe/"><meta property="og:site_name" content="CctoctoFX"><meta property="og:title" content="MoE环游记：1、从几何意义出发"><meta property="og:description" content="MoE（Mixture of Experts）架构的流行自不必多说，近来火出圈的DeepSeek-V3便是MoE架构，传言GPT-5也是MoE架构，国内最近出的一些模型（Qwen3系列相关）也有不少用上了MoE。然而，虽然MoE的研究由来已久，但其应用长时间内都不愠不火，大致上是从去年初的《Mixtral of Experts》开始，MoE才逐渐吸引大家的注意力，其显著优点是参数量大，但训练和推理成本都显著低。
但同时MoE也有一些难题，如训练不稳定、负载不均衡、效果不够好等，这也是它早年没有流行起来的主要原因。不过随着这两年关注度的提升，这些问题在很大程度上已经得到解决，我们在接下来的介绍中会逐一谈到这些内容。
问题定义 我们知道，Transformer模型由Attention层和MLP层组成，MoE替换的是模型中MLP层。MLP层又分FFN（FeedForward Network）和GLU（Gated Linear Unit）两种，主流的是GLU，但简单起见我们还是以FFN为例：$$y=f(xW^{(A)})W^{(B)}$$其中$x\in\mathbb{R}^d$ 是输入向量（行向量），$W^{(A)}\in\mathbb{R}^{d\times{D}}$, $W^{(B)}\in\mathbb{R}^{D\times{d}}$ 是两个参数矩阵，$f$是Element-wise的激活函数，设$n$是一个能整除$D$的整数，那么上面的FFN可以用分块矩阵等价：
$$ \begin{equation}\boldsymbol{y} = f\big(\boldsymbol{x}\begin{bmatrix}\boldsymbol{W}^{(A)}_1 & \boldsymbol{W}^{(A)}_2 & \cdots & \boldsymbol{W}^{(A)}_n\end{bmatrix}\big)\begin{bmatrix}\boldsymbol{W}^{(B)}_1 \\ \boldsymbol{W}^{(B)}_2 \\ \vdots \\ \boldsymbol{W}^{(B)}_n\end{bmatrix} = \sum_{i=1}^n \underbrace{f(\boldsymbol{x}\boldsymbol{W}^{(A)}_i)\boldsymbol{W}^{(B)}_i}_{\boldsymbol{v}_i}\end{equation} $$ 其中
$W^{(A)}_i = W^{(A)}_{[:,(i-1)c:ic]}$, $W^{(B)}_i = W^{(B)}_{[(i-1)c:ic,:]}$, $c= D/n$，这里的切片按照Python规则来。由此可见，FFN可以等价表示成n个向量
$\boldsymbol{v}_1,\boldsymbol{v}_2,\cdots,\boldsymbol{v}_n$
之和，每个向量代表了一个小模型$f(\boldsymbol{x}\boldsymbol{W}^{(A)}_i)\boldsymbol{W}^{(B)}_i$的输出，每个小模型计算量相同，这些小模型就是MoE中的“Expert”。
MoE提出的问题是：
能否只挑k个向量的和来逼近n个向量的和呢？这样就可以将计算量降低到k/n了。
模长排序 要解决上述的问题，实质上是要解决低秩近似的问题，数学公式就是:
$$\begin{equation}\mathop{\text{argmin}}_{\lambda_1,\lambda_2,\cdots,\lambda_n\in\{0,1\}}\left\Vert\sum_{i=1}^n \lambda_i \boldsymbol{v}_i - \sum_{i=1}^n\boldsymbol{v}_i\right\Vert^2\quad\text{s.t.}\quad \sum_{i=1}^n \lambda_i = k\end{equation}$$ 记$\gamma_i = 1 - \lambda_i$，那么它又可以写成：
$$\begin{equation}\mathop{\text{argmin}}_{\gamma_1,\gamma_2,\cdots,\gamma_n\in\{0,1\}}\left\Vert\sum_{i=1}^n \gamma_i \boldsymbol{v}_i\right\Vert^2\quad\text{s.t.}\quad \sum_{i=1}^n \gamma_i = n - k\end{equation}$$ 这个问题的精确求解是比较困难的（NP Hard），但有一个简单的近似解：当$v_i$两两正交时，我们有
$$\begin{equation}\left\Vert\sum_{i=1}^n \gamma_i \boldsymbol{v}_i\right\Vert^2 = \sum_{i=1}^n \gamma_i^2 \Vert\boldsymbol{v}_i\Vert^2 = \sum_{i=1}^n \gamma_i \Vert\boldsymbol{v}_i\Vert^2\end{equation}$$ 上式最优解显然就是让模长$\Vert\boldsymbol{v}_i\Vert$最小的$n-k$个$\gamma_i$等于1，这又等价于说挑出模长最大的$k$个向量来逼近$n$个向量之和。当$v_i$不满足两两正交的条件时，我们依然用它来作为一个近似解。它的几何意义也很直观，模长越大的向量，在求和过程中越不容易被抵消，从而作用越突出。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-08T15:05:12+08:00"><meta property="article:modified_time" content="2025-08-08T15:05:12+08:00"><meta property="article:tag" content="MoE"><meta property="og:image" content="https://pillumina.github.io/imgs/icon_head.png"><meta property="og:see_also" content="https://pillumina.github.io/posts/llmtheory/2-moe/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://pillumina.github.io/imgs/icon_head.png"><meta name=twitter:title content="MoE环游记：1、从几何意义出发"><meta name=twitter:description content="MoE（Mixture of Experts）架构的流行自不必多说，近来火出圈的DeepSeek-V3便是MoE架构，传言GPT-5也是MoE架构，国内最近出的一些模型（Qwen3系列相关）也有不少用上了MoE。然而，虽然MoE的研究由来已久，但其应用长时间内都不愠不火，大致上是从去年初的《Mixtral of Experts》开始，MoE才逐渐吸引大家的注意力，其显著优点是参数量大，但训练和推理成本都显著低。
但同时MoE也有一些难题，如训练不稳定、负载不均衡、效果不够好等，这也是它早年没有流行起来的主要原因。不过随着这两年关注度的提升，这些问题在很大程度上已经得到解决，我们在接下来的介绍中会逐一谈到这些内容。
问题定义
我们知道，Transformer模型由Attention层和MLP层组成，MoE替换的是模型中MLP层。MLP层又分FFN（FeedForward Network）和GLU（Gated Linear Unit）两种，主流的是GLU，但简单起见我们还是以FFN为例：$$y=f(xW^{(A)})W^{(B)}$$其中$x\in\mathbb{R}^d$ 是输入向量（行向量），$W^{(A)}\in\mathbb{R}^{d\times{D}}$, $W^{(B)}\in\mathbb{R}^{D\times{d}}$ 是两个参数矩阵，$f$是Element-wise的激活函数，设$n$是一个能整除$D$的整数，那么上面的FFN可以用分块矩阵等价：

$$ \begin{equation}\boldsymbol{y} = f\big(\boldsymbol{x}\begin{bmatrix}\boldsymbol{W}^{(A)}_1 & \boldsymbol{W}^{(A)}_2 & \cdots & \boldsymbol{W}^{(A)}_n\end{bmatrix}\big)\begin{bmatrix}\boldsymbol{W}^{(B)}_1 \\ \boldsymbol{W}^{(B)}_2 \\ \vdots \\ \boldsymbol{W}^{(B)}_n\end{bmatrix} = \sum_{i=1}^n \underbrace{f(\boldsymbol{x}\boldsymbol{W}^{(A)}_i)\boldsymbol{W}^{(B)}_i}_{\boldsymbol{v}_i}\end{equation} $$

其中
$W^{(A)}_i = W^{(A)}_{[:,(i-1)c:ic]}$, $W^{(B)}_i = W^{(B)}_{[(i-1)c:ic,:]}$, $c= D/n$，这里的切片按照Python规则来。由此可见，FFN可以等价表示成n个向量
$\boldsymbol{v}_1,\boldsymbol{v}_2,\cdots,\boldsymbol{v}_n$
之和，每个向量代表了一个小模型$f(\boldsymbol{x}\boldsymbol{W}^{(A)}_i)\boldsymbol{W}^{(B)}_i$的输出，每个小模型计算量相同，这些小模型就是MoE中的“Expert”。
MoE提出的问题是：

能否只挑k个向量的和来逼近n个向量的和呢？这样就可以将计算量降低到k/n了。
模长排序
要解决上述的问题，实质上是要解决低秩近似的问题，数学公式就是:

$$\begin{equation}\mathop{\text{argmin}}_{\lambda_1,\lambda_2,\cdots,\lambda_n\in\{0,1\}}\left\Vert\sum_{i=1}^n \lambda_i \boldsymbol{v}_i - \sum_{i=1}^n\boldsymbol{v}_i\right\Vert^2\quad\text{s.t.}\quad \sum_{i=1}^n \lambda_i = k\end{equation}$$ 

记$\gamma_i = 1 - \lambda_i$，那么它又可以写成：

$$\begin{equation}\mathop{\text{argmin}}_{\gamma_1,\gamma_2,\cdots,\gamma_n\in\{0,1\}}\left\Vert\sum_{i=1}^n \gamma_i \boldsymbol{v}_i\right\Vert^2\quad\text{s.t.}\quad \sum_{i=1}^n \gamma_i = n - k\end{equation}$$

这个问题的精确求解是比较困难的（NP Hard），但有一个简单的近似解：当$v_i$两两正交时，我们有

$$\begin{equation}\left\Vert\sum_{i=1}^n \gamma_i \boldsymbol{v}_i\right\Vert^2 = \sum_{i=1}^n \gamma_i^2 \Vert\boldsymbol{v}_i\Vert^2 = \sum_{i=1}^n \gamma_i \Vert\boldsymbol{v}_i\Vert^2\end{equation}$$

上式最优解显然就是让模长$\Vert\boldsymbol{v}_i\Vert$最小的$n-k$个$\gamma_i$等于1，这又等价于说挑出模长最大的$k$个向量来逼近$n$个向量之和。当$v_i$不满足两两正交的条件时，我们依然用它来作为一个近似解。它的几何意义也很直观，模长越大的向量，在求和过程中越不容易被抵消，从而作用越突出。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://pillumina.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Theory","item":"https://pillumina.github.io/posts/llmtheory/"},{"@type":"ListItem","position":3,"name":"MoE环游记：1、从几何意义出发","item":"https://pillumina.github.io/posts/llmtheory/1-moe/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"MoE环游记：1、从几何意义出发","name":"MoE环游记：1、从几何意义出发","description":"MoE（Mixture of Experts）架构的流行自不必多说，近来火出圈的DeepSeek-V3便是MoE架构，传言GPT-5也是MoE架构，国内最近出的一些模型（Qwen3系列相关）也有不少用上了MoE。然而，虽然MoE的研究由来已久，但其应用长时间内都不愠不火，大致上是从去年初的《Mixtral of Experts》开始，MoE才逐渐吸引大家的注意力，其显著优点是参数量大，但训练和推理成本都显著低。\n但同时MoE也有一些难题，如训练不稳定、负载不均衡、效果不够好等，这也是它早年没有流行起来的主要原因。不过随着这两年关注度的提升，这些问题在很大程度上已经得到解决，我们在接下来的介绍中会逐一谈到这些内容。\n问题定义 我们知道，Transformer模型由Attention层和MLP层组成，MoE替换的是模型中MLP层。MLP层又分FFN（FeedForward Network）和GLU（Gated Linear Unit）两种，主流的是GLU，但简单起见我们还是以FFN为例：$$y=f(xW^{(A)})W^{(B)}$$其中$x\\in\\mathbb{R}^d$ 是输入向量（行向量），$W^{(A)}\\in\\mathbb{R}^{d\\times{D}}$, $W^{(B)}\\in\\mathbb{R}^{D\\times{d}}$ 是两个参数矩阵，$f$是Element-wise的激活函数，设$n$是一个能整除$D$的整数，那么上面的FFN可以用分块矩阵等价：\n$$ \\begin{equation}\\boldsymbol{y} = f\\big(\\boldsymbol{x}\\begin{bmatrix}\\boldsymbol{W}^{(A)}_1 \u0026 \\boldsymbol{W}^{(A)}_2 \u0026 \\cdots \u0026 \\boldsymbol{W}^{(A)}_n\\end{bmatrix}\\big)\\begin{bmatrix}\\boldsymbol{W}^{(B)}_1 \\\\ \\boldsymbol{W}^{(B)}_2 \\\\ \\vdots \\\\ \\boldsymbol{W}^{(B)}_n\\end{bmatrix} = \\sum_{i=1}^n \\underbrace{f(\\boldsymbol{x}\\boldsymbol{W}^{(A)}_i)\\boldsymbol{W}^{(B)}_i}_{\\boldsymbol{v}_i}\\end{equation} $$ 其中\n$W^{(A)}_i = W^{(A)}_{[:,(i-1)c:ic]}$, $W^{(B)}_i = W^{(B)}_{[(i-1)c:ic,:]}$, $c= D/n$，这里的切片按照Python规则来。由此可见，FFN可以等价表示成n个向量\n$\\boldsymbol{v}_1,\\boldsymbol{v}_2,\\cdots,\\boldsymbol{v}_n$\n之和，每个向量代表了一个小模型$f(\\boldsymbol{x}\\boldsymbol{W}^{(A)}_i)\\boldsymbol{W}^{(B)}_i$的输出，每个小模型计算量相同，这些小模型就是MoE中的“Expert”。\nMoE提出的问题是：\n能否只挑k个向量的和来逼近n个向量的和呢？这样就可以将计算量降低到k/n了。\n模长排序 要解决上述的问题，实质上是要解决低秩近似的问题，数学公式就是:\n$$\\begin{equation}\\mathop{\\text{argmin}}_{\\lambda_1,\\lambda_2,\\cdots,\\lambda_n\\in\\{0,1\\}}\\left\\Vert\\sum_{i=1}^n \\lambda_i \\boldsymbol{v}_i - \\sum_{i=1}^n\\boldsymbol{v}_i\\right\\Vert^2\\quad\\text{s.t.}\\quad \\sum_{i=1}^n \\lambda_i = k\\end{equation}$$ 记$\\gamma_i = 1 - \\lambda_i$，那么它又可以写成：\n$$\\begin{equation}\\mathop{\\text{argmin}}_{\\gamma_1,\\gamma_2,\\cdots,\\gamma_n\\in\\{0,1\\}}\\left\\Vert\\sum_{i=1}^n \\gamma_i \\boldsymbol{v}_i\\right\\Vert^2\\quad\\text{s.t.}\\quad \\sum_{i=1}^n \\gamma_i = n - k\\end{equation}$$ 这个问题的精确求解是比较困难的（NP Hard），但有一个简单的近似解：当$v_i$两两正交时，我们有\n$$\\begin{equation}\\left\\Vert\\sum_{i=1}^n \\gamma_i \\boldsymbol{v}_i\\right\\Vert^2 = \\sum_{i=1}^n \\gamma_i^2 \\Vert\\boldsymbol{v}_i\\Vert^2 = \\sum_{i=1}^n \\gamma_i \\Vert\\boldsymbol{v}_i\\Vert^2\\end{equation}$$ 上式最优解显然就是让模长$\\Vert\\boldsymbol{v}_i\\Vert$最小的$n-k$个$\\gamma_i$等于1，这又等价于说挑出模长最大的$k$个向量来逼近$n$个向量之和。当$v_i$不满足两两正交的条件时，我们依然用它来作为一个近似解。它的几何意义也很直观，模长越大的向量，在求和过程中越不容易被抵消，从而作用越突出。\n","keywords":["MoE"],"articleBody":"MoE（Mixture of Experts）架构的流行自不必多说，近来火出圈的DeepSeek-V3便是MoE架构，传言GPT-5也是MoE架构，国内最近出的一些模型（Qwen3系列相关）也有不少用上了MoE。然而，虽然MoE的研究由来已久，但其应用长时间内都不愠不火，大致上是从去年初的《Mixtral of Experts》开始，MoE才逐渐吸引大家的注意力，其显著优点是参数量大，但训练和推理成本都显著低。\n但同时MoE也有一些难题，如训练不稳定、负载不均衡、效果不够好等，这也是它早年没有流行起来的主要原因。不过随着这两年关注度的提升，这些问题在很大程度上已经得到解决，我们在接下来的介绍中会逐一谈到这些内容。\n问题定义 我们知道，Transformer模型由Attention层和MLP层组成，MoE替换的是模型中MLP层。MLP层又分FFN（FeedForward Network）和GLU（Gated Linear Unit）两种，主流的是GLU，但简单起见我们还是以FFN为例：$$y=f(xW^{(A)})W^{(B)}$$其中$x\\in\\mathbb{R}^d$ 是输入向量（行向量），$W^{(A)}\\in\\mathbb{R}^{d\\times{D}}$, $W^{(B)}\\in\\mathbb{R}^{D\\times{d}}$ 是两个参数矩阵，$f$是Element-wise的激活函数，设$n$是一个能整除$D$的整数，那么上面的FFN可以用分块矩阵等价：\n$$ \\begin{equation}\\boldsymbol{y} = f\\big(\\boldsymbol{x}\\begin{bmatrix}\\boldsymbol{W}^{(A)}_1 \u0026 \\boldsymbol{W}^{(A)}_2 \u0026 \\cdots \u0026 \\boldsymbol{W}^{(A)}_n\\end{bmatrix}\\big)\\begin{bmatrix}\\boldsymbol{W}^{(B)}_1 \\\\ \\boldsymbol{W}^{(B)}_2 \\\\ \\vdots \\\\ \\boldsymbol{W}^{(B)}_n\\end{bmatrix} = \\sum_{i=1}^n \\underbrace{f(\\boldsymbol{x}\\boldsymbol{W}^{(A)}_i)\\boldsymbol{W}^{(B)}_i}_{\\boldsymbol{v}_i}\\end{equation} $$ 其中\n$W^{(A)}_i = W^{(A)}_{[:,(i-1)c:ic]}$, $W^{(B)}_i = W^{(B)}_{[(i-1)c:ic,:]}$, $c= D/n$，这里的切片按照Python规则来。由此可见，FFN可以等价表示成n个向量\n$\\boldsymbol{v}_1,\\boldsymbol{v}_2,\\cdots,\\boldsymbol{v}_n$\n之和，每个向量代表了一个小模型$f(\\boldsymbol{x}\\boldsymbol{W}^{(A)}_i)\\boldsymbol{W}^{(B)}_i$的输出，每个小模型计算量相同，这些小模型就是MoE中的“Expert”。\nMoE提出的问题是：\n能否只挑k个向量的和来逼近n个向量的和呢？这样就可以将计算量降低到k/n了。\n模长排序 要解决上述的问题，实质上是要解决低秩近似的问题，数学公式就是:\n$$\\begin{equation}\\mathop{\\text{argmin}}_{\\lambda_1,\\lambda_2,\\cdots,\\lambda_n\\in\\{0,1\\}}\\left\\Vert\\sum_{i=1}^n \\lambda_i \\boldsymbol{v}_i - \\sum_{i=1}^n\\boldsymbol{v}_i\\right\\Vert^2\\quad\\text{s.t.}\\quad \\sum_{i=1}^n \\lambda_i = k\\end{equation}$$ 记$\\gamma_i = 1 - \\lambda_i$，那么它又可以写成：\n$$\\begin{equation}\\mathop{\\text{argmin}}_{\\gamma_1,\\gamma_2,\\cdots,\\gamma_n\\in\\{0,1\\}}\\left\\Vert\\sum_{i=1}^n \\gamma_i \\boldsymbol{v}_i\\right\\Vert^2\\quad\\text{s.t.}\\quad \\sum_{i=1}^n \\gamma_i = n - k\\end{equation}$$ 这个问题的精确求解是比较困难的（NP Hard），但有一个简单的近似解：当$v_i$两两正交时，我们有\n$$\\begin{equation}\\left\\Vert\\sum_{i=1}^n \\gamma_i \\boldsymbol{v}_i\\right\\Vert^2 = \\sum_{i=1}^n \\gamma_i^2 \\Vert\\boldsymbol{v}_i\\Vert^2 = \\sum_{i=1}^n \\gamma_i \\Vert\\boldsymbol{v}_i\\Vert^2\\end{equation}$$ 上式最优解显然就是让模长$\\Vert\\boldsymbol{v}_i\\Vert$最小的$n-k$个$\\gamma_i$等于1，这又等价于说挑出模长最大的$k$个向量来逼近$n$个向量之和。当$v_i$不满足两两正交的条件时，我们依然用它来作为一个近似解。它的几何意义也很直观，模长越大的向量，在求和过程中越不容易被抵消，从而作用越突出。\nMoE初现 现在策略已经有了——“挑模长最大的$k$个向量”——可是细想之下我们会发现它并不实用：要挑模长最大的$k$个向量，就得把所有向量的模长都算出来，这又意味着要把所有的$\\boldsymbol{v}_i$先算出来，可我们的原本目的却是减少$v_i$的计算量！\n为了解决这个矛盾，我们需要重新设计每个Expert模型，使得它的模长可以低成本地计算出来。什么意思呢？首先我们将$v_i$归一化得到$\\boldsymbol{e}_i = \\boldsymbol{v}_i/\\Vert\\boldsymbol{v}_i\\Vert$，这样每个$e_i$的模长都相同了。接着我们定义\n$$\\begin{equation}\\underbrace{[\\rho_1,\\rho_2,\\cdots,\\rho_n]}_{\\boldsymbol{\\rho}} = h(\\boldsymbol{x}\\boldsymbol{W}^{(R)})\\quad\\in\\mathbb{R}_{\\geq 0}^n\\end{equation}$$ 其中$\\boldsymbol{W}^{(R)}\\in\\mathbb{R}^{d\\times n}$是参数矩阵，$h(\\cdot)$是一个$\\mathbb{R}\\to\\mathbb{R}_{\\geq 0}$的激活函数，说白了这就是一个$d$维到$n$维的线性变换加激活函数，所以计算量是比较小的，这部分模型在MoE中被称为“Router”。\n$\\boldsymbol{\\rho}$的作用是什么呢？预测每个Expert的模长！换言之，我们将$\\rho_i$作为第$i$个Expert的模长，$\\rho_i \\boldsymbol{e}_i$才是完整的Expert，它被分解为两部分：计算量比较小的模长$\\rho_i$以及计算量比较大的方向$\\boldsymbol{e}_i$。为了减少计算量，我们先计算出$\\boldsymbol{\\rho}$，挑出最大的$k$个后才去计算相应的$e_i$，最后乘上$\\rho_i$并求和：\n$$\\begin{equation}\\boldsymbol{y} = \\sum_{i\\in \\mathop{\\text{argtop}}_k \\boldsymbol{\\rho}} \\rho_i \\boldsymbol{e}_i\\end{equation}$$ 这便是MoE模型的基本公式。由于计算中只保留了Top-$k$部分，所以它本质上属于一种Sparse模型，而原本的FFN或者$k=n$时的模型，通常称为对应的Dense模型。\n思路概括 我们再来整理一下整个思路：\n1、一个常规的Dense模型FFN，可以等价改写为$n$个Expert向量$\\boldsymbol{v}_1,\\boldsymbol{v}_2,\\cdots,\\boldsymbol{v}_n$之和;\n2、为了节省计算量，我们试图挑出$k$个向量求和来逼近原本的$n$个向量之和 ;\n3、转化为数学问题求解后，我们发现挑选规则是模长最大的$k$个向量；\n4、直接去算$n$个Expert的模长然后选$k$个实际上是不省计算量的，所以要重新设计Expert；\n5、将$\\boldsymbol{v}_i$归一化得到$\\boldsymbol{e}_i$，然后用另外的小模型（Router）预测模长$\\rho_i$，最终的Expert为$\\rho_i \\boldsymbol{e}_i$；\n6、此时，我们就可以先算全体$\\rho_i$，挑出$k$个后才去计算$\\boldsymbol{e}_i$，达到节省计算量的目的\n为何如此 可能有些读者疑问，为什么要做这个看似复杂的过程？原本的MoE不是挺好理解的吗？一般的MoE形式为\n$$\\begin{equation}\\boldsymbol{y} = \\sum_{i\\in \\mathop{\\text{argtop}}_k \\boldsymbol{\\rho}} \\rho_i \\boldsymbol{v}_i\\end{equation}$$\n也就是求和前少了对$\\boldsymbol{v}_i$的归一化，此时$\\rho_i$也没有模长的意义，它纯粹是一个用来对Expert排序的打分模型（即Router）。可为什么将$\\rho_i$乘到Expert上去就能让Router学会正确排序Expert呢？笔者发现只有《Sparse Backpropagation for MoE Training》对此给出了一个解释，但还是稍欠直观。\n而在本文的几何视角下，我们会发现很多问题就“豁然开朗”了。我们将Expert重新参数化为$\\rho_i \\boldsymbol{e}_i$后，Dense模型对应于全体$\\rho_i \\boldsymbol{e}_i$求和，而MoE对应于$\\rho_i$选Top-$k$后求和，这是Dense模型的一个有理论保证的逼近。我们没有去考虑Router如何选择Expert，只是每一步都尽可能逼近Dense模型，这可以说是既要大参数、又要小计算量的最佳选择。\n现在$\\rho_i$的几何意义是模长而不是概率，所以激活函数$h(\\cdot)$就没有归一化的要求了，除了Softmax外，像Sigmoid、ReLU都可以考虑使用，也可以考虑我们在《Softmax后传：寻找Top-K的光滑近似》介绍的Top-$k$光滑近似。Router使用非归一化的激活函数，有助于避免$k \u003e 1$时Expert之间的恶性竞争，有时候能取得更好的效果。\n最后补充一点，我们前面定义$\\boldsymbol{e}_i = \\boldsymbol{v}_i/ \\Vert\\boldsymbol{v}_i\\Vert$，目的是让所有$\\boldsymbol{e}_i$模长相同，实际操作中不是一定要L2 Normalize，也可以是其他等价操作，比如gamma参数恒等于1的RMS Norm，它更符合我们的输出习惯。\n文章小结 本文从Dense模型的最佳逼近出发来推导和理解MoE，得到了一种特定的MoE形式，它比现有MoE多了一个Normalize步骤，但能让MoE的几何意义更加明显。当然，不管Normalize与否，MoE之路都只是刚刚开始，更多的困难还在路上。\n","wordCount":"146","inLanguage":"en","image":"https://pillumina.github.io/imgs/icon_head.png","datePublished":"2025-08-08T15:05:12+08:00","dateModified":"2025-08-08T15:05:12+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://pillumina.github.io/posts/llmtheory/1-moe/"},"publisher":{"@type":"Organization","name":"CctoctoFX","logo":{"@type":"ImageObject","url":"https://pillumina.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://pillumina.github.io/ accesskey=h title="CctoctoFX (Alt + H)"><img src=https://pillumina.github.io/apple-touch-icon.png alt aria-label=logo height=30>CctoctoFX</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://pillumina.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://pillumina.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://pillumina.github.io/posts/aiinfra/ title="AI Infra"><span>AI Infra</span></a></li><li><a href=https://pillumina.github.io/posts/llmtheory/ title=Thoery><span>Thoery</span></a></li><li><a href=https://pillumina.github.io/posts/programming/ title=Programming><span>Programming</span></a></li><li><a href=https://pillumina.github.io/social/ title=Social><span>Social</span></a></li><li><a href=https://pillumina.github.io/open_courses/ title=Study><span>Study</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://pillumina.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://pillumina.github.io/posts/llmtheory/>Theory</a></div><h1 class="post-title entry-hint-parent">MoE环游记：1、从几何意义出发</h1><div class=post-meta><span title='2025-08-08 15:05:12 +0800 CST'>August 8, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;146 words&nbsp;·&nbsp;Me</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#问题定义>问题定义</a></li><li><a href=#模长排序>模长排序</a></li><li><a href=#moe初现>MoE初现</a></li><li><a href=#思路概括>思路概括</a></li><li><a href=#为何如此>为何如此</a></li><li><a href=#文章小结>文章小结</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>MoE（Mixture of Experts）架构的流行自不必多说，近来火出圈的<a href=https://papers.cool/arxiv/2412.19437>DeepSeek-V3</a>便是MoE架构，传言GPT-5也是MoE架构，国内最近出的一些模型（Qwen3系列相关）也有不少用上了MoE。然而，虽然MoE的研究由来已久，但其应用长时间内都不愠不火，大致上是从去年初的<a href=https://papers.cool/arxiv/2401.04088>《Mixtral of Experts》</a>开始，MoE才逐渐吸引大家的注意力，其显著优点是参数量大，但训练和推理成本都显著低。</p><p>但同时MoE也有一些难题，如训练不稳定、负载不均衡、效果不够好等，这也是它早年没有流行起来的主要原因。不过随着这两年关注度的提升，这些问题在很大程度上已经得到解决，我们在接下来的介绍中会逐一谈到这些内容。</p><h3 id=问题定义>问题定义<a hidden class=anchor aria-hidden=true href=#问题定义>#</a></h3><p>我们知道，Transformer模型由Attention层和MLP层组成，MoE替换的是模型中MLP层。MLP层又分FFN（FeedForward Network）和GLU（Gated Linear Unit）两种，主流的是GLU，但简单起见我们还是以FFN为例：$$y=f(xW^{(A)})W^{(B)}$$其中$x\in\mathbb{R}^d$ 是输入向量（行向量），$W^{(A)}\in\mathbb{R}^{d\times{D}}$, $W^{(B)}\in\mathbb{R}^{D\times{d}}$ 是两个参数矩阵，$f$是<code>Element-wise</code>的激活函数，设$n$是一个能整除$D$的整数，那么上面的FFN可以用分块矩阵等价：<br>$$ \begin{equation}\boldsymbol{y} = f\big(\boldsymbol{x}\begin{bmatrix}\boldsymbol{W}^{(A)}_1 & \boldsymbol{W}^{(A)}_2 & \cdots & \boldsymbol{W}^{(A)}_n\end{bmatrix}\big)\begin{bmatrix}\boldsymbol{W}^{(B)}_1 \\ \boldsymbol{W}^{(B)}_2 \\ \vdots \\ \boldsymbol{W}^{(B)}_n\end{bmatrix} = \sum_{i=1}^n \underbrace{f(\boldsymbol{x}\boldsymbol{W}^{(A)}_i)\boldsymbol{W}^{(B)}_i}_{\boldsymbol{v}_i}\end{equation} $$</p><p>其中<br>$W^{(A)}_i = W^{(A)}_{[:,(i-1)c:ic]}$, $W^{(B)}_i = W^{(B)}_{[(i-1)c:ic,:]}$, $c= D/n$，这里的切片按照Python规则来。由此可见，FFN可以等价表示成n个向量<br>$\boldsymbol{v}_1,\boldsymbol{v}_2,\cdots,\boldsymbol{v}_n$<br>之和，每个向量代表了一个小模型$f(\boldsymbol{x}\boldsymbol{W}^{(A)}_i)\boldsymbol{W}^{(B)}_i$的输出，每个小模型计算量相同，这些小模型就是MoE中的“Expert”。</p><p>MoE提出的问题是：</p><blockquote><p>能否只挑k个向量的和来逼近n个向量的和呢？这样就可以将计算量降低到k/n了。</p></blockquote><h3 id=模长排序>模长排序<a hidden class=anchor aria-hidden=true href=#模长排序>#</a></h3><p>要解决上述的问题，实质上是要解决<strong>低秩近似</strong>的问题，数学公式就是:<br>$$\begin{equation}\mathop{\text{argmin}}_{\lambda_1,\lambda_2,\cdots,\lambda_n\in\{0,1\}}\left\Vert\sum_{i=1}^n \lambda_i \boldsymbol{v}_i - \sum_{i=1}^n\boldsymbol{v}_i\right\Vert^2\quad\text{s.t.}\quad \sum_{i=1}^n \lambda_i = k\end{equation}$$<br>记$\gamma_i = 1 - \lambda_i$，那么它又可以写成：<br>$$\begin{equation}\mathop{\text{argmin}}_{\gamma_1,\gamma_2,\cdots,\gamma_n\in\{0,1\}}\left\Vert\sum_{i=1}^n \gamma_i \boldsymbol{v}_i\right\Vert^2\quad\text{s.t.}\quad \sum_{i=1}^n \gamma_i = n - k\end{equation}$$<br>这个问题的精确求解是比较困难的（NP Hard），但有一个简单的近似解：当$v_i$<strong>两两正交</strong>时，我们有<br>$$\begin{equation}\left\Vert\sum_{i=1}^n \gamma_i \boldsymbol{v}_i\right\Vert^2 = \sum_{i=1}^n \gamma_i^2 \Vert\boldsymbol{v}_i\Vert^2 = \sum_{i=1}^n \gamma_i \Vert\boldsymbol{v}_i\Vert^2\end{equation}$$<br>上式最优解显然就是让模长$\Vert\boldsymbol{v}_i\Vert$最小的$n-k$个$\gamma_i$等于1，这又等价于说挑出模长最大的$k$个向量来逼近$n$个向量之和。当$v_i$不满足两两正交的条件时，我们依然用它来作为一个<strong>近似解</strong>。它的几何意义也很直观，<strong>模长越大的向量，在求和过程中越不容易被抵消，从而作用越突出</strong>。</p><h3 id=moe初现>MoE初现<a hidden class=anchor aria-hidden=true href=#moe初现>#</a></h3><p>现在策略已经有了——“挑模长最大的$k$个向量”——可是细想之下我们会发现它并不实用：要挑模长最大的$k$个向量，就得把所有向量的模长都算出来，这又意味着要把所有的$\boldsymbol{v}_i$先算出来，可我们的原本目的却是减少$v_i$的计算量！</p><p>为了解决这个矛盾，我们需要重新设计每个Expert模型，使得它的模长可以低成本地计算出来。什么意思呢？首先我们将$v_i$归一化得到$\boldsymbol{e}_i = \boldsymbol{v}_i/\Vert\boldsymbol{v}_i\Vert$，这样每个$e_i$的模长都相同了。接着我们定义<br>$$\begin{equation}\underbrace{[\rho_1,\rho_2,\cdots,\rho_n]}_{\boldsymbol{\rho}} = h(\boldsymbol{x}\boldsymbol{W}^{(R)})\quad\in\mathbb{R}_{\geq 0}^n\end{equation}$$<br>其中$\boldsymbol{W}^{(R)}\in\mathbb{R}^{d\times n}$是参数矩阵，$h(\cdot)$是一个$\mathbb{R}\to\mathbb{R}_{\geq 0}$的激活函数，说白了这就是一个$d$维到$n$维的线性变换加激活函数，所以计算量是比较小的，这部分模型在MoE中被称为“Router”。</p><p>$\boldsymbol{\rho}$的作用是什么呢？预测每个Expert的模长！换言之，我们将$\rho_i$作为第$i$个Expert的模长，$\rho_i \boldsymbol{e}_i$才是完整的Expert，它被分解为两部分：计算量比较小的模长$\rho_i$以及计算量比较大的方向$\boldsymbol{e}_i$。为了减少计算量，我们先计算出$\boldsymbol{\rho}$，挑出最大的$k$个后才去计算相应的$e_i$，最后乘上$\rho_i$并求和：<br>$$\begin{equation}\boldsymbol{y} = \sum_{i\in \mathop{\text{argtop}}_k \boldsymbol{\rho}} \rho_i \boldsymbol{e}_i\end{equation}$$<br>这便是MoE模型的基本公式。由于计算中只保留了Top-$k$部分，所以它本质上属于一种Sparse模型，而原本的FFN或者$k=n$时的模型，通常称为对应的Dense模型。</p><h3 id=思路概括>思路概括<a hidden class=anchor aria-hidden=true href=#思路概括>#</a></h3><p>我们再来整理一下整个思路：</p><blockquote><p>1、一个常规的Dense模型FFN，可以等价改写为$n$个Expert向量$\boldsymbol{v}_1,\boldsymbol{v}_2,\cdots,\boldsymbol{v}_n$之和;</p><p>2、为了节省计算量，我们试图挑出$k$个向量求和来逼近原本的$n$个向量之和 ;</p><p>3、转化为数学问题求解后，我们发现挑选规则是模长最大的$k$个向量；</p><p>4、直接去算$n$个Expert的模长然后选$k$个实际上是不省计算量的，所以要重新设计Expert；</p><p>5、将$\boldsymbol{v}_i$归一化得到$\boldsymbol{e}_i$，然后用另外的小模型（Router）预测模长$\rho_i$，最终的Expert为$\rho_i \boldsymbol{e}_i$；</p><p>6、此时，我们就可以先算全体$\rho_i$，挑出$k$个后才去计算$\boldsymbol{e}_i$，达到节省计算量的目的</p></blockquote><h3 id=为何如此>为何如此<a hidden class=anchor aria-hidden=true href=#为何如此>#</a></h3><p>可能有些读者疑问，为什么要做这个看似复杂的过程？原本的MoE不是挺好理解的吗？一般的MoE形式为<br>$$\begin{equation}\boldsymbol{y} = \sum_{i\in \mathop{\text{argtop}}_k \boldsymbol{\rho}} \rho_i \boldsymbol{v}_i\end{equation}$$<br>也就是求和前少了对$\boldsymbol{v}_i$的归一化，此时$\rho_i$也没有模长的意义，它纯粹是一个用来对Expert排序的打分模型（即Router）。可为什么将$\rho_i$乘到Expert上去就能让Router学会正确排序Expert呢？笔者发现只有<a href=https://papers.cool/arxiv/2310.00811>《Sparse Backpropagation for MoE Training》</a>对此给出了一个解释，但还是稍欠直观。</p><p>而在本文的几何视角下，我们会发现很多问题就“豁然开朗”了。我们将Expert重新参数化为$\rho_i \boldsymbol{e}_i$后，Dense模型对应于全体$\rho_i \boldsymbol{e}_i$求和，而MoE对应于$\rho_i$选Top-$k$后求和，这是Dense模型的一个有理论保证的逼近。我们没有去考虑Router如何选择Expert，只是每一步都尽可能逼近Dense模型，这可以说是<strong>既要</strong>大参数、<strong>又要</strong>小计算量的最佳选择。</p><p>现在$\rho_i$的几何意义是模长而不是概率，所以激活函数$h(\cdot)$就没有归一化的要求了，除了Softmax外，像Sigmoid、ReLU都可以考虑使用，也可以考虑我们在<a href=#ZgotmplZ>《Softmax后传：寻找Top-K的光滑近似》</a>介绍的Top-$k$光滑近似。Router使用非归一化的激活函数，有助于避免$k > 1$时Expert之间的恶性竞争，有时候能取得更好的效果。</p><p>最后补充一点，我们前面定义$\boldsymbol{e}_i = \boldsymbol{v}_i/ \Vert\boldsymbol{v}_i\Vert$，目的是让所有$\boldsymbol{e}_i$模长相同，实际操作中不是一定要L2 Normalize，也可以是其他等价操作，比如gamma参数恒等于1的RMS Norm，它更符合我们的输出习惯。</p><h3 id=文章小结>文章小结<a hidden class=anchor aria-hidden=true href=#文章小结>#</a></h3><p>本文从Dense模型的最佳逼近出发来推导和理解MoE，得到了一种特定的MoE形式，它比现有MoE多了一个Normalize步骤，但能让MoE的几何意义更加明显。当然，不管Normalize与否，MoE之路都只是刚刚开始，更多的困难还在路上。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://pillumina.github.io/tags/moe/>MoE</a></li></ul><nav class=paginav><a class=prev href=https://pillumina.github.io/posts/llmtheory/2-moe/><span class=title>« Prev</span><br><span>MoE环游记：2、深入负载均衡</span>
</a><a class=next href=https://pillumina.github.io/posts/aiinfra/02-slime/><span class=title>Next »</span><br><span>[RL4LLM] 异步RL框架: Slime</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share MoE环游记：1、从几何意义出发 on x" href="https://x.com/intent/tweet/?text=MoE%e7%8e%af%e6%b8%b8%e8%ae%b0%ef%bc%9a1%e3%80%81%e4%bb%8e%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89%e5%87%ba%e5%8f%91&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2fllmtheory%2f1-moe%2f&amp;hashtags=MoE"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MoE环游记：1、从几何意义出发 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2fllmtheory%2f1-moe%2f&amp;title=MoE%e7%8e%af%e6%b8%b8%e8%ae%b0%ef%bc%9a1%e3%80%81%e4%bb%8e%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89%e5%87%ba%e5%8f%91&amp;summary=MoE%e7%8e%af%e6%b8%b8%e8%ae%b0%ef%bc%9a1%e3%80%81%e4%bb%8e%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89%e5%87%ba%e5%8f%91&amp;source=https%3a%2f%2fpillumina.github.io%2fposts%2fllmtheory%2f1-moe%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MoE环游记：1、从几何意义出发 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpillumina.github.io%2fposts%2fllmtheory%2f1-moe%2f&title=MoE%e7%8e%af%e6%b8%b8%e8%ae%b0%ef%bc%9a1%e3%80%81%e4%bb%8e%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89%e5%87%ba%e5%8f%91"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MoE环游记：1、从几何意义出发 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpillumina.github.io%2fposts%2fllmtheory%2f1-moe%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MoE环游记：1、从几何意义出发 on whatsapp" href="https://api.whatsapp.com/send?text=MoE%e7%8e%af%e6%b8%b8%e8%ae%b0%ef%bc%9a1%e3%80%81%e4%bb%8e%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89%e5%87%ba%e5%8f%91%20-%20https%3a%2f%2fpillumina.github.io%2fposts%2fllmtheory%2f1-moe%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MoE环游记：1、从几何意义出发 on telegram" href="https://telegram.me/share/url?text=MoE%e7%8e%af%e6%b8%b8%e8%ae%b0%ef%bc%9a1%e3%80%81%e4%bb%8e%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89%e5%87%ba%e5%8f%91&amp;url=https%3a%2f%2fpillumina.github.io%2fposts%2fllmtheory%2f1-moe%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MoE环游记：1、从几何意义出发 on ycombinator" href="https://news.ycombinator.com/submitlink?t=MoE%e7%8e%af%e6%b8%b8%e8%ae%b0%ef%bc%9a1%e3%80%81%e4%bb%8e%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89%e5%87%ba%e5%8f%91&u=https%3a%2f%2fpillumina.github.io%2fposts%2fllmtheory%2f1-moe%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul><div class=related-posts><div class=related-series><h3>同系列文章</h3><ul><li><a href=/posts/llmtheory/2-moe/>MoE环游记：2、深入负载均衡</a>
<span class=meta>2025-08-10
· 5 min read</span></li></ul></div><div class=related-tags><h3>相关文章</h3><ul><li><a href=/posts/aiinfra/13-vllmascend-mc2/>[vLLM-Ascend] MC2技术深度解析：从MoE架构到通信融合优化</a>
<span class=meta>2025-09-20
· 20 min read
· Tags: vllm-ascend, MoE</span></li><li><a href=/posts/llmtheory/2-moe/>MoE环游记：2、深入负载均衡</a>
<span class=meta>2025-08-10
· 5 min read
· Tags: MoE</span></li></ul></div></div></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://pillumina.github.io/>CctoctoFX</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div class=reading-progress-bar></div><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelector(".reading-progress-bar");if(!t)return;const n=document.querySelector(".post-single");if(!n)return;function s(){const e=n.getBoundingClientRect(),s=e.height,o=window.innerHeight,i=window.scrollY||window.pageYOffset,a=i/(s-o)*100;t.style.width=`${Math.min(100,Math.max(0,a))}%`}let e=!1;window.addEventListener("scroll",function(){e||(window.requestAnimationFrame(function(){s(),e=!1}),e=!0)}),s()}),document.addEventListener("DOMContentLoaded",function(){mediumZoom("article img:not(.nozoom)",{margin:24,background:"var(--theme)",scrollOffset:0})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>